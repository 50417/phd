#!/usr/bin/env python3.6

import csv
import datetime
import logging
import os
import re
import spreadsheet

from argparse import ArgumentParser, FileType
from collections import defaultdict
from tempfile import TemporaryDirectory
from zipfile import ZipFile
from xml.dom import minidom


def get_all_atributes(records, excludes=[]):
    attributes = set()
    for record in records:
        for attr in record.keys():
            attributes.add(attr)
    attributes = sorted([x for x in attributes if x not in excludes])
    return attributes


def parse_datetime(string):
    return datetime.datetime.strptime(str(string), "%Y-%m-%d %H:%M:%S %z")


def parse_date(string):
    return parse_datetime(string).date()


def _process_records_generic(typename, records, outdir):
    # build a list of attributes names (columns)
    attributes = get_all_atributes(records, ["type"])

    # Create CSV file
    with open(f"{outdir}/{typename}.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        # Write header
        writer.writerow(attributes)

        # Write rows
        for record in records:
            row = []
            for attr in attributes:
                try:
                    row.append(record[attr].value)
                except:
                    row.append('')
            writer.writerow(row)

        nrows = len(records)
        logging.info(f"Exported {nrows} records for \"{typename}\"")


def sum_values_by_day(records, value_attr: str='value',
                      date_attr: str='endDate', filter_fn=None):
    rows = []
    for r in records:
        if filter_fn and not filter_fn(r):
            rows.append((parse_date(r[date_attr].value),
                         float(r[value_attr].value)))
    rows = sorted(rows, key=lambda x: x[0])

    rows2 = []
    last_date = None
    aggr = 0
    for row in rows:
        date, value = row
        if date != last_date:
            if last_date:
                rows2.append((last_date, aggr))
            last_date = date
            aggr = value
        else:
            aggr += value
    rows2.append((last_date, aggr))

    return rows2


def avg_values_by_day(records, value_attr: str='value',
                      date_attr: str='endDate', filter_fn=None,
                      min_max: bool=False):
    rows = []
    for r in records:
        if filter_fn and not filter_fn(r):
            rows.append((parse_date(r[date_attr].value),
                         float(r[value_attr].value)))
    rows = sorted(rows, key=lambda x: x[0])

    rows2 = []
    last_date = None
    aggr = []
    for row in rows:
        date, value = row
        if date != last_date:
            if last_date:
                if min_max:
                    rows2.append((last_date, min(aggr), sum(aggr) / len(aggr), max(aggr)))
                else:
                    rows2.append((last_date, sum(aggr) / len(aggr)))
            last_date = date
            aggr = [value]
        else:
            aggr.append(value)
    if min_max:
        rows2.append((last_date, min(aggr), sum(aggr) / len(aggr), max(aggr)))
    else:
        rows2.append((last_date, sum(aggr) / len(aggr)))

    return rows2


def get_records_start_date(records):
    startdate = sorted(records, key=lambda x: x['endDate'].value)[0]['endDate'].value
    return parse_date(startdate)


def _process_step_count(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'count')

    with open(f"{outdir}/{typename}.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Step Count"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_body_mass(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'kg')

    with open(f"{outdir}/Weight.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Weight (kg)"))
        rows = avg_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_dietary_energy_consumer(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'kcal')

    with open(f"{outdir}/Calories In.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Calories In (kcal)"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_active_energy_burned(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'kcal')

    with open(f"{outdir}/Calories Out.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Calories Out (kcal)"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_dietary_water(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'mL')

    with open(f"{outdir}/Water.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Water (mL)"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_dietary_caffeine(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'mg')

    with open(f"{outdir}/Caffeine.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Caffeine (mg)"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_body_mass_index(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'count')

    with open(f"{outdir}/BMI.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Body Mass Index"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_distance_walking_running(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'km')

    with open(f"{outdir}/Distance.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Distance Walking + Running (km)"))
        rows = sum_values_by_day(records, filter_fn=_attr_filter)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def _process_heart_rate(typename, records, outdir):

    def _attr_filter(record):
        assert(record['unit'].value == 'count/min')

    with open(f"{outdir}/Heart Rate.csv", "w") as outfile:
        logging.debug(f"Creating CSV file {outfile.name}")
        writer = csv.writer(outfile, delimiter=",", quoting=csv.QUOTE_MINIMAL)

        writer.writerow(("Date", "Min Heart Rate (bpm)", "Avg Heart Rate (bpm)",
                         "Max Heart Rate (bpm)"))
        rows = avg_values_by_day(records, filter_fn=_attr_filter, min_max=True)
        for row in rows:
            writer.writerow(row)

    nrows = len(rows)
    logging.info(f"Exported {nrows} records to \"{outfile.name}\"")


def process_records(typename, records, outdir):
    handler = {
        "Body Mass": _process_body_mass,
        "Step Count": _process_step_count,
        "Dietary Energy Consumed": _process_dietary_energy_consumer,
        "Active Energy Burned": _process_active_energy_burned,
        "Dietary Water": _process_dietary_water,
        "Dietary Caffeine": _process_dietary_caffeine,
        "Distance Walking Running": _process_distance_walking_running,
        "Body Mass Index": _process_body_mass_index,
        "Heart Rate": _process_heart_rate,
    }.get(typename, _process_records_generic)

    return handler(typename, records, outdir)


def process_file(infile, outdir):
    logging.debug(f"Parsing export.xml")
    xmldoc = minidom.parse(infile.name)
    recordlist = xmldoc.getElementsByTagName('Record')

    data = defaultdict(list)
    for s in recordlist:
        typename = s.attributes['type'].value
        # Strip the HealthKit prefix from the type name:
        typename = typename[len("HKQuantityTypeIdentifier"):]
        # Split the CamelCase name into separate words:
        typename = " ".join(re.findall('[A-Z][^A-Z]*', typename))

        data[typename].append(s.attributes)

    for typename in data:
        process_records(typename, data[typename], outdir)


def process_archive(infile, outdir):
    try:
        os.mkdir(outdir)
    except FileExistsError:
        pass

    with TemporaryDirectory(prefix="me.csv.") as unzipdir:
        logging.debug(f"Unpacking healthkit archive to {unzipdir}")
        archive = ZipFile(infile.name)
        archive.extract("apple_health_export/export.xml", path=unzipdir)
        with open(f"{unzipdir}/apple_health_export/export.xml") as infile:
            process_file(infile, outdir)


def main():
    parser = ArgumentParser()
    parser.add_argument("infile", metavar="<export.zip>", type=FileType('r'),
                        help="Path to HealthKit export")
    parser.add_argument("outdir", metavar="<dir>",
                        help="Path to output CSV files")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="enable more verbose logging output")
    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.INFO)
    logging.basicConfig(format="%(message)s")

    process_archive(args.infile, args.outdir)


if __name__ == "__main__":
    main()
