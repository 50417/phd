       # Copyright 2006 Google, Inc. All Rights Reserved.
       # Licensed to PSF under a Contributor Agreement.
       
       """Refactoring framework.
       
       Used as a main program, this can refactor any number of files and/or
       recursively descend down directories.  Imported as a module, this
       provides infrastructure to write your own refactoring tool.
    1: """
       
    1: from __future__ import with_statement
       
    1: __author__ = "Guido van Rossum <guido@python.org>"
       
       
       # Python imports
    1: import os
    1: import sys
    1: import logging
    1: import operator
    1: import collections
    1: import StringIO
    1: from itertools import chain
       
       # Local imports
    1: from .pgen2 import driver, tokenize, token
    1: from .fixer_util import find_root
    1: from . import pytree, pygram
    1: from . import btm_utils as bu
    1: from . import btm_matcher as bm
       
       
    1: def get_all_fix_names(fixer_pkg, remove_prefix=True):
           """Return a sorted list of all available fix names in the given package."""
   10:     pkg = __import__(fixer_pkg, [], [], ["*"])
   10:     fixer_dir = os.path.dirname(pkg.__file__)
   10:     fix_names = []
  989:     for name in sorted(os.listdir(fixer_dir)):
  979:         if name.startswith("fix_") and name.endswith(".py"):
  326:             if remove_prefix:
    5:                 name = name[4:]
  326:             fix_names.append(name[:-3])
   10:     return fix_names
       
       
    2: class _EveryNode(Exception):
    1:     pass
       
       
    1: def _get_head_types(pat):
           """ Accepts a pytree Pattern Node and returns a set
               of the pattern types which will match first. """
       
  115:     if isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):
               # NodePatters must either have no type and no content
               #   or a type and content -- so they don't get any farther
               # Always return leafs
   97:         if pat.type is None:
   18:             raise _EveryNode
   79:         return set([pat.type])
       
   18:     if isinstance(pat, pytree.NegatedPattern):
>>>>>>         if pat.content:
>>>>>>             return _get_head_types(pat.content)
>>>>>>         raise _EveryNode # Negated Patterns don't have a type
       
   18:     if isinstance(pat, pytree.WildcardPattern):
               # Recurse on each node in content
   18:         r = set()
   72:         for p in pat.content:
  126:             for x in p:
   72:                 r.update(_get_head_types(x))
>>>>>>         return r
       
>>>>>>     raise Exception("Oh no! I don't understand pattern %s" %(pat))
       
       
    1: def _get_headnode_dict(fixer_list):
           """ Accepts a list of fixers and returns a dictionary
               of head node type --> fixer list.  """
  965:     head_nodes = collections.defaultdict(list)
  965:     every = []
 1078:     for fixer in fixer_list:
  113:         if fixer.pattern:
   43:             try:
   43:                 heads = _get_head_types(fixer.pattern)
   18:             except _EveryNode:
   18:                 every.append(fixer)
                   else:
   50:                 for node_type in heads:
   25:                     head_nodes[node_type].append(fixer)
               else:
   70:             if fixer._accept_type is not None:
   32:                 head_nodes[fixer._accept_type].append(fixer)
                   else:
   38:                 every.append(fixer)
  965:     for node_type in chain(pygram.python_grammar.symbol2number.itervalues(),
139925:                            pygram.python_grammar.tokens):
138960:         head_nodes[node_type].extend(every)
  965:     return dict(head_nodes)
       
       
    1: def get_fixers_from_package(pkg_name):
           """
           Return the fully qualified names for fixers in the package pkg_name.
           """
    8:     return [pkg_name + "." + fix_name
  324:             for fix_name in get_all_fix_names(pkg_name, False)]
       
    1: def _identity(obj):
>>>>>>     return obj
       
    1: if sys.version_info < (3, 0):
    1:     import codecs
    1:     _open_with_encoding = codecs.open
           # codecs.open doesn't translate newlines sadly.
    1:     def _from_system_newlines(input):
   34:         return input.replace(u"\r\n", u"\n")
    1:     def _to_system_newlines(input):
   11:         if os.linesep != "\n":
    1:             return input.replace(u"\n", os.linesep)
               else:
   10:             return input
       else:
>>>>>>     _open_with_encoding = open
>>>>>>     _from_system_newlines = _identity
>>>>>>     _to_system_newlines = _identity
       
       
    1: def _detect_future_features(source):
 2078:     have_docstring = False
 2078:     gen = tokenize.generate_tokens(StringIO.StringIO(source).readline)
 2078:     def advance():
 3491:         tok = gen.next()
 3482:         return tok[0], tok[1]
 2078:     ignore = frozenset((token.NEWLINE, tokenize.NL, token.COMMENT))
 2078:     features = set()
 2078:     try:
 2720:         while True:
 2720:             tp, value = advance()
 2711:             if tp in ignore:
  607:                 continue
 2104:             elif tp == token.STRING:
   16:                 if have_docstring:
    1:                     break
   15:                 have_docstring = True
 2088:             elif tp == token.NAME and value == u"from":
  697:                 tp, value = advance()
  697:                 if tp != token.NAME or value != u"__future__":
  677:                     break
   20:                 tp, value = advance()
   20:                 if tp != token.NAME or value != u"import":
>>>>>>                     break
   20:                 tp, value = advance()
   20:                 if tp == token.OP and value == u"(":
    3:                     tp, value = advance()
   26:                 while tp == token.NAME:
   25:                     features.add(value)
   25:                     tp, value = advance()
   25:                     if tp != token.OP or value != u",":
   19:                         break
    6:                     tp, value = advance()
                   else:
 1391:                 break
    9:     except StopIteration:
    9:         pass
 2078:     return frozenset(features)
       
       
    2: class FixerError(Exception):
    1:     """A fixer could not be loaded."""
       
       
    2: class RefactoringTool(object):
       
    1:     _default_options = {"print_function" : False,
    1:                         "write_unchanged_files" : False}
       
    1:     CLASS_PREFIX = "Fix" # The prefix for fixer classes
    1:     FILE_PREFIX = "fix_" # The prefix for modules with a fixer within
       
    1:     def __init__(self, fixer_names, options=None, explicit=None):
               """Initializer.
       
               Args:
                   fixer_names: a list of fixers to import
                   options: a dict with configuration.
                   explicit: a list of fixers to run even if they are explicit.
               """
  485:         self.fixers = fixer_names
  485:         self.explicit = explicit or []
  485:         self.options = self._default_options.copy()
  485:         if options is not None:
  467:             self.options.update(options)
  485:         if self.options["print_function"]:
    1:             self.grammar = pygram.python_grammar_no_print_statement
               else:
  484:             self.grammar = pygram.python_grammar
               # When this is True, the refactor*() methods will call write_file() for
               # files processed even if they were not changed during refactoring. If
               # and only if the refactor method's write parameter was True.
  485:         self.write_unchanged_files = self.options.get("write_unchanged_files")
  485:         self.errors = []
  485:         self.logger = logging.getLogger("RefactoringTool")
  485:         self.fixer_log = []
  485:         self.wrote = False
  485:         self.driver = driver.Driver(self.grammar,
  485:                                     convert=pytree.convert,
  485:                                     logger=self.logger)
  485:         self.pre_order, self.post_order = self.get_fixers()
       
       
  482:         self.files = []  # List of files that were or should be modified
       
  482:         self.BM = bm.BottomMatcher()
  482:         self.bmi_pre_order = [] # Bottom Matcher incompatible fixers
  482:         self.bmi_post_order = []
       
 1381:         for fixer in chain(self.post_order, self.pre_order):
  899:             if fixer.BM_compatible:
  789:                 self.BM.add_fixer(fixer)
                       # remove fixers that will be handled by the bottom-up
                       # matcher
  110:             elif fixer in self.pre_order:
   12:                 self.bmi_pre_order.append(fixer)
   98:             elif fixer in self.post_order:
   98:                 self.bmi_post_order.append(fixer)
       
  482:         self.bmi_pre_order_heads = _get_headnode_dict(self.bmi_pre_order)
  482:         self.bmi_post_order_heads = _get_headnode_dict(self.bmi_post_order)
       
       
       
    1:     def get_fixers(self):
               """Inspects the options to load the requested patterns and handlers.
       
               Returns:
                 (pre_order, post_order), where pre_order is the list of fixers that
                 want a pre-order AST traversal, and post_order is the list that want
                 post-order traversal.
               """
  490:         pre_order_fixers = []
  490:         post_order_fixers = []
 1448:         for fix_mod_path in self.fixers:
  961:             mod = __import__(fix_mod_path, {}, {}, ["*"])
  960:             fix_name = fix_mod_path.rsplit(".", 1)[-1]
  960:             if fix_name.startswith(self.FILE_PREFIX):
  958:                 fix_name = fix_name[len(self.FILE_PREFIX):]
  960:             parts = fix_name.split("_")
 2036:             class_name = self.CLASS_PREFIX + "".join([p.title() for p in parts])
  960:             try:
  960:                 fix_class = getattr(mod, class_name)
    1:             except AttributeError:
    1:                 raise FixerError("Can't find %s.%s" % (fix_name, class_name))
  959:             fixer = fix_class(self.options, self.fixer_log)
  959:             if fixer.explicit and self.explicit is not True and \
   46:                     fix_mod_path not in self.explicit:
   45:                 self.log_message("Skipping optional fixer: %s", fix_name)
   45:                 continue
       
  914:             self.log_debug("Adding transformation: %s", fix_name)
  914:             if fixer.order == "pre":
  123:                 pre_order_fixers.append(fixer)
  791:             elif fixer.order == "post":
  790:                 post_order_fixers.append(fixer)
                   else:
    1:                 raise FixerError("Illegal fixer order: %r" % fixer.order)
       
  487:         key_func = operator.attrgetter("run_order")
  487:         pre_order_fixers.sort(key=key_func)
  487:         post_order_fixers.sort(key=key_func)
  487:         return (pre_order_fixers, post_order_fixers)
       
    1:     def log_error(self, msg, *args, **kwds):
               """Called when an error occurs."""
>>>>>>         raise
       
    1:     def log_message(self, msg, *args):
               """Hook to log a message."""
   69:         if args:
   62:             msg = msg % args
   69:         self.logger.info(msg)
       
    1:     def log_debug(self, msg, *args):
 2996:         if args:
 2996:             msg = msg % args
 2996:         self.logger.debug(msg)
       
    1:     def print_output(self, old_text, new_text, filename, equal):
               """Called with the old version, new version, and filename of a
               refactored file."""
   11:         pass
       
    1:     def refactor(self, items, write=False, doctests_only=False):
               """Refactor a list of files and directories."""
       
    7:         for dir_or_file in items:
    4:             if os.path.isdir(dir_or_file):
    1:                 self.refactor_dir(dir_or_file, write, doctests_only)
                   else:
    3:                 self.refactor_file(dir_or_file, write, doctests_only)
       
    1:     def refactor_dir(self, dir_name, write=False, doctests_only=False):
               """Descends down a directory and refactor every Python file found.
       
               Python files are assumed to have a .py extension.
       
               Files and subdirectories starting with '.' are skipped.
               """
    4:         py_ext = os.extsep + "py"
   11:         for dirpath, dirnames, filenames in os.walk(dir_name):
    7:             self.log_debug("Descending into %s", dirpath)
    7:             dirnames.sort()
    7:             filenames.sort()
   18:             for name in filenames:
   11:                 if (not name.startswith(".") and
    9:                     os.path.splitext(name)[1] == py_ext):
    6:                     fullname = os.path.join(dirpath, name)
    6:                     self.refactor_file(fullname, write, doctests_only)
                   # Modify dirnames in-place to remove subdirs with leading dots
   10:             dirnames[:] = [dn for dn in dirnames if not dn.startswith(".")]
       
    1:     def _read_python_source(self, filename):
               """
               Do our best to decode a Python source file correctly.
               """
   34:         try:
   34:             f = open(filename, "rb")
>>>>>>         except IOError as err:
>>>>>>             self.log_error("Can't open %s: %s", filename, err)
>>>>>>             return None, None
   34:         try:
   34:             encoding = tokenize.detect_encoding(f.readline)[0]
               finally:
   34:             f.close()
   34:         with _open_with_encoding(filename, "r", encoding=encoding) as f:
   34:             return _from_system_newlines(f.read()), encoding
       
    1:     def refactor_file(self, filename, write=False, doctests_only=False):
               """Refactors a file."""
   17:         input, encoding = self._read_python_source(filename)
   17:         if input is None:
                   # Reading the file failed.
>>>>>>             return
   17:         input += u"\n" # Silence certain parse errors
   17:         if doctests_only:
>>>>>>             self.log_debug("Refactoring doctests in %s", filename)
>>>>>>             output = self.refactor_docstring(input, filename)
>>>>>>             if self.write_unchanged_files or output != input:
>>>>>>                 self.processed_file(output, filename, input, write, encoding)
                   else:
>>>>>>                 self.log_debug("No doctest changes in %s", filename)
               else:
   17:             tree = self.refactor_string(input, filename)
   17:             if self.write_unchanged_files or (tree and tree.was_changed):
                       # The [:-1] is to take off the \n we added earlier
   17:                 self.processed_file(unicode(tree)[:-1], filename,
   17:                                     write=write, encoding=encoding)
                   else:
>>>>>>                 self.log_debug("No changes in %s", filename)
       
    1:     def refactor_string(self, data, name):
               """Refactor a given input string.
       
               Args:
                   data: a string holding the code to be refactored.
                   name: a human-readable name for use in error/log messages.
       
               Returns:
                   An AST corresponding to the refactored input stream; None if
                   there were errors during the parse.
               """
 2056:         features = _detect_future_features(data)
 2056:         if "print_function" in features:
    1:             self.driver.grammar = pygram.python_grammar_no_print_statement
 2056:         try:
 2056:             tree = self.driver.parse_string(data)
>>>>>>         except Exception as err:
>>>>>>             self.log_error("Can't parse %s: %s: %s",
>>>>>>                            name, err.__class__.__name__, err)
>>>>>>             return
               finally:
 2056:             self.driver.grammar = self.grammar
 2056:         tree.future_features = features
 2056:         self.log_debug("Refactoring %s", name)
 2056:         self.refactor_tree(tree, name)
 2056:         return tree
       
    1:     def refactor_stdin(self, doctests_only=False):
    2:         input = sys.stdin.read()
    2:         if doctests_only:
>>>>>>             self.log_debug("Refactoring doctests in stdin")
>>>>>>             output = self.refactor_docstring(input, "<stdin>")
>>>>>>             if self.write_unchanged_files or output != input:
>>>>>>                 self.processed_file(output, "<stdin>", input)
                   else:
>>>>>>                 self.log_debug("No doctest changes in stdin")
               else:
    2:             tree = self.refactor_string(input, "<stdin>")
    2:             if self.write_unchanged_files or (tree and tree.was_changed):
    2:                 self.processed_file(unicode(tree), "<stdin>", input)
                   else:
>>>>>>                 self.log_debug("No changes in stdin")
       
    1:     def refactor_tree(self, tree, name):
               """Refactors a parse tree (modifying the tree in place).
       
               For compatible patterns the bottom matcher module is
               used. Otherwise the tree is traversed node-to-node for
               matches.
       
               Args:
                   tree: a pytree.Node instance representing the root of the tree
                         to be refactored.
                   name: a human-readable name for this tree.
       
               Returns:
                   True if the tree was modified, False otherwise.
               """
       
 4911:         for fixer in chain(self.pre_order, self.post_order):
 2853:             fixer.start_tree(tree, name)
       
               #use traditional matching for the incompatible fixers
 2058:         self.traverse_by(self.bmi_pre_order_heads, tree.pre_order())
 2058:         self.traverse_by(self.bmi_post_order_heads, tree.post_order())
       
               # obtain a set of candidate nodes
 2058:         match_set = self.BM.run(tree.leaves())
       
 4148:         while any(match_set.values()):
 5383:             for fixer in self.BM.fixers:
 3293:                 if fixer in match_set and match_set[fixer]:
                           #sort by depth; apply fixers from bottom(of the AST) to top
 2207:                     match_set[fixer].sort(key=pytree.Base.depth, reverse=True)
       
 2207:                     if fixer.keep_line_order:
                               #some fixers(eg fix_imports) must be applied
                               #with the original file's line order
 1287:                         match_set[fixer].sort(key=pytree.Base.get_lineno)
       
10509:                     for node in list(match_set[fixer]):
 8302:                         if node in match_set[fixer]:
 8302:                             match_set[fixer].remove(node)
       
 8302:                         try:
 8302:                             find_root(node)
 1182:                         except ValueError:
                                   # this node has been cut off from a
                                   # previous transformation ; skip
 1182:                             continue
       
 7120:                         if node.fixers_applied and fixer in node.fixers_applied:
                                   # do not apply the same fixer again
  178:                             continue
       
 6942:                         results = fixer.match(node)
       
 6942:                         if results:
 2736:                             new = fixer.transform(node, results)
 2736:                             if new is not None:
  591:                                 node.replace(new)
                                       #new.fixers_applied.append(fixer)
 5377:                                 for node in new.post_order():
                                           # do not apply the fixer again to
                                           # this or any subnode
 4786:                                     if not node.fixers_applied:
 4712:                                         node.fixers_applied = []
 4786:                                     node.fixers_applied.append(fixer)
       
                                       # update the original match set for
                                       # the added code
  591:                                 new_matches = self.BM.run(new.leaves())
 1060:                                 for fxr in new_matches:
  469:                                     if not fxr in match_set:
>>>>>>                                         match_set[fxr]=[]
       
  469:                                     match_set[fxr].extend(new_matches[fxr])
       
 4911:         for fixer in chain(self.pre_order, self.post_order):
 2853:             fixer.finish_tree(tree, name)
 2058:         return tree.was_changed
       
    1:     def traverse_by(self, fixers, traversal):
               """Traverse an AST, applying a set of fixers to each node.
       
               This is a helper method for refactor_tree().
       
               Args:
                   fixers: a list of fixer instances.
                   traversal: a generator that yields AST nodes.
       
               Returns:
                   None
               """
 4116:         if not fixers:
>>>>>>             return
90140:         for node in traversal:
88676:             for fixer in fixers[node.type]:
 2652:                 results = fixer.match(node)
 2652:                 if results:
  197:                     new = fixer.transform(node, results)
  197:                     if new is not None:
  117:                         node.replace(new)
  117:                         node = new
       
    1:     def processed_file(self, new_text, filename, old_text=None, write=False,
    1:                        encoding=None):
               """
               Called when a file has been refactored and there may be changes.
               """
   19:         self.files.append(filename)
   19:         if old_text is None:
   17:             old_text = self._read_python_source(filename)[0]
   17:             if old_text is None:
>>>>>>                 return
   19:         equal = old_text == new_text
   19:         self.print_output(old_text, new_text, filename, equal)
   19:         if equal:
    3:             self.log_debug("No changes to %s", filename)
    3:             if not self.write_unchanged_files:
>>>>>>                 return
   19:         if write:
   11:             self.write_file(new_text, filename, old_text, encoding)
               else:
    8:             self.log_debug("Not writing changes to %s", filename)
       
    1:     def write_file(self, new_text, filename, old_text, encoding=None):
               """Writes a string to a file.
       
               It first shows a unified diff between the old text and the new text, and
               then rewrites the file; the latter is only done if the write option is
               set.
               """
   11:         try:
   11:             f = _open_with_encoding(filename, "w", encoding=encoding)
>>>>>>         except os.error as err:
>>>>>>             self.log_error("Can't create %s: %s", filename, err)
>>>>>>             return
   11:         try:
   11:             f.write(_to_system_newlines(new_text))
>>>>>>         except os.error as err:
>>>>>>             self.log_error("Can't write %s: %s", filename, err)
               finally:
   11:             f.close()
   11:         self.log_debug("Wrote changes to %s", filename)
   11:         self.wrote = True
       
    1:     PS1 = ">>> "
    1:     PS2 = "... "
       
    1:     def refactor_docstring(self, input, filename):
               """Refactors a docstring, looking for doctests.
       
               This returns a modified version of the input string.  It looks
               for doctests, which start with a ">>>" prompt, and may be
               continued with "..." prompts, as long as the "..." is indented
               the same as the ">>>".
       
               (Unfortunately we can't use the doctest module's parser,
               since, like most parsers, it is not geared towards preserving
               the original source.)
               """
    2:         result = []
    2:         block = None
    2:         block_lineno = None
    2:         indent = None
    2:         lineno = 0
    8:         for line in input.splitlines(True):
    6:             lineno += 1
    6:             if line.lstrip().startswith(self.PS1):
    2:                 if block is not None:
>>>>>>                     result.extend(self.refactor_doctest(block, block_lineno,
>>>>>>                                                         indent, filename))
    2:                 block_lineno = lineno
    2:                 block = [line]
    2:                 i = line.find(self.PS1)
    2:                 indent = line[:i]
    4:             elif (indent is not None and
    2:                   (line.startswith(indent + self.PS2) or
    1:                    line == indent + self.PS2.rstrip() + u"\n")):
    1:                 block.append(line)
                   else:
    3:                 if block is not None:
    1:                     result.extend(self.refactor_doctest(block, block_lineno,
    1:                                                         indent, filename))
    3:                 block = None
    3:                 indent = None
    3:                 result.append(line)
    2:         if block is not None:
    1:             result.extend(self.refactor_doctest(block, block_lineno,
    1:                                                 indent, filename))
    2:         return u"".join(result)
       
    1:     def refactor_doctest(self, block, lineno, indent, filename):
               """Refactors one doctest.
       
               A doctest is given as a block of lines, the first of which starts
               with ">>>" (possibly indented), while the remaining lines start
               with "..." (identically indented).
       
               """
    2:         try:
    2:             tree = self.parse_block(block, lineno, indent)
>>>>>>         except Exception as err:
>>>>>>             if self.logger.isEnabledFor(logging.DEBUG):
>>>>>>                 for line in block:
>>>>>>                     self.log_debug("Source: %s", line.rstrip(u"\n"))
>>>>>>             self.log_error("Can't parse docstring in %s line %s: %s: %s",
>>>>>>                            filename, lineno, err.__class__.__name__, err)
>>>>>>             return block
    2:         if self.refactor_tree(tree, filename):
    1:             new = unicode(tree).splitlines(True)
                   # Undo the adjustment of the line numbers in wrap_toks() below.
    1:             clipped, new = new[:lineno-1], new[lineno-1:]
    1:             assert clipped == [u"\n"] * (lineno-1), clipped
    1:             if not new[-1].endswith(u"\n"):
>>>>>>                 new[-1] += u"\n"
    1:             block = [indent + self.PS1 + new.pop(0)]
    1:             if new:
    2:                 block += [indent + self.PS2 + line for line in new]
    2:         return block
       
    1:     def summarize(self):
    4:         if self.wrote:
    3:             were = "were"
               else:
    1:             were = "need to be"
    4:         if not self.files:
>>>>>>             self.log_message("No files %s modified.", were)
               else:
    4:             self.log_message("Files that %s modified:", were)
   11:             for file in self.files:
    7:                 self.log_message(file)
    4:         if self.fixer_log:
>>>>>>             self.log_message("Warnings/messages while refactoring:")
>>>>>>             for message in self.fixer_log:
>>>>>>                 self.log_message(message)
    4:         if self.errors:
>>>>>>             if len(self.errors) == 1:
>>>>>>                 self.log_message("There was 1 error:")
                   else:
>>>>>>                 self.log_message("There were %d errors:", len(self.errors))
>>>>>>             for msg, args, kwds in self.errors:
>>>>>>                 self.log_message(msg, *args, **kwds)
       
    1:     def parse_block(self, block, lineno, indent):
               """Parses a block into a tree.
       
               This is necessary to get correct line number / offset information
               in the parser diagnostics and embedded into the parse tree.
               """
    2:         tree = self.driver.parse_tokens(self.wrap_toks(block, lineno, indent))
    2:         tree.future_features = frozenset()
    2:         return tree
       
    1:     def wrap_toks(self, block, lineno, indent):
               """Wraps a tokenize stream to systematically modify start/end."""
    2:         tokens = tokenize.generate_tokens(self.gen_lines(block, indent).next)
   17:         for type, value, (line0, col0), (line1, col1), line_text in tokens:
   17:             line0 += lineno - 1
   17:             line1 += lineno - 1
                   # Don't bother updating the columns; this is too complicated
                   # since line_text would also have to be updated and it would
                   # still break for tokens spanning lines.  Let the user guess
                   # that the column numbers for doctests are relative to the
                   # end of the prompt string (PS1 or PS2).
   17:             yield type, value, (line0, col0), (line1, col1), line_text
       
       
    1:     def gen_lines(self, block, indent):
               """Generates lines as expected by tokenize from a list of lines.
       
               This strips the first len(indent + self.PS1) characters off each line.
               """
    2:         prefix1 = indent + self.PS1
    2:         prefix2 = indent + self.PS2
    2:         prefix = prefix1
    5:         for line in block:
    3:             if line.startswith(prefix):
    3:                 yield line[len(prefix):]
>>>>>>             elif line == prefix.rstrip() + u"\n":
>>>>>>                 yield u"\n"
                   else:
>>>>>>                 raise AssertionError("line=%r, prefix=%r" % (line, prefix))
    3:             prefix = prefix2
    2:         while True:
    2:             yield ""
       
       
    2: class MultiprocessingUnsupported(Exception):
    1:     pass
       
       
    2: class MultiprocessRefactoringTool(RefactoringTool):
       
    1:     def __init__(self, *args, **kwargs):
    4:         super(MultiprocessRefactoringTool, self).__init__(*args, **kwargs)
    4:         self.queue = None
    4:         self.output_lock = None
       
    1:     def refactor(self, items, write=False, doctests_only=False,
    1:                  num_processes=1):
    3:         if num_processes == 1:
    3:             return super(MultiprocessRefactoringTool, self).refactor(
    3:                 items, write, doctests_only)
>>>>>>         try:
>>>>>>             import multiprocessing
>>>>>>         except ImportError:
>>>>>>             raise MultiprocessingUnsupported
>>>>>>         if self.queue is not None:
>>>>>>             raise RuntimeError("already doing multiple processes")
>>>>>>         self.queue = multiprocessing.JoinableQueue()
>>>>>>         self.output_lock = multiprocessing.Lock()
>>>>>>         processes = [multiprocessing.Process(target=self._child)
>>>>>>                      for i in xrange(num_processes)]
>>>>>>         try:
>>>>>>             for p in processes:
>>>>>>                 p.start()
>>>>>>             super(MultiprocessRefactoringTool, self).refactor(items, write,
>>>>>>                                                               doctests_only)
               finally:
>>>>>>             self.queue.join()
>>>>>>             for i in xrange(num_processes):
>>>>>>                 self.queue.put(None)
>>>>>>             for p in processes:
>>>>>>                 if p.is_alive():
>>>>>>                     p.join()
>>>>>>             self.queue = None
       
    1:     def _child(self):
>>>>>>         task = self.queue.get()
>>>>>>         while task is not None:
>>>>>>             args, kwargs = task
>>>>>>             try:
>>>>>>                 super(MultiprocessRefactoringTool, self).refactor_file(
>>>>>>                     *args, **kwargs)
                   finally:
>>>>>>                 self.queue.task_done()
>>>>>>             task = self.queue.get()
       
    1:     def refactor_file(self, *args, **kwargs):
    6:         if self.queue is not None:
>>>>>>             self.queue.put((args, kwargs))
               else:
    6:             return super(MultiprocessRefactoringTool, self).refactor_file(
    6:                 *args, **kwargs)
