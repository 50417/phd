       #
       # Unit tests for the multiprocessing package
       #
       
    2: import unittest
    2: import Queue
    2: import time
    2: import sys
    2: import os
    2: import gc
    2: import signal
    2: import array
    2: import socket
    2: import random
    2: import logging
    2: import errno
    2: import weakref
    2: import test.script_helper
    2: from test import test_support
    2: from StringIO import StringIO
    2: _multiprocessing = test_support.import_module('_multiprocessing')
       # import threading after _multiprocessing to raise a more relevant error
       # message: "No module named _multiprocessing". _multiprocessing is not compiled
       # without thread support.
    2: import threading
       
       # Work around broken sem_open implementations
    2: test_support.import_module('multiprocessing.synchronize')
       
    2: import multiprocessing.dummy
    2: import multiprocessing.connection
    2: import multiprocessing.managers
    2: import multiprocessing.heap
    2: import multiprocessing.pool
       
    2: from multiprocessing import util
       
    2: try:
    2:     from multiprocessing import reduction
    2:     HAS_REDUCTION = True
>>>>>> except ImportError:
>>>>>>     HAS_REDUCTION = False
       
    2: try:
    2:     from multiprocessing.sharedctypes import Value, copy
    2:     HAS_SHAREDCTYPES = True
>>>>>> except ImportError:
>>>>>>     HAS_SHAREDCTYPES = False
       
    2: try:
    2:     import msvcrt
    2: except ImportError:
    2:     msvcrt = None
       
       #
       #
       #
       
    2: latin = str
       
       #
       # Constants
       #
       
    2: LOG_LEVEL = util.SUBWARNING
       #LOG_LEVEL = logging.DEBUG
       
    2: DELTA = 0.1
    2: CHECK_TIMINGS = False     # making true makes tests take a lot longer
                                 # and can sometimes cause some non-serious
                                 # failures because some calls block a bit
                                 # longer than expected
    2: if CHECK_TIMINGS:
>>>>>>     TIMEOUT1, TIMEOUT2, TIMEOUT3 = 0.82, 0.35, 1.4
       else:
    2:     TIMEOUT1, TIMEOUT2, TIMEOUT3 = 0.1, 0.1, 0.1
       
    2: HAVE_GETVALUE = not getattr(_multiprocessing,
    2:                             'HAVE_BROKEN_SEM_GETVALUE', False)
       
    2: WIN32 = (sys.platform == "win32")
       
    2: try:
    2:     MAXFD = os.sysconf("SC_OPEN_MAX")
>>>>>> except:
>>>>>>     MAXFD = 256
       
       #
       # Some tests require ctypes
       #
       
    2: try:
    2:     from ctypes import Structure, c_int, c_double
>>>>>> except ImportError:
>>>>>>     Structure = object
>>>>>>     c_int = c_double = None
       
       
    2: def check_enough_semaphores():
           """Check that the system supports enough semaphores to run the test."""
           # minimum number of semaphores available according to POSIX
    1:     nsems_min = 256
    1:     try:
    1:         nsems = os.sysconf("SC_SEM_NSEMS_MAX")
>>>>>>     except (AttributeError, ValueError):
               # sysconf not available or setting not available
>>>>>>         return
    1:     if nsems == -1 or nsems >= nsems_min:
    1:         return
>>>>>>     raise unittest.SkipTest("The OS doesn't support enough semaphores "
>>>>>>                             "to run the test (required: %d)." % nsems_min)
       
       
       #
       # Creates a wrapper for a function which records the time it takes to finish
       #
       
    4: class TimingWrapper(object):
       
    2:     def __init__(self, func):
   31:         self.func = func
   31:         self.elapsed = None
       
    2:     def __call__(self, *args, **kwds):
   75:         t = time.time()
   75:         try:
   75:             return self.func(*args, **kwds)
               finally:
   75:             self.elapsed = time.time() - t
       
       #
       # Base class for test cases
       #
       
    4: class BaseTestCase(object):
       
    2:     ALLOWED_TYPES = ('processes', 'manager', 'threads')
       
    2:     def assertTimingAlmostEqual(self, a, b):
   69:         if CHECK_TIMINGS:
>>>>>>             self.assertAlmostEqual(a, b, 1)
       
    2:     def assertReturnsIfImplemented(self, value, func, *args):
   60:         try:
   60:             res = func(*args)
   40:         except NotImplementedError:
   40:             pass
               else:
   20:             return self.assertEqual(value, res)
       
           # For the sanity of Windows users, rather than crashing or freezing in
           # multiple ways.
    2:     def __reduce__(self, *args):
>>>>>>         raise NotImplementedError("shouldn't try to pickle a test case")
       
    2:     __reduce_ex__ = __reduce__
       
       #
       # Return the value of a semaphore
       #
       
    2: def get_value(self):
   64:     try:
   64:         return self.get_value()
   64:     except AttributeError:
   43:         try:
   43:             return self._Semaphore__value
   21:         except AttributeError:
   21:             try:
   21:                 return self._value
   21:             except AttributeError:
   21:                 raise NotImplementedError
       
       #
       # Testcases
       #
       
    4: class DummyCallable(object):
    2:     def __call__(self, q, c):
    1:         assert isinstance(c, DummyCallable)
    1:         q.put(5)
       
       
    4: class _TestProcess(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes', 'threads')
       
    2:     def test_current(self):
    2:         if self.TYPE == 'threads':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    1:         current = self.current_process()
    1:         authkey = current.authkey
       
    1:         self.assertTrue(current.is_alive())
    1:         self.assertTrue(not current.daemon)
    1:         self.assertIsInstance(authkey, bytes)
    1:         self.assertTrue(len(authkey) > 0)
    1:         self.assertEqual(current.ident, os.getpid())
    1:         self.assertEqual(current.exitcode, None)
       
    2:     @classmethod
           def _test(cls, q, *args, **kwds):
    1:         current = cls.current_process()
    1:         q.put(args)
    1:         q.put(kwds)
    1:         q.put(current.name)
    1:         if cls.TYPE != 'threads':
>>>>>>             q.put(bytes(current.authkey))
>>>>>>             q.put(current.pid)
       
    2:     def test_process(self):
    2:         q = self.Queue(1)
    2:         e = self.Event()
    2:         args = (q, 1, 2)
    2:         kwargs = {'hello':23, 'bye':2.54}
    2:         name = 'SomeProcess'
    2:         p = self.Process(
    2:             target=self._test, args=args, kwargs=kwargs, name=name
                   )
    2:         p.daemon = True
    2:         current = self.current_process()
       
    2:         if self.TYPE != 'threads':
    1:             self.assertEqual(p.authkey, current.authkey)
    2:         self.assertEqual(p.is_alive(), False)
    2:         self.assertEqual(p.daemon, True)
    2:         self.assertNotIn(p, self.active_children())
    2:         self.assertTrue(type(self.active_children()) is list)
    2:         self.assertEqual(p.exitcode, None)
       
    2:         p.start()
       
    2:         self.assertEqual(p.exitcode, None)
    2:         self.assertEqual(p.is_alive(), True)
    2:         self.assertIn(p, self.active_children())
       
    2:         self.assertEqual(q.get(), args[1:])
    2:         self.assertEqual(q.get(), kwargs)
    2:         self.assertEqual(q.get(), p.name)
    2:         if self.TYPE != 'threads':
    1:             self.assertEqual(q.get(), current.authkey)
    1:             self.assertEqual(q.get(), p.pid)
       
    2:         p.join()
       
    2:         self.assertEqual(p.exitcode, 0)
    2:         self.assertEqual(p.is_alive(), False)
    2:         self.assertNotIn(p, self.active_children())
       
    2:     @classmethod
           def _test_terminate(cls):
>>>>>>         time.sleep(1000)
       
    2:     def test_terminate(self):
    2:         if self.TYPE == 'threads':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    1:         p = self.Process(target=self._test_terminate)
    1:         p.daemon = True
    1:         p.start()
       
    1:         self.assertEqual(p.is_alive(), True)
    1:         self.assertIn(p, self.active_children())
    1:         self.assertEqual(p.exitcode, None)
       
    1:         p.terminate()
       
    1:         join = TimingWrapper(p.join)
    1:         self.assertEqual(join(), None)
    1:         self.assertTimingAlmostEqual(join.elapsed, 0.0)
       
    1:         self.assertEqual(p.is_alive(), False)
    1:         self.assertNotIn(p, self.active_children())
       
    1:         p.join()
       
               # XXX sometimes get p.exitcode == 0 on Windows ...
               #self.assertEqual(p.exitcode, -signal.SIGTERM)
       
    2:     def test_cpu_count(self):
    2:         try:
    2:             cpus = multiprocessing.cpu_count()
>>>>>>         except NotImplementedError:
>>>>>>             cpus = 1
    2:         self.assertTrue(type(cpus) is int)
    2:         self.assertTrue(cpus >= 1)
       
    2:     def test_active_children(self):
    2:         self.assertEqual(type(self.active_children()), list)
       
    2:         p = self.Process(target=time.sleep, args=(DELTA,))
    2:         self.assertNotIn(p, self.active_children())
       
    2:         p.daemon = True
    2:         p.start()
    2:         self.assertIn(p, self.active_children())
       
    2:         p.join()
    2:         self.assertNotIn(p, self.active_children())
       
    2:     @classmethod
           def _test_recursion(cls, wconn, id):
    8:         from multiprocessing import forking
    8:         wconn.send(id)
    8:         if len(id) < 2:
   12:             for i in range(2):
    8:                 p = cls.Process(
    8:                     target=cls._test_recursion, args=(wconn, id+[i])
                           )
    8:                 p.start()
    8:                 p.join()
       
    2:     def test_recursion(self):
    2:         rconn, wconn = self.Pipe(duplex=False)
    2:         self._test_recursion(wconn, [])
       
    2:         time.sleep(DELTA)
    2:         result = []
   16:         while rconn.poll():
   14:             result.append(rconn.recv())
       
               expected = [
    2:             [],
    2:               [0],
    2:                 [0, 0],
    2:                 [0, 1],
    2:               [1],
    2:                 [1, 0],
    2:                 [1, 1]
                   ]
    2:         self.assertEqual(result, expected)
       
    2:     @classmethod
           def _test_sys_exit(cls, reason, testfn):
>>>>>>         sys.stderr = open(testfn, 'w')
>>>>>>         sys.exit(reason)
       
    2:     def test_sys_exit(self):
               # See Issue 13854
    2:         if self.TYPE == 'threads':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    1:         testfn = test_support.TESTFN
    1:         self.addCleanup(test_support.unlink, testfn)
       
    3:         for reason, code in (([1, 2, 3], 1), ('ignore this', 1)):
    2:             p = self.Process(target=self._test_sys_exit, args=(reason, testfn))
    2:             p.daemon = True
    2:             p.start()
    2:             p.join(5)
    2:             self.assertEqual(p.exitcode, code)
       
    2:             with open(testfn, 'r') as f:
    2:                 self.assertEqual(f.read().rstrip(), str(reason))
       
    4:         for reason in (True, False, 8):
    3:             p = self.Process(target=sys.exit, args=(reason,))
    3:             p.daemon = True
    3:             p.start()
    3:             p.join(5)
    3:             self.assertEqual(p.exitcode, reason)
       
    2:     def test_lose_target_ref(self):
    2:         c = DummyCallable()
    2:         wr = weakref.ref(c)
    2:         q = self.Queue()
    2:         p = self.Process(target=c, args=(q, c))
    2:         del c
    2:         p.start()
    2:         p.join()
    2:         self.assertIs(wr(), None)
    2:         self.assertEqual(q.get(), 5)
       
       
       #
       #
       #
       
    4: class _UpperCaser(multiprocessing.Process):
       
    2:     def __init__(self):
    1:         multiprocessing.Process.__init__(self)
    1:         self.child_conn, self.parent_conn = multiprocessing.Pipe()
       
    2:     def run(self):
>>>>>>         self.parent_conn.close()
>>>>>>         for s in iter(self.child_conn.recv, None):
>>>>>>             self.child_conn.send(s.upper())
>>>>>>         self.child_conn.close()
       
    2:     def submit(self, s):
    2:         assert type(s) is str
    2:         self.parent_conn.send(s)
    2:         return self.parent_conn.recv()
       
    2:     def stop(self):
    1:         self.parent_conn.send(None)
    1:         self.parent_conn.close()
    1:         self.child_conn.close()
       
    4: class _TestSubclassingProcess(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def test_subclassing(self):
    1:         uppercaser = _UpperCaser()
    1:         uppercaser.daemon = True
    1:         uppercaser.start()
    1:         self.assertEqual(uppercaser.submit('hello'), 'HELLO')
    1:         self.assertEqual(uppercaser.submit('world'), 'WORLD')
    1:         uppercaser.stop()
    1:         uppercaser.join()
       
       #
       #
       #
       
    2: def queue_empty(q):
   18:     if hasattr(q, 'empty'):
   18:         return q.empty()
           else:
>>>>>>         return q.qsize() == 0
       
    2: def queue_full(q, maxsize):
    9:     if hasattr(q, 'full'):
    9:         return q.full()
           else:
>>>>>>         return q.qsize() == maxsize
       
       
    4: class _TestQueue(BaseTestCase):
       
       
    2:     @classmethod
           def _test_put(cls, queue, child_can_start, parent_can_continue):
    1:         child_can_start.wait()
    7:         for i in range(6):
    6:             queue.get()
    1:         parent_can_continue.set()
       
    2:     def test_put(self):
    3:         MAXSIZE = 6
    3:         queue = self.Queue(maxsize=MAXSIZE)
    3:         child_can_start = self.Event()
    3:         parent_can_continue = self.Event()
       
    3:         proc = self.Process(
    3:             target=self._test_put,
    3:             args=(queue, child_can_start, parent_can_continue)
                   )
    3:         proc.daemon = True
    3:         proc.start()
       
    3:         self.assertEqual(queue_empty(queue), True)
    3:         self.assertEqual(queue_full(queue, MAXSIZE), False)
       
    3:         queue.put(1)
    3:         queue.put(2, True)
    3:         queue.put(3, True, None)
    3:         queue.put(4, False)
    3:         queue.put(5, False, None)
    3:         queue.put_nowait(6)
       
               # the values may be in buffer but not yet in pipe so sleep a bit
    3:         time.sleep(DELTA)
       
    3:         self.assertEqual(queue_empty(queue), False)
    3:         self.assertEqual(queue_full(queue, MAXSIZE), True)
       
    3:         put = TimingWrapper(queue.put)
    3:         put_nowait = TimingWrapper(queue.put_nowait)
       
    3:         self.assertRaises(Queue.Full, put, 7, False)
    3:         self.assertTimingAlmostEqual(put.elapsed, 0)
       
    3:         self.assertRaises(Queue.Full, put, 7, False, None)
    3:         self.assertTimingAlmostEqual(put.elapsed, 0)
       
    3:         self.assertRaises(Queue.Full, put_nowait, 7)
    3:         self.assertTimingAlmostEqual(put_nowait.elapsed, 0)
       
    3:         self.assertRaises(Queue.Full, put, 7, True, TIMEOUT1)
    3:         self.assertTimingAlmostEqual(put.elapsed, TIMEOUT1)
       
    3:         self.assertRaises(Queue.Full, put, 7, False, TIMEOUT2)
    3:         self.assertTimingAlmostEqual(put.elapsed, 0)
       
    3:         self.assertRaises(Queue.Full, put, 7, True, timeout=TIMEOUT3)
    3:         self.assertTimingAlmostEqual(put.elapsed, TIMEOUT3)
       
    3:         child_can_start.set()
    3:         parent_can_continue.wait()
       
    3:         self.assertEqual(queue_empty(queue), True)
    3:         self.assertEqual(queue_full(queue, MAXSIZE), False)
       
    3:         proc.join()
       
    2:     @classmethod
           def _test_get(cls, queue, child_can_start, parent_can_continue):
    1:         child_can_start.wait()
               #queue.put(1)
    1:         queue.put(2)
    1:         queue.put(3)
    1:         queue.put(4)
    1:         queue.put(5)
    1:         parent_can_continue.set()
       
    2:     def test_get(self):
    3:         queue = self.Queue()
    3:         child_can_start = self.Event()
    3:         parent_can_continue = self.Event()
       
    3:         proc = self.Process(
    3:             target=self._test_get,
    3:             args=(queue, child_can_start, parent_can_continue)
                   )
    3:         proc.daemon = True
    3:         proc.start()
       
    3:         self.assertEqual(queue_empty(queue), True)
       
    3:         child_can_start.set()
    3:         parent_can_continue.wait()
       
    3:         time.sleep(DELTA)
    3:         self.assertEqual(queue_empty(queue), False)
       
               # Hangs unexpectedly, remove for now
               #self.assertEqual(queue.get(), 1)
    3:         self.assertEqual(queue.get(True, None), 2)
    3:         self.assertEqual(queue.get(True), 3)
    3:         self.assertEqual(queue.get(timeout=1), 4)
    3:         self.assertEqual(queue.get_nowait(), 5)
       
    3:         self.assertEqual(queue_empty(queue), True)
       
    3:         get = TimingWrapper(queue.get)
    3:         get_nowait = TimingWrapper(queue.get_nowait)
       
    3:         self.assertRaises(Queue.Empty, get, False)
    3:         self.assertTimingAlmostEqual(get.elapsed, 0)
       
    3:         self.assertRaises(Queue.Empty, get, False, None)
    3:         self.assertTimingAlmostEqual(get.elapsed, 0)
       
    3:         self.assertRaises(Queue.Empty, get_nowait)
    3:         self.assertTimingAlmostEqual(get_nowait.elapsed, 0)
       
    3:         self.assertRaises(Queue.Empty, get, True, TIMEOUT1)
    3:         self.assertTimingAlmostEqual(get.elapsed, TIMEOUT1)
       
    3:         self.assertRaises(Queue.Empty, get, False, TIMEOUT2)
    3:         self.assertTimingAlmostEqual(get.elapsed, 0)
       
    3:         self.assertRaises(Queue.Empty, get, timeout=TIMEOUT3)
    3:         self.assertTimingAlmostEqual(get.elapsed, TIMEOUT3)
       
    3:         proc.join()
       
    2:     @classmethod
           def _test_fork(cls, queue):
   11:         for i in range(10, 20):
   10:             queue.put(i)
               # note that at this point the items may only be buffered, so the
               # process cannot shutdown until the feeder thread has finished
               # pushing items onto the pipe.
       
    2:     def test_fork(self):
               # Old versions of Queue would fail to create a new feeder
               # thread for a forked process if the original process had its
               # own feeder thread.  This test checks that this no longer
               # happens.
       
    3:         queue = self.Queue()
       
               # put items on queue so that main process starts a feeder thread
   33:         for i in range(10):
   30:             queue.put(i)
       
               # wait to make sure thread starts before we fork a new process
    3:         time.sleep(DELTA)
       
               # fork process
    3:         p = self.Process(target=self._test_fork, args=(queue,))
    3:         p.daemon = True
    3:         p.start()
       
               # check that all expected items are in the queue
   63:         for i in range(20):
   60:             self.assertEqual(queue.get(), i)
    3:         self.assertRaises(Queue.Empty, queue.get, False)
       
    3:         p.join()
       
    2:     def test_qsize(self):
    3:         q = self.Queue()
    3:         try:
    3:             self.assertEqual(q.qsize(), 0)
    1:         except NotImplementedError:
    1:             self.skipTest('qsize method not implemented')
    2:         q.put(1)
    2:         self.assertEqual(q.qsize(), 1)
    2:         q.put(5)
    2:         self.assertEqual(q.qsize(), 2)
    2:         q.get()
    2:         self.assertEqual(q.qsize(), 1)
    2:         q.get()
    2:         self.assertEqual(q.qsize(), 0)
       
    2:     @classmethod
           def _test_task_done(cls, q):
   14:         for obj in iter(q.get, None):
   10:             time.sleep(DELTA)
   10:             q.task_done()
       
    2:     def test_task_done(self):
    3:         queue = self.JoinableQueue()
       
    3:         if sys.version_info < (2, 5) and not hasattr(queue, 'task_done'):
>>>>>>             self.skipTest("requires 'queue.task_done()' method")
       
    3:         workers = [self.Process(target=self._test_task_done, args=(queue,))
   15:                    for i in xrange(4)]
       
   15:         for p in workers:
   12:             p.daemon = True
   12:             p.start()
       
   33:         for i in xrange(10):
   30:             queue.put(i)
       
    3:         queue.join()
       
   15:         for p in workers:
   12:             queue.put(None)
       
   15:         for p in workers:
   12:             p.join()
       
    2:     def test_no_import_lock_contention(self):
    3:         with test_support.temp_cwd():
    3:             module_name = 'imported_by_an_imported_module'
    3:             with open(module_name + '.py', 'w') as f:
    3:                 f.write("""if 1:
                           import multiprocessing
       
                           q = multiprocessing.Queue()
                           q.put('knock knock')
                           q.get(timeout=3)
                           q.close()
    3:                 """)
       
    3:             with test_support.DirsOnSysPath(os.getcwd()):
    3:                 try:
    3:                     __import__(module_name)
>>>>>>                 except Queue.Empty:
>>>>>>                     self.fail("Probable regression on import lock contention;"
                                     " see Issue #22853")
       
    2:     def test_queue_feeder_donot_stop_onexc(self):
               # bpo-30414: verify feeder handles exceptions correctly
    3:         if self.TYPE != 'processes':
    2:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    2:         class NotSerializable(object):
    1:             def __reduce__(self):
    1:                 raise AttributeError
    1:         with test.support.captured_stderr():
    1:             q = self.Queue()
    1:             q.put(NotSerializable())
    1:             q.put(True)
    1:             self.assertTrue(q.get(timeout=0.1))
       
       
       #
       #
       #
       
    4: class _TestLock(BaseTestCase):
       
    2:     def test_lock(self):
    3:         lock = self.Lock()
    3:         self.assertEqual(lock.acquire(), True)
    3:         self.assertEqual(lock.acquire(False), False)
    3:         self.assertEqual(lock.release(), None)
    3:         self.assertRaises((ValueError, threading.ThreadError), lock.release)
       
    2:     def test_rlock(self):
    3:         lock = self.RLock()
    3:         self.assertEqual(lock.acquire(), True)
    3:         self.assertEqual(lock.acquire(), True)
    3:         self.assertEqual(lock.acquire(), True)
    3:         self.assertEqual(lock.release(), None)
    3:         self.assertEqual(lock.release(), None)
    3:         self.assertEqual(lock.release(), None)
    3:         self.assertRaises((AssertionError, RuntimeError), lock.release)
       
    2:     def test_lock_context(self):
    3:         with self.Lock():
    3:             pass
       
       
    4: class _TestSemaphore(BaseTestCase):
       
    2:     def _test_semaphore(self, sem):
    6:         self.assertReturnsIfImplemented(2, get_value, sem)
    6:         self.assertEqual(sem.acquire(), True)
    6:         self.assertReturnsIfImplemented(1, get_value, sem)
    6:         self.assertEqual(sem.acquire(), True)
    6:         self.assertReturnsIfImplemented(0, get_value, sem)
    6:         self.assertEqual(sem.acquire(False), False)
    6:         self.assertReturnsIfImplemented(0, get_value, sem)
    6:         self.assertEqual(sem.release(), None)
    6:         self.assertReturnsIfImplemented(1, get_value, sem)
    6:         self.assertEqual(sem.release(), None)
    6:         self.assertReturnsIfImplemented(2, get_value, sem)
       
    2:     def test_semaphore(self):
    3:         sem = self.Semaphore(2)
    3:         self._test_semaphore(sem)
    3:         self.assertEqual(sem.release(), None)
    3:         self.assertReturnsIfImplemented(3, get_value, sem)
    3:         self.assertEqual(sem.release(), None)
    3:         self.assertReturnsIfImplemented(4, get_value, sem)
       
    2:     def test_bounded_semaphore(self):
    3:         sem = self.BoundedSemaphore(2)
    3:         self._test_semaphore(sem)
               # Currently fails on OS/X
               #if HAVE_GETVALUE:
               #    self.assertRaises(ValueError, sem.release)
               #    self.assertReturnsIfImplemented(2, get_value, sem)
       
    2:     def test_timeout(self):
    3:         if self.TYPE != 'processes':
    2:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    1:         sem = self.Semaphore(0)
    1:         acquire = TimingWrapper(sem.acquire)
       
    1:         self.assertEqual(acquire(False), False)
    1:         self.assertTimingAlmostEqual(acquire.elapsed, 0.0)
       
    1:         self.assertEqual(acquire(False, None), False)
    1:         self.assertTimingAlmostEqual(acquire.elapsed, 0.0)
       
    1:         self.assertEqual(acquire(False, TIMEOUT1), False)
    1:         self.assertTimingAlmostEqual(acquire.elapsed, 0)
       
    1:         self.assertEqual(acquire(True, TIMEOUT2), False)
    1:         self.assertTimingAlmostEqual(acquire.elapsed, TIMEOUT2)
       
    1:         self.assertEqual(acquire(timeout=TIMEOUT3), False)
    1:         self.assertTimingAlmostEqual(acquire.elapsed, TIMEOUT3)
       
       
    4: class _TestCondition(BaseTestCase):
       
    2:     @classmethod
    2:     def f(cls, cond, sleeping, woken, timeout=None):
   28:         cond.acquire()
   28:         sleeping.release()
   28:         cond.wait(timeout)
   28:         woken.release()
   28:         cond.release()
       
    2:     def check_invariant(self, cond):
               # this is only supposed to succeed when there are no sleepers
    9:         if self.TYPE == 'processes':
    3:             try:
    3:                 sleepers = (cond._sleeping_count.get_value() -
>>>>>>                             cond._woken_count.get_value())
>>>>>>                 self.assertEqual(sleepers, 0)
>>>>>>                 self.assertEqual(cond._wait_semaphore.get_value(), 0)
    3:             except NotImplementedError:
    3:                 pass
       
    2:     def test_notify(self):
    3:         cond = self.Condition()
    3:         sleeping = self.Semaphore(0)
    3:         woken = self.Semaphore(0)
       
    3:         p = self.Process(target=self.f, args=(cond, sleeping, woken))
    3:         p.daemon = True
    3:         p.start()
       
    3:         p = threading.Thread(target=self.f, args=(cond, sleeping, woken))
    3:         p.daemon = True
    3:         p.start()
       
               # wait for both children to start sleeping
    3:         sleeping.acquire()
    3:         sleeping.acquire()
       
               # check no process/thread has woken up
    3:         time.sleep(DELTA)
    3:         self.assertReturnsIfImplemented(0, get_value, woken)
       
               # wake up one process/thread
    3:         cond.acquire()
    3:         cond.notify()
    3:         cond.release()
       
               # check one process/thread has woken up
    3:         time.sleep(DELTA)
    3:         self.assertReturnsIfImplemented(1, get_value, woken)
       
               # wake up another
    3:         cond.acquire()
    3:         cond.notify()
    3:         cond.release()
       
               # check other has woken up
    3:         time.sleep(DELTA)
    3:         self.assertReturnsIfImplemented(2, get_value, woken)
       
               # check state is not mucked up
    3:         self.check_invariant(cond)
    3:         p.join()
       
    2:     def test_notify_all(self):
    3:         cond = self.Condition()
    3:         sleeping = self.Semaphore(0)
    3:         woken = self.Semaphore(0)
       
               # start some threads/processes which will timeout
   12:         for i in range(3):
    9:             p = self.Process(target=self.f,
    9:                              args=(cond, sleeping, woken, TIMEOUT1))
    9:             p.daemon = True
    9:             p.start()
       
    9:             t = threading.Thread(target=self.f,
    9:                                  args=(cond, sleeping, woken, TIMEOUT1))
    9:             t.daemon = True
    9:             t.start()
       
               # wait for them all to sleep
   21:         for i in xrange(6):
   18:             sleeping.acquire()
       
               # check they have all timed out
   21:         for i in xrange(6):
   18:             woken.acquire()
    3:         self.assertReturnsIfImplemented(0, get_value, woken)
       
               # check state is not mucked up
    3:         self.check_invariant(cond)
       
               # start some more threads/processes
   12:         for i in range(3):
    9:             p = self.Process(target=self.f, args=(cond, sleeping, woken))
    9:             p.daemon = True
    9:             p.start()
       
    9:             t = threading.Thread(target=self.f, args=(cond, sleeping, woken))
    9:             t.daemon = True
    9:             t.start()
       
               # wait for them to all sleep
   21:         for i in xrange(6):
   18:             sleeping.acquire()
       
               # check no process/thread has woken up
    3:         time.sleep(DELTA)
    3:         self.assertReturnsIfImplemented(0, get_value, woken)
       
               # wake them all up
    3:         cond.acquire()
    3:         cond.notify_all()
    3:         cond.release()
       
               # check they have all woken
    4:         for i in range(10):
    4:             try:
    4:                 if get_value(woken) == 6:
    1:                     break
    2:             except NotImplementedError:
    2:                 break
    1:             time.sleep(DELTA)
    3:         self.assertReturnsIfImplemented(6, get_value, woken)
       
               # check state is not mucked up
    3:         self.check_invariant(cond)
       
    2:     def test_timeout(self):
    3:         cond = self.Condition()
    3:         wait = TimingWrapper(cond.wait)
    3:         cond.acquire()
    3:         res = wait(TIMEOUT1)
    3:         cond.release()
    3:         self.assertEqual(res, None)
    3:         self.assertTimingAlmostEqual(wait.elapsed, TIMEOUT1)
       
       
    4: class _TestEvent(BaseTestCase):
       
    2:     @classmethod
           def _test_event(cls, event):
    1:         time.sleep(TIMEOUT2)
    1:         event.set()
       
    2:     def test_event(self):
    3:         event = self.Event()
    3:         wait = TimingWrapper(event.wait)
       
               # Removed temporarily, due to API shear, this does not
               # work with threading._Event objects. is_set == isSet
    3:         self.assertEqual(event.is_set(), False)
       
               # Removed, threading.Event.wait() will return the value of the __flag
               # instead of None. API Shear with the semaphore backed mp.Event
    3:         self.assertEqual(wait(0.0), False)
    3:         self.assertTimingAlmostEqual(wait.elapsed, 0.0)
    3:         self.assertEqual(wait(TIMEOUT1), False)
    3:         self.assertTimingAlmostEqual(wait.elapsed, TIMEOUT1)
       
    3:         event.set()
       
               # See note above on the API differences
    3:         self.assertEqual(event.is_set(), True)
    3:         self.assertEqual(wait(), True)
    3:         self.assertTimingAlmostEqual(wait.elapsed, 0.0)
    3:         self.assertEqual(wait(TIMEOUT1), True)
    3:         self.assertTimingAlmostEqual(wait.elapsed, 0.0)
               # self.assertEqual(event.is_set(), True)
       
    3:         event.clear()
       
               #self.assertEqual(event.is_set(), False)
       
    3:         p = self.Process(target=self._test_event, args=(event,))
    3:         p.daemon = True
    3:         p.start()
    3:         self.assertEqual(wait(), True)
       
       #
       #
       #
       
    4: class _TestValue(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
           codes_values = [
    2:         ('i', 4343, 24234),
    2:         ('d', 3.625, -4.25),
    2:         ('h', -232, 234),
    2:         ('c', latin('x'), latin('y'))
               ]
       
    2:     def setUp(self):
    3:         if not HAS_SHAREDCTYPES:
>>>>>>             self.skipTest("requires multiprocessing.sharedctypes")
       
    2:     @classmethod
           def _test(cls, values):
>>>>>>         for sv, cv in zip(values, cls.codes_values):
>>>>>>             sv.value = cv[2]
       
       
    2:     def test_value(self, raw=False):
    2:         if raw:
    1:             values = [self.RawValue(code, value)
    5:                       for code, value, _ in self.codes_values]
               else:
    1:             values = [self.Value(code, value)
    5:                       for code, value, _ in self.codes_values]
       
   10:         for sv, cv in zip(values, self.codes_values):
    8:             self.assertEqual(sv.value, cv[1])
       
    2:         proc = self.Process(target=self._test, args=(values,))
    2:         proc.daemon = True
    2:         proc.start()
    2:         proc.join()
       
   10:         for sv, cv in zip(values, self.codes_values):
    8:             self.assertEqual(sv.value, cv[2])
       
    2:     def test_rawvalue(self):
    1:         self.test_value(raw=True)
       
    2:     def test_getobj_getlock(self):
    1:         val1 = self.Value('i', 5)
    1:         lock1 = val1.get_lock()
    1:         obj1 = val1.get_obj()
       
    1:         val2 = self.Value('i', 5, lock=None)
    1:         lock2 = val2.get_lock()
    1:         obj2 = val2.get_obj()
       
    1:         lock = self.Lock()
    1:         val3 = self.Value('i', 5, lock=lock)
    1:         lock3 = val3.get_lock()
    1:         obj3 = val3.get_obj()
    1:         self.assertEqual(lock, lock3)
       
    1:         arr4 = self.Value('i', 5, lock=False)
    1:         self.assertFalse(hasattr(arr4, 'get_lock'))
    1:         self.assertFalse(hasattr(arr4, 'get_obj'))
       
    1:         self.assertRaises(AttributeError, self.Value, 'i', 5, lock='navalue')
       
    1:         arr5 = self.RawValue('i', 5)
    1:         self.assertFalse(hasattr(arr5, 'get_lock'))
    1:         self.assertFalse(hasattr(arr5, 'get_obj'))
       
       
    4: class _TestArray(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     @classmethod
           def f(cls, seq):
   20:         for i in range(1, len(seq)):
   18:             seq[i] += seq[i-1]
       
    2:     @unittest.skipIf(c_int is None, "requires _ctypes")
    2:     def test_array(self, raw=False):
    2:         seq = [680, 626, 934, 821, 150, 233, 548, 982, 714, 831]
    2:         if raw:
    1:             arr = self.RawArray('i', seq)
               else:
    1:             arr = self.Array('i', seq)
       
    2:         self.assertEqual(len(arr), len(seq))
    2:         self.assertEqual(arr[3], seq[3])
    2:         self.assertEqual(list(arr[2:7]), list(seq[2:7]))
       
    2:         arr[4:8] = seq[4:8] = array.array('i', [1, 2, 3, 4])
       
    2:         self.assertEqual(list(arr[:]), seq)
       
    2:         self.f(seq)
       
    2:         p = self.Process(target=self.f, args=(arr,))
    2:         p.daemon = True
    2:         p.start()
    2:         p.join()
       
    2:         self.assertEqual(list(arr[:]), seq)
       
    2:     @unittest.skipIf(c_int is None, "requires _ctypes")
           def test_array_from_size(self):
    1:         size = 10
               # Test for zeroing (see issue #11675).
               # The repetition below strengthens the test by increasing the chances
               # of previously allocated non-zero memory being used for the new array
               # on the 2nd and 3rd loops.
    4:         for _ in range(3):
    3:             arr = self.Array('i', size)
    3:             self.assertEqual(len(arr), size)
    3:             self.assertEqual(list(arr), [0] * size)
    3:             arr[:] = range(10)
    3:             self.assertEqual(list(arr), range(10))
    3:             del arr
       
    2:     @unittest.skipIf(c_int is None, "requires _ctypes")
           def test_rawarray(self):
    1:         self.test_array(raw=True)
       
    2:     @unittest.skipIf(c_int is None, "requires _ctypes")
           def test_array_accepts_long(self):
    1:         arr = self.Array('i', 10L)
    1:         self.assertEqual(len(arr), 10)
    1:         raw_arr = self.RawArray('i', 10L)
    1:         self.assertEqual(len(raw_arr), 10)
       
    2:     @unittest.skipIf(c_int is None, "requires _ctypes")
           def test_getobj_getlock_obj(self):
    1:         arr1 = self.Array('i', range(10))
    1:         lock1 = arr1.get_lock()
    1:         obj1 = arr1.get_obj()
       
    1:         arr2 = self.Array('i', range(10), lock=None)
    1:         lock2 = arr2.get_lock()
    1:         obj2 = arr2.get_obj()
       
    1:         lock = self.Lock()
    1:         arr3 = self.Array('i', range(10), lock=lock)
    1:         lock3 = arr3.get_lock()
    1:         obj3 = arr3.get_obj()
    1:         self.assertEqual(lock, lock3)
       
    1:         arr4 = self.Array('i', range(10), lock=False)
    1:         self.assertFalse(hasattr(arr4, 'get_lock'))
    1:         self.assertFalse(hasattr(arr4, 'get_obj'))
    1:         self.assertRaises(AttributeError,
    1:                           self.Array, 'i', range(10), lock='notalock')
       
    1:         arr5 = self.RawArray('i', range(10))
    1:         self.assertFalse(hasattr(arr5, 'get_lock'))
    1:         self.assertFalse(hasattr(arr5, 'get_obj'))
       
       #
       #
       #
       
    4: class _TestContainers(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('manager',)
       
    2:     def test_list(self):
    1:         a = self.list(range(10))
    1:         self.assertEqual(a[:], range(10))
       
    1:         b = self.list()
    1:         self.assertEqual(b[:], [])
       
    1:         b.extend(range(5))
    1:         self.assertEqual(b[:], range(5))
       
    1:         self.assertEqual(b[2], 2)
    1:         self.assertEqual(b[2:10], [2,3,4])
       
    1:         b *= 2
    1:         self.assertEqual(b[:], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4])
       
    1:         self.assertEqual(b + [5, 6], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6])
       
    1:         self.assertEqual(a[:], range(10))
       
    1:         d = [a, b]
    1:         e = self.list(d)
    1:         self.assertEqual(
    1:             e[:],
    1:             [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]]
                   )
       
    1:         f = self.list([a])
    1:         a.append('hello')
    1:         self.assertEqual(f[:], [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'hello']])
       
    2:     def test_dict(self):
    1:         d = self.dict()
    1:         indices = range(65, 70)
    6:         for i in indices:
    5:             d[i] = chr(i)
    7:         self.assertEqual(d.copy(), dict((i, chr(i)) for i in indices))
    1:         self.assertEqual(sorted(d.keys()), indices)
    6:         self.assertEqual(sorted(d.values()), [chr(i) for i in indices])
    6:         self.assertEqual(sorted(d.items()), [(i, chr(i)) for i in indices])
       
    2:     def test_namespace(self):
    1:         n = self.Namespace()
    1:         n.name = 'Bob'
    1:         n.job = 'Builder'
    1:         n._hidden = 'hidden'
    1:         self.assertEqual((n.name, n.job), ('Bob', 'Builder'))
    1:         del n.job
    1:         self.assertEqual(str(n), "Namespace(name='Bob')")
    1:         self.assertTrue(hasattr(n, 'name'))
    1:         self.assertTrue(not hasattr(n, 'job'))
       
       #
       #
       #
       
    2: def sqr(x, wait=0.0):
 9688:     time.sleep(wait)
 9665:     return x*x
       
    2: def identity(x):
   10:     return x
       
    4: class CountedObject(object):
    2:     n_instances = 0
       
    2:     def __new__(cls):
   50:         cls.n_instances += 1
   50:         return object.__new__(cls)
       
    2:     def __del__(self):
   20:         type(self).n_instances -= 1
       
    4: class SayWhenError(ValueError): pass
       
    2: def exception_throwing_generator(total, when):
   64:     for i in range(total):
   64:         if i == when:
   10:             raise SayWhenError("Somebody said when")
   54:         yield i
       
    4: class _TestPool(BaseTestCase):
       
    2:     def test_apply(self):
    3:         papply = self.pool.apply
    3:         self.assertEqual(papply(sqr, (5,)), sqr(5))
    3:         self.assertEqual(papply(sqr, (), {'x':3}), sqr(x=3))
       
    2:     def test_map(self):
    3:         pmap = self.pool.map
    3:         self.assertEqual(pmap(sqr, range(10)), map(sqr, range(10)))
    3:         self.assertEqual(pmap(sqr, range(100), chunksize=20),
    3:                          map(sqr, range(100)))
       
    2:     def test_map_unplicklable(self):
               # Issue #19425 -- failure to pickle should not cause a hang
    3:         if self.TYPE == 'threads':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
    4:         class A(object):
    2:             def __reduce__(self):
   11:                 raise RuntimeError('cannot pickle')
    2:         with self.assertRaises(RuntimeError):
    2:             self.pool.map(sqr, [A()]*10)
       
    2:     def test_map_chunksize(self):
    3:         try:
    3:             self.pool.map_async(sqr, [], chunksize=1).get(timeout=TIMEOUT1)
>>>>>>         except multiprocessing.TimeoutError:
>>>>>>             self.fail("pool.map_async with chunksize stalled on null list")
       
    2:     def test_async(self):
    3:         res = self.pool.apply_async(sqr, (7, TIMEOUT1,))
    3:         get = TimingWrapper(res.get)
    3:         self.assertEqual(get(), 49)
    3:         self.assertTimingAlmostEqual(get.elapsed, TIMEOUT1)
       
    2:     def test_async_timeout(self):
    3:         res = self.pool.apply_async(sqr, (6, TIMEOUT2 + 1.0))
    3:         get = TimingWrapper(res.get)
    3:         self.assertRaises(multiprocessing.TimeoutError, get, timeout=TIMEOUT2)
    3:         self.assertTimingAlmostEqual(get.elapsed, TIMEOUT2)
       
    2:     def test_imap(self):
    3:         it = self.pool.imap(sqr, range(10))
    3:         self.assertEqual(list(it), map(sqr, range(10)))
       
    3:         it = self.pool.imap(sqr, range(10))
   33:         for i in range(10):
   30:             self.assertEqual(it.next(), i*i)
    3:         self.assertRaises(StopIteration, it.next)
       
    3:         it = self.pool.imap(sqr, range(1000), chunksize=100)
 3003:         for i in range(1000):
 3000:             self.assertEqual(it.next(), i*i)
    3:         self.assertRaises(StopIteration, it.next)
       
    2:     def test_imap_handle_iterable_exception(self):
    3:         if self.TYPE == 'manager':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    2:         it = self.pool.imap(sqr, exception_throwing_generator(10, 3), 1)
    8:         for i in range(3):
    6:             self.assertEqual(next(it), i*i)
    2:         self.assertRaises(SayWhenError, it.next)
       
               # SayWhenError seen at start of problematic chunk's results
    2:         it = self.pool.imap(sqr, exception_throwing_generator(20, 7), 2)
   14:         for i in range(6):
   12:             self.assertEqual(next(it), i*i)
    2:         self.assertRaises(SayWhenError, it.next)
    2:         it = self.pool.imap(sqr, exception_throwing_generator(20, 7), 4)
   10:         for i in range(4):
    8:             self.assertEqual(next(it), i*i)
    2:         self.assertRaises(SayWhenError, it.next)
       
    2:     def test_imap_unordered(self):
    3:         it = self.pool.imap_unordered(sqr, range(1000))
    3:         self.assertEqual(sorted(it), map(sqr, range(1000)))
       
    3:         it = self.pool.imap_unordered(sqr, range(1000), chunksize=53)
    3:         self.assertEqual(sorted(it), map(sqr, range(1000)))
       
    2:     def test_imap_unordered_handle_iterable_exception(self):
    3:         if self.TYPE == 'manager':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    2:         it = self.pool.imap_unordered(sqr,
    2:                                       exception_throwing_generator(10, 3),
    2:                                       1)
    2:         expected_values = map(sqr, range(10))
    2:         with self.assertRaises(SayWhenError):
                   # imap_unordered makes it difficult to anticipate the SayWhenError
    3:             for i in range(10):
    3:                 value = next(it)
    1:                 self.assertIn(value, expected_values)
    1:                 expected_values.remove(value)
       
    2:         it = self.pool.imap_unordered(sqr,
    2:                                       exception_throwing_generator(20, 7),
    2:                                       2)
    2:         expected_values = map(sqr, range(20))
    2:         with self.assertRaises(SayWhenError):
    4:             for i in range(20):
    4:                 value = next(it)
    2:                 self.assertIn(value, expected_values)
    2:                 expected_values.remove(value)
       
    2:     def test_make_pool(self):
    3:         self.assertRaises(ValueError, multiprocessing.Pool, -1)
    3:         self.assertRaises(ValueError, multiprocessing.Pool, 0)
       
    3:         p = multiprocessing.Pool(3)
    3:         self.assertEqual(3, len(p._pool))
    3:         p.close()
    3:         p.join()
       
    2:     def test_terminate(self):
    3:         p = self.Pool(4)
    3:         result = p.map_async(
30003:             time.sleep, [0.1 for i in range(10000)], chunksize=1
                   )
    3:         p.terminate()
    3:         join = TimingWrapper(p.join)
    3:         join()
    3:         self.assertTrue(join.elapsed < 0.2)
       
    2:     def test_empty_iterable(self):
               # See Issue 12157
    3:         p = self.Pool(1)
       
    3:         self.assertEqual(p.map(sqr, []), [])
    3:         self.assertEqual(list(p.imap(sqr, [])), [])
    3:         self.assertEqual(list(p.imap_unordered(sqr, [])), [])
    3:         self.assertEqual(p.map_async(sqr, []).get(), [])
       
    3:         p.close()
    3:         p.join()
       
    2:     def test_release_task_refs(self):
               # Issue #29861: task arguments and results should not be kept
               # alive after we are done with them.
   36:         objs = list(CountedObject() for i in range(10))
   36:         refs = list(weakref.ref(o) for o in objs)
    3:         self.pool.map(identity, objs)
       
    3:         del objs
    3:         time.sleep(DELTA)  # let threaded cleanup code run
   36:         self.assertEqual(set(wr() for wr in refs), {None})
               # With a process pool, copies of the objects are returned, check
               # they were released too.
    3:         self.assertEqual(CountedObject.n_instances, 0)
       
       
    2: def unpickleable_result():
>>>>>>     return lambda: 42
       
    4: class _TestPoolWorkerErrors(BaseTestCase):
    2:     ALLOWED_TYPES = ('processes', )
       
    2:     def test_unpickleable_result(self):
    1:         from multiprocessing.pool import MaybeEncodingError
    1:         p = multiprocessing.Pool(2)
       
               # Make sure we don't lose pool processes because of encoding errors.
   21:         for iteration in range(20):
   20:             res = p.apply_async(unpickleable_result)
   20:             self.assertRaises(MaybeEncodingError, res.get)
       
    1:         p.close()
    1:         p.join()
       
    4: class _TestPoolWorkerLifetime(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes', )
    2:     def test_pool_worker_lifetime(self):
    1:         p = multiprocessing.Pool(3, maxtasksperchild=10)
    1:         self.assertEqual(3, len(p._pool))
    4:         origworkerpids = [w.pid for w in p._pool]
               # Run many tasks so each worker gets replaced (hopefully)
    1:         results = []
  101:         for i in range(100):
  100:             results.append(p.apply_async(sqr, (i, )))
               # Fetch the results and verify we got the right answers,
               # also ensuring all the tasks have completed.
  101:         for (j, res) in enumerate(results):
  100:             self.assertEqual(res.get(), sqr(j))
               # Refill the pool
    1:         p._repopulate_pool()
               # Wait until all workers are alive
               # (countdown * DELTA = 5 seconds max startup process time)
    1:         countdown = 50
    5:         while countdown and not all(w.is_alive() for w in p._pool):
>>>>>>             countdown -= 1
>>>>>>             time.sleep(DELTA)
    4:         finalworkerpids = [w.pid for w in p._pool]
               # All pids should be assigned.  See issue #7805.
    1:         self.assertNotIn(None, origworkerpids)
    1:         self.assertNotIn(None, finalworkerpids)
               # Finally, check that the worker pids have changed
    1:         self.assertNotEqual(sorted(origworkerpids), sorted(finalworkerpids))
    1:         p.close()
    1:         p.join()
       
    2:     def test_pool_worker_lifetime_early_close(self):
               # Issue #10332: closing a pool whose workers have limited lifetimes
               # before all the tasks completed would make join() hang.
    1:         p = multiprocessing.Pool(3, maxtasksperchild=1)
    1:         results = []
    7:         for i in range(6):
    6:             results.append(p.apply_async(sqr, (i, 0.3)))
    1:         p.close()
    1:         p.join()
               # check the results
    7:         for (j, res) in enumerate(results):
    6:             self.assertEqual(res.get(), sqr(j))
       
       
       #
       # Test that manager has expected number of shared objects left
       #
       
    4: class _TestZZZNumberOfObjects(BaseTestCase):
           # Because test cases are sorted alphabetically, this one will get
           # run after all the other tests for the manager.  It tests that
           # there have been no "reference leaks" for the manager's shared
           # objects.  Note the comment in _TestPool.test_terminate().
    2:     ALLOWED_TYPES = ('manager',)
       
    2:     def test_number_of_objects(self):
    1:         EXPECTED_NUMBER = 1                # the pool object is still alive
    1:         multiprocessing.active_children()  # discard dead process objs
    1:         gc.collect()                       # do garbage collection
    1:         refs = self.manager._number_of_objects()
    1:         debug_info = self.manager._debug_info()
    1:         if refs != EXPECTED_NUMBER:
>>>>>>             print self.manager._debug_info()
>>>>>>             print debug_info
       
    1:         self.assertEqual(refs, EXPECTED_NUMBER)
       
       #
       # Test of creating a customized manager class
       #
       
    2: from multiprocessing.managers import BaseManager, BaseProxy, RemoteError
       
    4: class FooBar(object):
    2:     def f(self):
>>>>>>         return 'f()'
    2:     def g(self):
>>>>>>         raise ValueError
    2:     def _h(self):
>>>>>>         return '_h()'
       
    2: def baz():
>>>>>>     for i in xrange(10):
>>>>>>         yield i*i
       
    4: class IteratorProxy(BaseProxy):
    2:     _exposed_ = ('next', '__next__')
    2:     def __iter__(self):
    1:         return self
    2:     def next(self):
   11:         return self._callmethod('next')
    2:     def __next__(self):
>>>>>>         return self._callmethod('__next__')
       
    4: class MyManager(BaseManager):
    2:     pass
       
    2: MyManager.register('Foo', callable=FooBar)
    2: MyManager.register('Bar', callable=FooBar, exposed=('f', '_h'))
    2: MyManager.register('baz', callable=baz, proxytype=IteratorProxy)
       
       
    4: class _TestMyManager(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('manager',)
       
    2:     def test_mymanager(self):
    1:         manager = MyManager()
    1:         manager.start()
       
    1:         foo = manager.Foo()
    1:         bar = manager.Bar()
    1:         baz = manager.baz()
       
    4:         foo_methods = [name for name in ('f', 'g', '_h') if hasattr(foo, name)]
    4:         bar_methods = [name for name in ('f', 'g', '_h') if hasattr(bar, name)]
       
    1:         self.assertEqual(foo_methods, ['f', 'g'])
    1:         self.assertEqual(bar_methods, ['f', '_h'])
       
    1:         self.assertEqual(foo.f(), 'f()')
    1:         self.assertRaises(ValueError, foo.g)
    1:         self.assertEqual(foo._callmethod('f'), 'f()')
    1:         self.assertRaises(RemoteError, foo._callmethod, '_h')
       
    1:         self.assertEqual(bar.f(), 'f()')
    1:         self.assertEqual(bar._h(), '_h()')
    1:         self.assertEqual(bar._callmethod('f'), 'f()')
    1:         self.assertEqual(bar._callmethod('_h'), '_h()')
       
   11:         self.assertEqual(list(baz), [i*i for i in range(10)])
       
    1:         manager.shutdown()
       
       #
       # Test of connecting to a remote server and using xmlrpclib for serialization
       #
       
    2: _queue = Queue.Queue()
    2: def get_queue():
>>>>>>     return _queue
       
    4: class QueueManager(BaseManager):
    2:     '''manager class used by server process'''
    2: QueueManager.register('get_queue', callable=get_queue)
       
    4: class QueueManager2(BaseManager):
    2:     '''manager class which specifies the same interface as QueueManager'''
    2: QueueManager2.register('get_queue')
       
       
    2: SERIALIZER = 'xmlrpclib'
       
    4: class _TestRemoteManager(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('manager',)
    2:     values = ['hello world', None, True, 2.25,
                     #'hall\xc3\xa5 v\xc3\xa4rlden'] # UTF-8
                     ]
    2:     result = values[:]
    2:     if test_support.have_unicode:
               #result[-1] = u'hall\xe5 v\xe4rlden'
    2:         uvalue = test_support.u(r'\u043f\u0440\u0438\u0432\u0456\u0442 '
                                       r'\u0441\u0432\u0456\u0442')
    2:         values.append(uvalue)
    2:         result.append(uvalue)
       
    2:     @classmethod
           def _putter(cls, address, authkey):
>>>>>>         manager = QueueManager2(
>>>>>>             address=address, authkey=authkey, serializer=SERIALIZER
                   )
>>>>>>         manager.connect()
>>>>>>         queue = manager.get_queue()
               # Note that xmlrpclib will deserialize object as a list not a tuple
>>>>>>         queue.put(tuple(cls.values))
       
    2:     def test_remote(self):
    1:         authkey = os.urandom(32)
       
    1:         manager = QueueManager(
    1:             address=(test.test_support.HOST, 0), authkey=authkey, serializer=SERIALIZER
                   )
    1:         manager.start()
       
    1:         p = self.Process(target=self._putter, args=(manager.address, authkey))
    1:         p.daemon = True
    1:         p.start()
       
    1:         manager2 = QueueManager2(
    1:             address=manager.address, authkey=authkey, serializer=SERIALIZER
                   )
    1:         manager2.connect()
    1:         queue = manager2.get_queue()
       
    1:         self.assertEqual(queue.get(), self.result)
       
               # Because we are using xmlrpclib for serialization instead of
               # pickle this will cause a serialization error.
    1:         self.assertRaises(Exception, queue.put, time.sleep)
       
               # Make queue finalizer run before the server is stopped
    1:         del queue
    1:         manager.shutdown()
       
    4: class _TestManagerRestart(BaseTestCase):
       
    2:     @classmethod
           def _putter(cls, address, authkey):
    1:         manager = QueueManager(
    1:             address=address, authkey=authkey, serializer=SERIALIZER)
    1:         manager.connect()
    1:         queue = manager.get_queue()
    1:         queue.put('hello world')
       
    2:     def test_rapid_restart(self):
    3:         authkey = os.urandom(32)
    3:         manager = QueueManager(
    3:             address=(test.test_support.HOST, 0), authkey=authkey, serializer=SERIALIZER)
    3:         srvr = manager.get_server()
    3:         addr = srvr.address
               # Close the connection.Listener socket which gets opened as a part
               # of manager.get_server(). It's not needed for the test.
    3:         srvr.listener.close()
    3:         manager.start()
       
    3:         p = self.Process(target=self._putter, args=(manager.address, authkey))
    3:         p.start()
    3:         p.join()
    3:         queue = manager.get_queue()
    3:         self.assertEqual(queue.get(), 'hello world')
    3:         del queue
    3:         manager.shutdown()
       
    3:         manager = QueueManager(
    3:             address=addr, authkey=authkey, serializer=SERIALIZER)
    3:         manager.start()
    3:         manager.shutdown()
       
       #
       #
       #
       
    2: SENTINEL = latin('')
       
    4: class _TestConnection(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes', 'threads')
       
    2:     @classmethod
           def _echo(cls, conn):
    7:         for msg in iter(conn.recv_bytes, SENTINEL):
    5:             conn.send_bytes(msg)
    2:         conn.close()
       
    2:     def test_connection(self):
    2:         conn, child_conn = self.Pipe()
       
    2:         p = self.Process(target=self._echo, args=(child_conn,))
    2:         p.daemon = True
    2:         p.start()
       
    2:         seq = [1, 2.25, None]
    2:         msg = latin('hello world')
    2:         longmsg = msg * 10
    2:         arr = array.array('i', range(4))
       
    2:         if self.TYPE == 'processes':
    1:             self.assertEqual(type(conn.fileno()), int)
       
    2:         self.assertEqual(conn.send(seq), None)
    2:         self.assertEqual(conn.recv(), seq)
       
    2:         self.assertEqual(conn.send_bytes(msg), None)
    2:         self.assertEqual(conn.recv_bytes(), msg)
       
    2:         if self.TYPE == 'processes':
    1:             buffer = array.array('i', [0]*10)
    1:             expected = list(arr) + [0] * (10 - len(arr))
    1:             self.assertEqual(conn.send_bytes(arr), None)
    1:             self.assertEqual(conn.recv_bytes_into(buffer),
    1:                              len(arr) * buffer.itemsize)
    1:             self.assertEqual(list(buffer), expected)
       
    1:             buffer = array.array('i', [0]*10)
    1:             expected = [0] * 3 + list(arr) + [0] * (10 - 3 - len(arr))
    1:             self.assertEqual(conn.send_bytes(arr), None)
    1:             self.assertEqual(conn.recv_bytes_into(buffer, 3 * buffer.itemsize),
    1:                              len(arr) * buffer.itemsize)
    1:             self.assertEqual(list(buffer), expected)
       
    1:             buffer = bytearray(latin(' ' * 40))
    1:             self.assertEqual(conn.send_bytes(longmsg), None)
    1:             try:
    1:                 res = conn.recv_bytes_into(buffer)
    1:             except multiprocessing.BufferTooShort, e:
    1:                 self.assertEqual(e.args, (longmsg,))
                   else:
>>>>>>                 self.fail('expected BufferTooShort, got %s' % res)
       
    2:         poll = TimingWrapper(conn.poll)
       
    2:         self.assertEqual(poll(), False)
    2:         self.assertTimingAlmostEqual(poll.elapsed, 0)
       
    2:         self.assertEqual(poll(TIMEOUT1), False)
    2:         self.assertTimingAlmostEqual(poll.elapsed, TIMEOUT1)
       
    2:         conn.send(None)
    2:         time.sleep(.1)
       
    2:         self.assertEqual(poll(TIMEOUT1), True)
    2:         self.assertTimingAlmostEqual(poll.elapsed, 0)
       
    2:         self.assertEqual(conn.recv(), None)
       
    2:         really_big_msg = latin('X') * (1024 * 1024 * 16)   # 16Mb
    2:         conn.send_bytes(really_big_msg)
    2:         self.assertEqual(conn.recv_bytes(), really_big_msg)
       
    2:         conn.send_bytes(SENTINEL)                          # tell child to quit
    2:         child_conn.close()
       
    2:         if self.TYPE == 'processes':
    1:             self.assertEqual(conn.readable, True)
    1:             self.assertEqual(conn.writable, True)
    1:             self.assertRaises(EOFError, conn.recv)
    1:             self.assertRaises(EOFError, conn.recv_bytes)
       
    2:         p.join()
       
    2:     def test_duplex_false(self):
    2:         reader, writer = self.Pipe(duplex=False)
    2:         self.assertEqual(writer.send(1), None)
    2:         self.assertEqual(reader.recv(), 1)
    2:         if self.TYPE == 'processes':
    1:             self.assertEqual(reader.readable, True)
    1:             self.assertEqual(reader.writable, False)
    1:             self.assertEqual(writer.readable, False)
    1:             self.assertEqual(writer.writable, True)
    1:             self.assertRaises(IOError, reader.send, 2)
    1:             self.assertRaises(IOError, writer.recv)
    1:             self.assertRaises(IOError, writer.poll)
       
    2:     def test_spawn_close(self):
               # We test that a pipe connection can be closed by parent
               # process immediately after child is spawned.  On Windows this
               # would have sometimes failed on old versions because
               # child_conn would be closed before the child got a chance to
               # duplicate it.
    2:         conn, child_conn = self.Pipe()
       
    2:         p = self.Process(target=self._echo, args=(child_conn,))
    2:         p.daemon = True
    2:         p.start()
    2:         child_conn.close()    # this might complete before child initializes
       
    2:         msg = latin('hello')
    2:         conn.send_bytes(msg)
    2:         self.assertEqual(conn.recv_bytes(), msg)
       
    2:         conn.send_bytes(SENTINEL)
    2:         conn.close()
    2:         p.join()
       
    2:     def test_sendbytes(self):
    2:         if self.TYPE != 'processes':
    1:             self.skipTest('test not appropriate for {}'.format(self.TYPE))
       
    1:         msg = latin('abcdefghijklmnopqrstuvwxyz')
    1:         a, b = self.Pipe()
       
    1:         a.send_bytes(msg)
    1:         self.assertEqual(b.recv_bytes(), msg)
       
    1:         a.send_bytes(msg, 5)
    1:         self.assertEqual(b.recv_bytes(), msg[5:])
       
    1:         a.send_bytes(msg, 7, 8)
    1:         self.assertEqual(b.recv_bytes(), msg[7:7+8])
       
    1:         a.send_bytes(msg, 26)
    1:         self.assertEqual(b.recv_bytes(), latin(''))
       
    1:         a.send_bytes(msg, 26, 0)
    1:         self.assertEqual(b.recv_bytes(), latin(''))
       
    1:         self.assertRaises(ValueError, a.send_bytes, msg, 27)
       
    1:         self.assertRaises(ValueError, a.send_bytes, msg, 22, 5)
       
    1:         self.assertRaises(ValueError, a.send_bytes, msg, 26, 1)
       
    1:         self.assertRaises(ValueError, a.send_bytes, msg, -1)
       
    1:         self.assertRaises(ValueError, a.send_bytes, msg, 4, -1)
       
    2:     @classmethod
           def _is_fd_assigned(cls, fd):
>>>>>>         try:
>>>>>>             os.fstat(fd)
>>>>>>         except OSError as e:
>>>>>>             if e.errno == errno.EBADF:
>>>>>>                 return False
>>>>>>             raise
               else:
>>>>>>             return True
       
    2:     @classmethod
    2:     def _writefd(cls, conn, data, create_dummy_fds=False):
>>>>>>         if create_dummy_fds:
>>>>>>             for i in range(0, 256):
>>>>>>                 if not cls._is_fd_assigned(i):
>>>>>>                     os.dup2(conn.fileno(), i)
>>>>>>         fd = reduction.recv_handle(conn)
>>>>>>         if msvcrt:
>>>>>>             fd = msvcrt.open_osfhandle(fd, os.O_WRONLY)
>>>>>>         os.write(fd, data)
>>>>>>         os.close(fd)
       
    2:     @unittest.skipUnless(HAS_REDUCTION, "test needs multiprocessing.reduction")
           def test_fd_transfer(self):
    2:         if self.TYPE != 'processes':
    1:             self.skipTest("only makes sense with processes")
    1:         conn, child_conn = self.Pipe(duplex=True)
       
    1:         p = self.Process(target=self._writefd, args=(child_conn, b"foo"))
    1:         p.daemon = True
    1:         p.start()
    1:         with open(test_support.TESTFN, "wb") as f:
    1:             fd = f.fileno()
    1:             if msvcrt:
>>>>>>                 fd = msvcrt.get_osfhandle(fd)
    1:             reduction.send_handle(conn, fd, p.pid)
    1:         p.join()
    1:         with open(test_support.TESTFN, "rb") as f:
    1:             self.assertEqual(f.read(), b"foo")
       
    2:     @unittest.skipUnless(HAS_REDUCTION, "test needs multiprocessing.reduction")
    2:     @unittest.skipIf(sys.platform == "win32",
    2:                      "test semantics don't make sense on Windows")
    2:     @unittest.skipIf(MAXFD <= 256,
    2:                      "largest assignable fd number is too small")
    2:     @unittest.skipUnless(hasattr(os, "dup2"),
    2:                          "test needs os.dup2()")
           def test_large_fd_transfer(self):
               # With fd > 256 (issue #11657)
>>>>>>         if self.TYPE != 'processes':
>>>>>>             self.skipTest("only makes sense with processes")
>>>>>>         conn, child_conn = self.Pipe(duplex=True)
       
>>>>>>         p = self.Process(target=self._writefd, args=(child_conn, b"bar", True))
>>>>>>         p.daemon = True
>>>>>>         p.start()
>>>>>>         with open(test_support.TESTFN, "wb") as f:
>>>>>>             fd = f.fileno()
>>>>>>             for newfd in range(256, MAXFD):
>>>>>>                 if not self._is_fd_assigned(newfd):
>>>>>>                     break
                   else:
>>>>>>                 self.fail("could not find an unassigned large file descriptor")
>>>>>>             os.dup2(fd, newfd)
>>>>>>             try:
>>>>>>                 reduction.send_handle(conn, newfd, p.pid)
                   finally:
>>>>>>                 os.close(newfd)
>>>>>>         p.join()
>>>>>>         with open(test_support.TESTFN, "rb") as f:
>>>>>>             self.assertEqual(f.read(), b"bar")
       
    2:     @classmethod
           def _send_data_without_fd(self, conn):
>>>>>>         os.write(conn.fileno(), b"\0")
       
    2:     @unittest.skipUnless(HAS_REDUCTION, "test needs multiprocessing.reduction")
    2:     @unittest.skipIf(sys.platform == "win32", "doesn't make sense on Windows")
           def test_missing_fd_transfer(self):
               # Check that exception is raised when received data is not
               # accompanied by a file descriptor in ancillary data.
    2:         if self.TYPE != 'processes':
    1:             self.skipTest("only makes sense with processes")
    1:         conn, child_conn = self.Pipe(duplex=True)
       
    1:         p = self.Process(target=self._send_data_without_fd, args=(child_conn,))
    1:         p.daemon = True
    1:         p.start()
    1:         self.assertRaises(RuntimeError, reduction.recv_handle, conn)
    1:         p.join()
       
    4: class _TestListenerClient(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes', 'threads')
       
    2:     @classmethod
           def _test(cls, address):
    2:         conn = cls.connection.Client(address)
    2:         conn.send('hello')
    2:         conn.close()
       
    2:     def test_listener_client(self):
    5:         for family in self.connection.families:
    3:             l = self.connection.Listener(family=family)
    3:             p = self.Process(target=self._test, args=(l.address,))
    3:             p.daemon = True
    3:             p.start()
    3:             conn = l.accept()
    3:             self.assertEqual(conn.recv(), 'hello')
    3:             p.join()
    3:             l.close()
       
    2:     def test_issue14725(self):
    2:         l = self.connection.Listener()
    2:         p = self.Process(target=self._test, args=(l.address,))
    2:         p.daemon = True
    2:         p.start()
    2:         time.sleep(1)
               # On Windows the client process should by now have connected,
               # written data and closed the pipe handle by now.  This causes
               # ConnectNamdedPipe() to fail with ERROR_NO_DATA.  See Issue
               # 14725.
    2:         conn = l.accept()
    2:         self.assertEqual(conn.recv(), 'hello')
    2:         conn.close()
    2:         p.join()
    2:         l.close()
       
       #
       # Test of sending connection and socket objects between processes
       #
       """
       class _TestPicklingConnections(BaseTestCase):
       
           ALLOWED_TYPES = ('processes',)
       
           def _listener(self, conn, families):
               for fam in families:
                   l = self.connection.Listener(family=fam)
                   conn.send(l.address)
                   new_conn = l.accept()
                   conn.send(new_conn)
       
               if self.TYPE == 'processes':
                   l = socket.socket()
                   l.bind(('localhost', 0))
                   conn.send(l.getsockname())
                   l.listen(1)
                   new_conn, addr = l.accept()
                   conn.send(new_conn)
       
               conn.recv()
       
           def _remote(self, conn):
               for (address, msg) in iter(conn.recv, None):
                   client = self.connection.Client(address)
                   client.send(msg.upper())
                   client.close()
       
               if self.TYPE == 'processes':
                   address, msg = conn.recv()
                   client = socket.socket()
                   client.connect(address)
                   client.sendall(msg.upper())
                   client.close()
       
               conn.close()
       
           def test_pickling(self):
               try:
                   multiprocessing.allow_connection_pickling()
               except ImportError:
                   return
       
               families = self.connection.families
       
               lconn, lconn0 = self.Pipe()
               lp = self.Process(target=self._listener, args=(lconn0, families))
               lp.daemon = True
               lp.start()
               lconn0.close()
       
               rconn, rconn0 = self.Pipe()
               rp = self.Process(target=self._remote, args=(rconn0,))
               rp.daemon = True
               rp.start()
               rconn0.close()
       
               for fam in families:
                   msg = ('This connection uses family %s' % fam).encode('ascii')
                   address = lconn.recv()
                   rconn.send((address, msg))
                   new_conn = lconn.recv()
                   self.assertEqual(new_conn.recv(), msg.upper())
       
               rconn.send(None)
       
               if self.TYPE == 'processes':
                   msg = latin('This connection uses a normal socket')
                   address = lconn.recv()
                   rconn.send((address, msg))
                   if hasattr(socket, 'fromfd'):
                       new_conn = lconn.recv()
                       self.assertEqual(new_conn.recv(100), msg.upper())
                   else:
                       # XXX On Windows with Py2.6 need to backport fromfd()
                       discard = lconn.recv_bytes()
       
               lconn.send(None)
       
               rconn.close()
               lconn.close()
       
               lp.join()
               rp.join()
       """
       #
       #
       #
       
    4: class _TestHeap(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def test_heap(self):
    1:         iterations = 5000
    1:         maxblocks = 50
    1:         blocks = []
       
               # create and destroy lots of blocks of different sizes
 5001:         for i in xrange(iterations):
 5000:             size = int(random.lognormvariate(0, 1) * 1000)
 5000:             b = multiprocessing.heap.BufferWrapper(size)
 5000:             blocks.append(b)
 5000:             if len(blocks) > maxblocks:
 4950:                 i = random.randrange(maxblocks)
 4950:                 del blocks[i]
       
               # get the heap object
    1:         heap = multiprocessing.heap.BufferWrapper._heap
       
               # verify the state of the heap
    1:         all = []
    1:         occupied = 0
    1:         heap._lock.acquire()
    1:         self.addCleanup(heap._lock.release)
   26:         for L in heap._len_to_seq.values():
   52:             for arena, start, stop in L:
   27:                 all.append((heap._arenas.index(arena), start, stop,
   27:                             stop-start, 'free'))
  251:         for arena, start, stop in heap._allocated_blocks:
  250:             all.append((heap._arenas.index(arena), start, stop,
  250:                         stop-start, 'occupied'))
  250:             occupied += (stop-start)
       
    1:         all.sort()
       
  277:         for i in range(len(all)-1):
  276:             (arena, start, stop) = all[i][:3]
  276:             (narena, nstart, nstop) = all[i+1][:3]
  276:             self.assertTrue((arena != narena and nstart == 0) or
  271:                             (stop == nstart))
       
    2:     def test_free_from_gc(self):
               # Check that freeing of blocks by the garbage collector doesn't deadlock
               # (issue #12352).
               # Make sure the GC is enabled, and set lower collection thresholds to
               # make collections more frequent (and increase the probability of
               # deadlock).
    1:         if not gc.isenabled():
>>>>>>             gc.enable()
>>>>>>             self.addCleanup(gc.disable)
    1:         thresholds = gc.get_threshold()
    1:         self.addCleanup(gc.set_threshold, *thresholds)
    1:         gc.set_threshold(10)
       
               # perform numerous block allocations, with cyclic references to make
               # sure objects are collected asynchronously by the gc
 5001:         for i in range(5000):
 5000:             a = multiprocessing.heap.BufferWrapper(1)
 5000:             b = multiprocessing.heap.BufferWrapper(1)
                   # circular references
 5000:             a.buddy = b
 5000:             b.buddy = a
       
       #
       #
       #
       
    4: class _Foo(Structure):
           _fields_ = [
    2:         ('x', c_int),
    2:         ('y', c_double)
               ]
       
    4: class _TestSharedCTypes(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def setUp(self):
    3:         if not HAS_SHAREDCTYPES:
>>>>>>             self.skipTest("requires multiprocessing.sharedctypes")
       
    2:     @classmethod
           def _double(cls, x, y, foo, arr, string):
>>>>>>         x.value *= 2
>>>>>>         y.value *= 2
>>>>>>         foo.x *= 2
>>>>>>         foo.y *= 2
>>>>>>         string.value *= 2
>>>>>>         for i in range(len(arr)):
>>>>>>             arr[i] *= 2
       
    2:     def test_sharedctypes(self, lock=False):
    2:         x = Value('i', 7, lock=lock)
    2:         y = Value(c_double, 1.0/3.0, lock=lock)
    2:         foo = Value(_Foo, 3, 2, lock=lock)
    2:         arr = self.Array('d', range(10), lock=lock)
    2:         string = self.Array('c', 20, lock=lock)
    2:         string.value = latin('hello')
       
    2:         p = self.Process(target=self._double, args=(x, y, foo, arr, string))
    2:         p.daemon = True
    2:         p.start()
    2:         p.join()
       
    2:         self.assertEqual(x.value, 14)
    2:         self.assertAlmostEqual(y.value, 2.0/3.0)
    2:         self.assertEqual(foo.x, 6)
    2:         self.assertAlmostEqual(foo.y, 4.0)
   22:         for i in range(10):
   20:             self.assertAlmostEqual(arr[i], i*2)
    2:         self.assertEqual(string.value, latin('hellohello'))
       
    2:     def test_synchronize(self):
    1:         self.test_sharedctypes(lock=True)
       
    2:     def test_copy(self):
    1:         foo = _Foo(2, 5.0)
    1:         bar = copy(foo)
    1:         foo.x = 0
    1:         foo.y = 0
    1:         self.assertEqual(bar.x, 2)
    1:         self.assertAlmostEqual(bar.y, 5.0)
       
       #
       #
       #
       
    4: class _TestFinalize(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def setUp(self):
    2:         self.registry_backup = util._finalizer_registry.copy()
    2:         util._finalizer_registry.clear()
       
    2:     def tearDown(self):
    2:         self.assertFalse(util._finalizer_registry)
    2:         util._finalizer_registry.update(self.registry_backup)
       
    2:     @classmethod
           def _test_finalize(cls, conn):
>>>>>>         class Foo(object):
>>>>>>             pass
       
>>>>>>         a = Foo()
>>>>>>         util.Finalize(a, conn.send, args=('a',))
>>>>>>         del a           # triggers callback for a
       
>>>>>>         b = Foo()
>>>>>>         close_b = util.Finalize(b, conn.send, args=('b',))
>>>>>>         close_b()       # triggers callback for b
>>>>>>         close_b()       # does nothing because callback has already been called
>>>>>>         del b           # does nothing because callback has already been called
       
>>>>>>         c = Foo()
>>>>>>         util.Finalize(c, conn.send, args=('c',))
       
>>>>>>         d10 = Foo()
>>>>>>         util.Finalize(d10, conn.send, args=('d10',), exitpriority=1)
       
>>>>>>         d01 = Foo()
>>>>>>         util.Finalize(d01, conn.send, args=('d01',), exitpriority=0)
>>>>>>         d02 = Foo()
>>>>>>         util.Finalize(d02, conn.send, args=('d02',), exitpriority=0)
>>>>>>         d03 = Foo()
>>>>>>         util.Finalize(d03, conn.send, args=('d03',), exitpriority=0)
       
>>>>>>         util.Finalize(None, conn.send, args=('e',), exitpriority=-10)
       
>>>>>>         util.Finalize(None, conn.send, args=('STOP',), exitpriority=-100)
       
               # call multiprocessing's cleanup function then exit process without
               # garbage collecting locals
>>>>>>         util._exit_function()
>>>>>>         conn.close()
>>>>>>         os._exit(0)
       
    2:     def test_finalize(self):
    1:         conn, child_conn = self.Pipe()
       
    1:         p = self.Process(target=self._test_finalize, args=(child_conn,))
    1:         p.daemon = True
    1:         p.start()
    1:         p.join()
       
    8:         result = [obj for obj in iter(conn.recv, 'STOP')]
    1:         self.assertEqual(result, ['a', 'b', 'd10', 'd03', 'd02', 'd01', 'e'])
       
    2:     def test_thread_safety(self):
               # bpo-24484: _run_finalizers() should be thread-safe
    1:         def cb():
 7100:             pass
       
    2:         class Foo(object):
    1:             def __init__(self):
14440:                 self.ref = self  # create reference cycle
                       # insert finalizer at random key
14440:                 util.Finalize(self, cb, exitpriority=random.randint(1, 100))
       
    1:         finish = False
    1:         exc = []
       
    1:         def run_finalizers():
    6:             while not finish:
    5:                 time.sleep(random.random() * 1e-1)
    5:                 try:
                           # A GC run will eventually happen during this,
                           # collecting stale Foo's and mutating the registry
    5:                     util._run_finalizers()
>>>>>>                 except Exception as e:
>>>>>>                     exc.append(e)
       
    1:         def make_finalizers():
    1:             d = {}
 1445:             while not finish:
 1444:                 try:
                           # Old Foo's get gradually replaced and later
                           # collected by the GC (because of the cyclic ref)
17328:                     d[random.getrandbits(5)] = {Foo() for i in range(10)}
>>>>>>                 except Exception as e:
>>>>>>                     exc.append(e)
>>>>>>                     d.clear()
       
    1:         old_interval = sys.getcheckinterval()
    1:         old_threshold = gc.get_threshold()
    1:         try:
    1:             sys.setcheckinterval(10)
    1:             gc.set_threshold(5, 5, 5)
    1:             threads = [threading.Thread(target=run_finalizers),
    1:                        threading.Thread(target=make_finalizers)]
    1:             with test_support.start_threads(threads):
    1:                 time.sleep(4.0)  # Wait a bit to trigger race condition
    1:                 finish = True
    1:             if exc:
>>>>>>                 raise exc[0]
               finally:
    1:             sys.setcheckinterval(old_interval)
    1:             gc.set_threshold(*old_threshold)
    1:             gc.collect()  # Collect remaining Foo's
       
       
       #
       # Test that from ... import * works for each module
       #
       
    4: class _TestImportStar(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def test_import(self):
               modules = [
    1:             'multiprocessing', 'multiprocessing.connection',
    1:             'multiprocessing.heap', 'multiprocessing.managers',
    1:             'multiprocessing.pool', 'multiprocessing.process',
    1:             'multiprocessing.synchronize', 'multiprocessing.util'
                   ]
       
    1:         if HAS_REDUCTION:
    1:             modules.append('multiprocessing.reduction')
       
    1:         if c_int is not None:
                   # This module requires _ctypes
    1:             modules.append('multiprocessing.sharedctypes')
       
   11:         for name in modules:
   10:             __import__(name)
   10:             mod = sys.modules[name]
       
   75:             for attr in getattr(mod, '__all__', ()):
   65:                 self.assertTrue(
   65:                     hasattr(mod, attr),
   65:                     '%r does not have attribute %r' % (mod, attr)
                           )
       
       #
       # Quick test that logging works -- does not test logging output
       #
       
    4: class _TestLogging(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     def test_enable_logging(self):
    1:         logger = multiprocessing.get_logger()
    1:         logger.setLevel(util.SUBWARNING)
    1:         self.assertTrue(logger is not None)
    1:         logger.debug('this will not be printed')
    1:         logger.info('nor will this')
    1:         logger.setLevel(LOG_LEVEL)
       
    2:     @classmethod
           def _test_level(cls, conn):
>>>>>>         logger = multiprocessing.get_logger()
>>>>>>         conn.send(logger.getEffectiveLevel())
       
    2:     def test_level(self):
    1:         LEVEL1 = 32
    1:         LEVEL2 = 37
       
    1:         logger = multiprocessing.get_logger()
    1:         root_logger = logging.getLogger()
    1:         root_level = root_logger.level
       
    1:         reader, writer = multiprocessing.Pipe(duplex=False)
       
    1:         logger.setLevel(LEVEL1)
    1:         p = self.Process(target=self._test_level, args=(writer,))
    1:         p.daemon = True
    1:         p.start()
    1:         self.assertEqual(LEVEL1, reader.recv())
       
    1:         logger.setLevel(logging.NOTSET)
    1:         root_logger.setLevel(LEVEL2)
    1:         p = self.Process(target=self._test_level, args=(writer,))
    1:         p.daemon = True
    1:         p.start()
    1:         self.assertEqual(LEVEL2, reader.recv())
       
    1:         root_logger.setLevel(root_level)
    1:         logger.setLevel(level=LOG_LEVEL)
       
       
       # class _TestLoggingProcessName(BaseTestCase):
       #
       #     def handle(self, record):
       #         assert record.processName == multiprocessing.current_process().name
       #         self.__handled = True
       #
       #     def test_logging(self):
       #         handler = logging.Handler()
       #         handler.handle = self.handle
       #         self.__handled = False
       #         # Bypass getLogger() and side-effects
       #         logger = logging.getLoggerClass()(
       #                 'multiprocessing.test.TestLoggingProcessName')
       #         logger.addHandler(handler)
       #         logger.propagate = False
       #
       #         logger.warn('foo')
       #         assert self.__handled
       
       #
       # Check that Process.join() retries if os.waitpid() fails with EINTR
       #
       
    4: class _TestPollEintr(BaseTestCase):
       
    2:     ALLOWED_TYPES = ('processes',)
       
    2:     @classmethod
           def _killer(cls, pid):
>>>>>>         time.sleep(0.5)
>>>>>>         os.kill(pid, signal.SIGUSR1)
       
    2:     @unittest.skipUnless(hasattr(signal, 'SIGUSR1'), 'requires SIGUSR1')
           def test_poll_eintr(self):
    1:         got_signal = [False]
    1:         def record(*args):
    1:             got_signal[0] = True
    1:         pid = os.getpid()
    1:         oldhandler = signal.signal(signal.SIGUSR1, record)
    1:         try:
    1:             killer = self.Process(target=self._killer, args=(pid,))
    1:             killer.start()
    1:             p = self.Process(target=time.sleep, args=(1,))
    1:             p.start()
    1:             p.join()
    1:             self.assertTrue(got_signal[0])
    1:             self.assertEqual(p.exitcode, 0)
    1:             killer.join()
               finally:
    1:             signal.signal(signal.SIGUSR1, oldhandler)
       
       #
       # Test to verify handle verification, see issue 3321
       #
       
    4: class TestInvalidHandle(unittest.TestCase):
       
    2:     @unittest.skipIf(WIN32, "skipped on Windows")
           def test_invalid_handles(self):
    1:         conn = _multiprocessing.Connection(44977608)
    1:         self.assertRaises(IOError, conn.poll)
    1:         self.assertRaises(IOError, _multiprocessing.Connection, -1)
       
       #
       # Functions used to create test cases from the base ones in this module
       #
       
    2: def get_attributes(Source, names):
    6:     d = {}
  104:     for name in names:
   98:         obj = getattr(Source, name)
   98:         if type(obj) == type(get_attributes):
   50:             obj = staticmethod(obj)
   98:         d[name] = obj
    6:     return d
       
    2: def create_test_cases(Mixin, type):
    6:     result = {}
    6:     glob = globals()
    6:     Type = type.capitalize()
       
  730:     for name in glob.keys():
  724:         if name.startswith('_Test'):
  150:             base = glob[name]
  150:             if type in base.ALLOWED_TYPES:
   84:                 newname = 'With' + Type + name[1:]
  168:                 class Temp(base, unittest.TestCase, Mixin):
   84:                     pass
   84:                 result[newname] = Temp
   84:                 Temp.__name__ = newname
   84:                 Temp.__module__ = Mixin.__module__
    6:     return result
       
       #
       # Create test cases
       #
       
    4: class ProcessesMixin(object):
    2:     TYPE = 'processes'
    2:     Process = multiprocessing.Process
    2:     locals().update(get_attributes(multiprocessing, (
               'Queue', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore',
               'Condition', 'Event', 'Value', 'Array', 'RawValue',
               'RawArray', 'current_process', 'active_children', 'Pipe',
    2:         'connection', 'JoinableQueue', 'Pool'
               )))
       
    2: testcases_processes = create_test_cases(ProcessesMixin, type='processes')
    2: globals().update(testcases_processes)
       
       
    4: class ManagerMixin(object):
    2:     TYPE = 'manager'
    2:     Process = multiprocessing.Process
    2:     manager = object.__new__(multiprocessing.managers.SyncManager)
    2:     locals().update(get_attributes(manager, (
               'Queue', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore',
              'Condition', 'Event', 'Value', 'Array', 'list', 'dict',
    2:         'Namespace', 'JoinableQueue', 'Pool'
               )))
       
    2: testcases_manager = create_test_cases(ManagerMixin, type='manager')
    2: globals().update(testcases_manager)
       
       
    4: class ThreadsMixin(object):
    2:     TYPE = 'threads'
    2:     Process = multiprocessing.dummy.Process
    2:     locals().update(get_attributes(multiprocessing.dummy, (
               'Queue', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore',
               'Condition', 'Event', 'Value', 'Array', 'current_process',
               'active_children', 'Pipe', 'connection', 'dict', 'list',
    2:         'Namespace', 'JoinableQueue', 'Pool'
               )))
       
    2: testcases_threads = create_test_cases(ThreadsMixin, type='threads')
    2: globals().update(testcases_threads)
       
    4: class OtherTest(unittest.TestCase):
           # TODO: add more tests for deliver/answer challenge.
    2:     def test_deliver_challenge_auth_failure(self):
    2:         class _FakeConnection(object):
    1:             def recv_bytes(self, size):
    1:                 return b'something bogus'
    1:             def send_bytes(self, data):
    2:                 pass
    1:         self.assertRaises(multiprocessing.AuthenticationError,
    1:                           multiprocessing.connection.deliver_challenge,
    1:                           _FakeConnection(), b'abc')
       
    2:     def test_answer_challenge_auth_failure(self):
    2:         class _FakeConnection(object):
    1:             def __init__(self):
    1:                 self.count = 0
    1:             def recv_bytes(self, size):
    2:                 self.count += 1
    2:                 if self.count == 1:
    1:                     return multiprocessing.connection.CHALLENGE
    1:                 elif self.count == 2:
    1:                     return b'something bogus'
>>>>>>                 return b''
    1:             def send_bytes(self, data):
    1:                 pass
    1:         self.assertRaises(multiprocessing.AuthenticationError,
    1:                           multiprocessing.connection.answer_challenge,
    1:                           _FakeConnection(), b'abc')
       
       #
       # Test Manager.start()/Pool.__init__() initializer feature - see issue 5585
       #
       
    2: def initializer(ns):
>>>>>>     ns.test += 1
       
    4: class TestInitializers(unittest.TestCase):
    2:     def setUp(self):
    2:         self.mgr = multiprocessing.Manager()
    2:         self.ns = self.mgr.Namespace()
    2:         self.ns.test = 0
       
    2:     def tearDown(self):
    2:         self.mgr.shutdown()
       
    2:     def test_manager_initializer(self):
    1:         m = multiprocessing.managers.SyncManager()
    1:         self.assertRaises(TypeError, m.start, 1)
    1:         m.start(initializer, (self.ns,))
    1:         self.assertEqual(self.ns.test, 1)
    1:         m.shutdown()
       
    2:     def test_pool_initializer(self):
    1:         self.assertRaises(TypeError, multiprocessing.Pool, initializer=1)
    1:         p = multiprocessing.Pool(1, initializer, (self.ns,))
    1:         p.close()
    1:         p.join()
    1:         self.assertEqual(self.ns.test, 1)
       
       #
       # Issue 5155, 5313, 5331: Test process in processes
       # Verifies os.close(sys.stdin.fileno) vs. sys.stdin.close() behavior
       #
       
    2: def _this_sub_process(q):
>>>>>>     try:
>>>>>>         item = q.get(block=False)
>>>>>>     except Queue.Empty:
>>>>>>         pass
       
    2: def _test_process(q):
>>>>>>     queue = multiprocessing.Queue()
>>>>>>     subProc = multiprocessing.Process(target=_this_sub_process, args=(queue,))
>>>>>>     subProc.daemon = True
>>>>>>     subProc.start()
>>>>>>     subProc.join()
       
    2: def _afunc(x):
>>>>>>     return x*x
       
    2: def pool_in_process():
>>>>>>     pool = multiprocessing.Pool(processes=4)
>>>>>>     x = pool.map(_afunc, [1, 2, 3, 4, 5, 6, 7])
       
    4: class _file_like(object):
    2:     def __init__(self, delegate):
    1:         self._delegate = delegate
    1:         self._pid = None
       
    2:     @property
           def cache(self):
    2:         pid = os.getpid()
               # There are no race conditions since fork keeps only the running thread
    2:         if pid != self._pid:
    1:             self._pid = pid
    1:             self._cache = []
    2:         return self._cache
       
    2:     def write(self, data):
    1:         self.cache.append(data)
       
    2:     def flush(self):
    1:         self._delegate.write(''.join(self.cache))
    1:         self._cache = []
       
    4: class TestStdinBadfiledescriptor(unittest.TestCase):
       
    2:     def test_queue_in_process(self):
    1:         queue = multiprocessing.Queue()
    1:         proc = multiprocessing.Process(target=_test_process, args=(queue,))
    1:         proc.start()
    1:         proc.join()
       
    2:     def test_pool_in_process(self):
    1:         p = multiprocessing.Process(target=pool_in_process)
    1:         p.start()
    1:         p.join()
       
    2:     def test_flushing(self):
    1:         sio = StringIO()
    1:         flike = _file_like(sio)
    1:         flike.write('foo')
    1:         proc = multiprocessing.Process(target=lambda: flike.flush())
    1:         flike.flush()
    1:         assert sio.getvalue() == 'foo'
       
       #
       # Test interaction with socket timeouts - see Issue #6056
       #
       
    4: class TestTimeouts(unittest.TestCase):
    2:     @classmethod
           def _test_timeout(cls, child, address):
>>>>>>         time.sleep(1)
>>>>>>         child.send(123)
>>>>>>         child.close()
>>>>>>         conn = multiprocessing.connection.Client(address)
>>>>>>         conn.send(456)
>>>>>>         conn.close()
       
    2:     def test_timeout(self):
    1:         old_timeout = socket.getdefaulttimeout()
    1:         try:
    1:             socket.setdefaulttimeout(0.1)
    1:             parent, child = multiprocessing.Pipe(duplex=True)
    1:             l = multiprocessing.connection.Listener(family='AF_INET')
    1:             p = multiprocessing.Process(target=self._test_timeout,
    1:                                         args=(child, l.address))
    1:             p.start()
    1:             child.close()
    1:             self.assertEqual(parent.recv(), 123)
    1:             parent.close()
    1:             conn = l.accept()
    1:             self.assertEqual(conn.recv(), 456)
    1:             conn.close()
    1:             l.close()
    1:             p.join(10)
               finally:
    1:             socket.setdefaulttimeout(old_timeout)
       
       #
       # Test what happens with no "if __name__ == '__main__'"
       #
       
    4: class TestNoForkBomb(unittest.TestCase):
    2:     def test_noforkbomb(self):
    1:         name = os.path.join(os.path.dirname(__file__), 'mp_fork_bomb.py')
    1:         if WIN32:
>>>>>>             rc, out, err = test.script_helper.assert_python_failure(name)
>>>>>>             self.assertEqual(out, '')
>>>>>>             self.assertIn('RuntimeError', err)
               else:
    1:             rc, out, err = test.script_helper.assert_python_ok(name)
    1:             self.assertEqual(out.rstrip(), '123')
    1:             self.assertEqual(err, '')
       
       #
       # Issue 12098: check sys.flags of child matches that for parent
       #
       
    4: class TestFlags(unittest.TestCase):
    2:     @classmethod
           def run_in_grandchild(cls, conn):
>>>>>>         conn.send(tuple(sys.flags))
       
    2:     @classmethod
           def run_in_child(cls):
>>>>>>         import json
>>>>>>         r, w = multiprocessing.Pipe(duplex=False)
>>>>>>         p = multiprocessing.Process(target=cls.run_in_grandchild, args=(w,))
>>>>>>         p.start()
>>>>>>         grandchild_flags = r.recv()
>>>>>>         p.join()
>>>>>>         r.close()
>>>>>>         w.close()
>>>>>>         flags = (tuple(sys.flags), grandchild_flags)
>>>>>>         print(json.dumps(flags))
       
    2:     @test_support.requires_unicode  # XXX json needs unicode support
           def test_flags(self):
    1:         import json, subprocess
               # start child process using unusual flags
    1:         prog = ('from test.test_multiprocessing import TestFlags; ' +
    1:                 'TestFlags.run_in_child()')
    1:         data = subprocess.check_output(
    1:             [sys.executable, '-E', '-B', '-O', '-c', prog])
    1:         child_flags, grandchild_flags = json.loads(data.decode('ascii'))
    1:         self.assertEqual(child_flags, grandchild_flags)
       
       #
       # Issue #17555: ForkAwareThreadLock
       #
       
    4: class TestForkAwareThreadLock(unittest.TestCase):
           # We recurisvely start processes.  Issue #17555 meant that the
           # after fork registry would get duplicate entries for the same
           # lock.  The size of the registry at generation n was ~2**n.
       
    2:     @classmethod
           def child(cls, n, conn):
>>>>>>         if n > 1:
>>>>>>             p = multiprocessing.Process(target=cls.child, args=(n-1, conn))
>>>>>>             p.start()
>>>>>>             p.join()
               else:
>>>>>>             conn.send(len(util._afterfork_registry))
>>>>>>         conn.close()
       
    2:     def test_lock(self):
    1:         r, w = multiprocessing.Pipe(False)
    1:         l = util.ForkAwareThreadLock()
    1:         old_size = len(util._afterfork_registry)
    1:         p = multiprocessing.Process(target=self.child, args=(5, w))
    1:         p.start()
    1:         new_size = r.recv()
    1:         p.join()
    1:         self.assertLessEqual(new_size, old_size)
       
       #
       # Issue #17097: EINTR should be ignored by recv(), send(), accept() etc
       #
       
    4: class TestIgnoreEINTR(unittest.TestCase):
       
    2:     @classmethod
           def _test_ignore(cls, conn):
>>>>>>         def handler(signum, frame):
>>>>>>             pass
>>>>>>         signal.signal(signal.SIGUSR1, handler)
>>>>>>         conn.send('ready')
>>>>>>         x = conn.recv()
>>>>>>         conn.send(x)
>>>>>>         conn.send_bytes(b'x'*(1024*1024))   # sending 1 MB should block
       
    2:     @unittest.skipUnless(hasattr(signal, 'SIGUSR1'), 'requires SIGUSR1')
           def test_ignore(self):
    1:         conn, child_conn = multiprocessing.Pipe()
    1:         try:
    1:             p = multiprocessing.Process(target=self._test_ignore,
    1:                                         args=(child_conn,))
    1:             p.daemon = True
    1:             p.start()
    1:             child_conn.close()
    1:             self.assertEqual(conn.recv(), 'ready')
    1:             time.sleep(0.1)
    1:             os.kill(p.pid, signal.SIGUSR1)
    1:             time.sleep(0.1)
    1:             conn.send(1234)
    1:             self.assertEqual(conn.recv(), 1234)
    1:             time.sleep(0.1)
    1:             os.kill(p.pid, signal.SIGUSR1)
    1:             self.assertEqual(conn.recv_bytes(), b'x'*(1024*1024))
    1:             time.sleep(0.1)
    1:             p.join()
               finally:
    1:             conn.close()
       
    2:     @classmethod
           def _test_ignore_listener(cls, conn):
>>>>>>         def handler(signum, frame):
>>>>>>             pass
>>>>>>         signal.signal(signal.SIGUSR1, handler)
>>>>>>         l = multiprocessing.connection.Listener()
>>>>>>         conn.send(l.address)
>>>>>>         a = l.accept()
>>>>>>         a.send('welcome')
       
    2:     @unittest.skipUnless(hasattr(signal, 'SIGUSR1'), 'requires SIGUSR1')
           def test_ignore_listener(self):
    1:         conn, child_conn = multiprocessing.Pipe()
    1:         try:
    1:             p = multiprocessing.Process(target=self._test_ignore_listener,
    1:                                         args=(child_conn,))
    1:             p.daemon = True
    1:             p.start()
    1:             child_conn.close()
    1:             address = conn.recv()
    1:             time.sleep(0.1)
    1:             os.kill(p.pid, signal.SIGUSR1)
    1:             time.sleep(0.1)
    1:             client = multiprocessing.connection.Client(address)
    1:             self.assertEqual(client.recv(), 'welcome')
    1:             p.join()
               finally:
    1:             conn.close()
       
       #
       #
       #
       
    2: testcases_other = [OtherTest, TestInvalidHandle, TestInitializers,
    2:                    TestStdinBadfiledescriptor, TestTimeouts, TestNoForkBomb,
    2:                    TestFlags, TestForkAwareThreadLock, TestIgnoreEINTR]
       
       #
       #
       #
       
    2: def test_main(run=None):
    1:     if sys.platform.startswith("linux"):
>>>>>>         try:
>>>>>>             lock = multiprocessing.RLock()
>>>>>>         except OSError:
>>>>>>             raise unittest.SkipTest("OSError raises on RLock creation, see issue 3111!")
       
    1:     check_enough_semaphores()
       
    1:     if run is None:
    1:         from test.test_support import run_unittest as run
       
    1:     util.get_temp_dir()     # creates temp directory for use by all processes
       
    1:     multiprocessing.get_logger().setLevel(LOG_LEVEL)
       
    1:     ProcessesMixin.pool = multiprocessing.Pool(4)
    1:     ThreadsMixin.pool = multiprocessing.dummy.Pool(4)
    1:     ManagerMixin.manager.__init__()
    1:     ManagerMixin.manager.start()
    1:     ManagerMixin.pool = ManagerMixin.manager.Pool(4)
       
           testcases = (
   21:         sorted(testcases_processes.values(), key=lambda tc:tc.__name__) +
   10:         sorted(testcases_threads.values(), key=lambda tc:tc.__name__) +
   12:         sorted(testcases_manager.values(), key=lambda tc:tc.__name__) +
    1:         testcases_other
               )
       
    1:     loadTestsFromTestCase = unittest.defaultTestLoader.loadTestsFromTestCase
   53:     suite = unittest.TestSuite(loadTestsFromTestCase(tc) for tc in testcases)
           # (ncoghlan): Whether or not sys.exc_clear is executed by the threading
           # module during these tests is at least platform dependent and possibly
           # non-deterministic on any given platform. So we don't mind if the listed
           # warnings aren't actually raised.
    1:     with test_support.check_py3k_warnings(
    1:             (".+__(get|set)slice__ has been removed", DeprecationWarning),
    1:             (r"sys.exc_clear\(\) not supported", DeprecationWarning),
    1:             quiet=True):
    1:         run(suite)
       
    1:     ThreadsMixin.pool.terminate()
    1:     ProcessesMixin.pool.terminate()
    1:     ManagerMixin.pool.terminate()
    1:     ManagerMixin.manager.shutdown()
       
    1:     del ProcessesMixin.pool, ThreadsMixin.pool, ManagerMixin.pool
       
    2: def main():
>>>>>>     test_main(unittest.TextTestRunner(verbosity=2).run)
       
    2: if __name__ == '__main__':
>>>>>>     main()
