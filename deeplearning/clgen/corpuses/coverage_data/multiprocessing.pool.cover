       #
       # Module providing the `Pool` class for managing a process pool
       #
       # multiprocessing/pool.py
       #
       # Copyright (c) 2006-2008, R Oudkerk
       # All rights reserved.
       #
       # Redistribution and use in source and binary forms, with or without
       # modification, are permitted provided that the following conditions
       # are met:
       #
       # 1. Redistributions of source code must retain the above copyright
       #    notice, this list of conditions and the following disclaimer.
       # 2. Redistributions in binary form must reproduce the above copyright
       #    notice, this list of conditions and the following disclaimer in the
       #    documentation and/or other materials provided with the distribution.
       # 3. Neither the name of author nor the names of any contributors may be
       #    used to endorse or promote products derived from this software
       #    without specific prior written permission.
       #
       # THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS "AS IS" AND
       # ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
       # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
       # ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
       # FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
       # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
       # OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
       # HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
       # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
       # OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
       # SUCH DAMAGE.
       #
       
    1: __all__ = ['Pool']
       
       #
       # Imports
       #
       
    1: import threading
    1: import Queue
    1: import itertools
    1: import collections
    1: import time
       
    1: from multiprocessing import Process, cpu_count, TimeoutError
    1: from multiprocessing.util import Finalize, debug
       
       #
       # Constants representing the state of a pool
       #
       
    1: RUN = 0
    1: CLOSE = 1
    1: TERMINATE = 2
       
       #
       # Miscellaneous
       #
       
    1: job_counter = itertools.count()
       
    1: def mapstar(args):
   61:     return map(*args)
       
       #
       # Code run by worker processes
       #
       
    2: class MaybeEncodingError(Exception):
           """Wraps possible unpickleable errors, so they can be
    1:     safely sent through the socket."""
       
    1:     def __init__(self, exc, value):
   20:         self.exc = repr(exc)
   20:         self.value = repr(value)
   20:         super(MaybeEncodingError, self).__init__(self.exc, self.value)
       
    1:     def __str__(self):
>>>>>>         return "Error sending result: '%s'. Reason: '%s'" % (self.value,
>>>>>>                                                              self.exc)
       
    1:     def __repr__(self):
>>>>>>         return "<MaybeEncodingError: %s>" % str(self)
       
       
    1: def worker(inqueue, outqueue, initializer=None, initargs=(), maxtasks=None):
    9:     assert maxtasks is None or (type(maxtasks) in (int, long) and maxtasks > 0)
    9:     put = outqueue.put
    9:     get = inqueue.get
    9:     if hasattr(inqueue, '_writer'):
>>>>>>         inqueue._writer.close()
>>>>>>         outqueue._reader.close()
       
    9:     if initializer is not None:
>>>>>>         initializer(*initargs)
       
    9:     completed = 0
 1100:     while maxtasks is None or (maxtasks and completed < maxtasks):
 1100:         try:
 1100:             task = get()
>>>>>>         except (EOFError, IOError):
>>>>>>             debug('worker got EOFError or IOError -- exiting')
>>>>>>             break
       
 1092:         if task is None:
    9:             debug('worker got sentinel -- exiting')
    9:             break
       
 1090:         job, i, func, args, kwds = task
 1091:         try:
 1091:             result = (True, func(*args, **kwds))
>>>>>>         except Exception, e:
>>>>>>             result = (False, e)
 1091:         try:
 1091:             put((job, i, result))
>>>>>>         except Exception as e:
>>>>>>             wrapped = MaybeEncodingError(e, result[1])
>>>>>>             debug("Possible encoding error while sending result: %s" % (
>>>>>>                 wrapped))
>>>>>>             put((job, i, (False, wrapped)))
       
 1091:         task = job = result = func = args = kwds = None
 1091:         completed += 1
    9:     debug('worker exiting after %d tasks' % completed)
       
       #
       # Class representing a process pool
       #
       
    2: class Pool(object):
           '''
           Class which supports an async version of the `apply()` builtin
    1:     '''
    1:     Process = Process
       
    1:     def __init__(self, processes=None, initializer=None, initargs=(),
    1:                  maxtasksperchild=None):
   20:         self._setup_queues()
   20:         self._taskqueue = Queue.Queue()
   20:         self._cache = {}
   20:         self._state = RUN
   20:         self._maxtasksperchild = maxtasksperchild
   20:         self._initializer = initializer
   20:         self._initargs = initargs
       
   20:         if processes is None:
    1:             try:
    1:                 processes = cpu_count()
>>>>>>             except NotImplementedError:
>>>>>>                 processes = 1
   20:         if processes < 1:
    6:             raise ValueError("Number of processes must be at least 1")
       
   14:         if initializer is not None and not hasattr(initializer, '__call__'):
    1:             raise TypeError('initializer must be a callable')
       
   13:         self._processes = processes
   13:         self._pool = []
   13:         self._repopulate_pool()
       
   13:         self._worker_handler = threading.Thread(
   13:             target=Pool._handle_workers,
   13:             args=(self, )
                   )
   13:         self._worker_handler.daemon = True
   13:         self._worker_handler._state = RUN
   13:         self._worker_handler.start()
       
       
   13:         self._task_handler = threading.Thread(
   13:             target=Pool._handle_tasks,
   13:             args=(self._taskqueue, self._quick_put, self._outqueue,
   13:                   self._pool, self._cache)
                   )
   13:         self._task_handler.daemon = True
   13:         self._task_handler._state = RUN
   13:         self._task_handler.start()
       
   13:         self._result_handler = threading.Thread(
   13:             target=Pool._handle_results,
   13:             args=(self._outqueue, self._quick_get, self._cache)
                   )
   13:         self._result_handler.daemon = True
   13:         self._result_handler._state = RUN
   13:         self._result_handler.start()
       
   13:         self._terminate = Finalize(
   13:             self, self._terminate_pool,
   13:             args=(self._taskqueue, self._inqueue, self._outqueue, self._pool,
   13:                   self._worker_handler, self._task_handler,
   13:                   self._result_handler, self._cache),
   13:             exitpriority=15
                   )
       
    1:     def _join_exited_workers(self):
               """Cleanup after any worker processes which have exited due to reaching
               their specified lifetime.  Returns True if any workers were cleaned up.
               """
  567:         cleaned = False
 2805:         for i in reversed(range(len(self._pool))):
 2240:             worker = self._pool[i]
 2241:             if worker.exitcode is not None:
                       # worker exited
   12:                 debug('cleaning up worker %d' % i)
   12:                 worker.join()
   12:                 cleaned = True
   12:                 del self._pool[i]
  567:         return cleaned
       
    1:     def _repopulate_pool(self):
               """Bring the number of pool processes up to the specified number,
               for use after reaping workers which have exited.
               """
   66:         for i in range(self._processes - len(self._pool)):
   48:             w = self.Process(target=worker,
   48:                              args=(self._inqueue, self._outqueue,
   48:                                    self._initializer,
   48:                                    self._initargs, self._maxtasksperchild)
                                   )
   48:             self._pool.append(w)
   48:             w.name = w.name.replace('Process', 'PoolWorker')
   48:             w.daemon = True
   48:             w.start()
   48:             debug('added worker')
       
    1:     def _maintain_pool(self):
               """Clean up any exited workers and start replacements for them.
               """
  567:         if self._join_exited_workers():
    4:             self._repopulate_pool()
       
    1:     def _setup_queues(self):
   17:         from .queues import SimpleQueue
   17:         self._inqueue = SimpleQueue()
   17:         self._outqueue = SimpleQueue()
   17:         self._quick_put = self._inqueue._writer.send
   17:         self._quick_get = self._outqueue._reader.recv
       
    1:     def apply(self, func, args=(), kwds={}):
               '''
               Equivalent of `apply()` builtin
               '''
    4:         assert self._state == RUN
    4:         return self.apply_async(func, args, kwds).get()
       
    1:     def map(self, func, iterable, chunksize=None):
               '''
               Equivalent of `map()` builtin
               '''
    9:         assert self._state == RUN
    9:         return self.map_async(func, iterable, chunksize).get()
       
    1:     def imap(self, func, iterable, chunksize=1):
               '''
               Equivalent of `itertools.imap()` -- can be MUCH slower than `Pool.map()`
               '''
   14:         assert self._state == RUN
   14:         if chunksize == 1:
    8:             result = IMapIterator(self._cache)
   62:             self._taskqueue.put((((result._job, i, func, (x,), {})
   54:                          for i, x in enumerate(iterable)), result._set_length))
    8:             return result
               else:
    6:             assert chunksize > 1
    6:             task_batches = Pool._get_tasks(func, iterable, chunksize)
    6:             result = IMapIterator(self._cache)
   40:             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
   34:                      for i, x in enumerate(task_batches)), result._set_length))
 2060:             return (item for chunk in result for item in chunk)
       
    1:     def imap_unordered(self, func, iterable, chunksize=1):
               '''
               Like `imap()` method but ordering of results is arbitrary
               '''
   10:         assert self._state == RUN
   10:         if chunksize == 1:
    6:             result = IMapUnorderedIterator(self._cache)
 2018:             self._taskqueue.put((((result._job, i, func, (x,), {})
 2012:                          for i, x in enumerate(iterable)), result._set_length))
    6:             return result
               else:
    4:             assert chunksize > 1
    4:             task_batches = Pool._get_tasks(func, iterable, chunksize)
    4:             result = IMapUnorderedIterator(self._cache)
   52:             self._taskqueue.put((((result._job, i, mapstar, (x,), {})
   48:                      for i, x in enumerate(task_batches)), result._set_length))
 2049:             return (item for chunk in result for item in chunk)
       
    1:     def apply_async(self, func, args=(), kwds={}, callback=None):
               '''
               Asynchronous equivalent of `apply()` builtin
               '''
  134:         assert self._state == RUN
  134:         result = ApplyResult(self._cache, callback)
  134:         self._taskqueue.put(([(result._job, None, func, args, kwds)], None))
  134:         return result
       
    1:     def map_async(self, func, iterable, chunksize=None, callback=None):
               '''
               Asynchronous equivalent of `map()` builtin
               '''
   15:         assert self._state == RUN
   15:         if not hasattr(iterable, '__len__'):
>>>>>>             iterable = list(iterable)
       
   15:         if chunksize is None:
    9:             chunksize, extra = divmod(len(iterable), len(self._pool) * 4)
    9:             if extra:
    5:                 chunksize += 1
   15:         if len(iterable) == 0:
    6:             chunksize = 0
       
   15:         task_batches = Pool._get_tasks(func, iterable, chunksize)
   15:         result = MapResult(self._cache, chunksize, len(iterable), callback)
   90:         self._taskqueue.put((((result._job, i, mapstar, (x,), {})
   77:                               for i, x in enumerate(task_batches)), None))
   15:         return result
       
    1:     @staticmethod
           def _handle_workers(pool):
   13:         thread = threading.current_thread()
       
               # Keep maintaining workers until the cache gets drained, unless the pool
               # is terminated.
  579:         while thread._state == RUN or (pool._cache and thread._state != TERMINATE):
  567:             pool._maintain_pool()
  567:             time.sleep(0.1)
               # send sentinel to stop workers
   13:         pool._taskqueue.put(None)
   13:         debug('worker handler exiting')
       
    1:     @staticmethod
           def _handle_tasks(taskqueue, put, outqueue, pool, cache):
   13:         thread = threading.current_thread()
       
  184:         for taskseq, set_length in iter(taskqueue.get, None):
  173:             task = None
  173:             i = -1
  173:             try:
 2491:                 for i, task in enumerate(taskseq):
 2320:                     if thread._state:
    2:                         debug('task handler found thread._state != RUN')
    2:                         break
 2318:                     try:
 2318:                         put(task)
   10:                     except Exception as e:
   10:                         job, ind = task[:2]
   10:                         try:
   10:                             cache[job]._set(ind, (False, e))
    9:                         except KeyError:
    9:                             pass
                       else:
  161:                     if set_length:
   14:                         debug('doing set_length()')
   14:                         set_length(i+1)
  161:                     continue
    2:                 break
   10:             except Exception as ex:
   10:                 job, ind = task[:2] if task else (0, 0)
   10:                 if job in cache:
   10:                     cache[job]._set(ind + 1, (False, ex))
   10:                 if set_length:
   10:                     debug('doing set_length()')
   10:                     set_length(i+1)
                   finally:
  173:                 task = taskseq = job = None
               else:
   11:             debug('task handler got sentinel')
       
   13:         try:
                   # tell result handler to finish when cache is empty
   13:             debug('task handler sending sentinel to result handler')
   13:             outqueue.put(None)
       
                   # tell workers there is no more work
   13:             debug('task handler sending sentinel to workers')
   49:             for p in pool:
   36:                 put(None)
>>>>>>         except IOError:
>>>>>>             debug('task handler got IOError when sending sentinels')
       
   13:         debug('task handler exiting')
       
    1:     @staticmethod
           def _handle_results(outqueue, get, cache):
   13:         thread = threading.current_thread()
       
   13:         while 1:
 2321:             try:
 2321:                 task = get()
>>>>>>             except (IOError, EOFError):
>>>>>>                 debug('result handler got EOFError/IOError -- exiting')
>>>>>>                 return
       
 2321:             if thread._state:
    2:                 assert thread._state == TERMINATE
    2:                 debug('result handler found thread._state=TERMINATE')
    2:                 break
       
 2319:             if task is None:
   11:                 debug('result handler got sentinel')
   11:                 break
       
 2308:             job, i, obj = task
 2308:             try:
 2308:                 cache[job]._set(i, obj)
    4:             except KeyError:
    4:                 pass
 2308:             task = job = obj = None
       
   15:         while cache and thread._state != TERMINATE:
    2:             try:
    2:                 task = get()
>>>>>>             except (IOError, EOFError):
>>>>>>                 debug('result handler got EOFError/IOError -- exiting')
>>>>>>                 return
       
    2:             if task is None:
    2:                 debug('result handler ignoring extra sentinel')
    2:                 continue
>>>>>>             job, i, obj = task
>>>>>>             try:
>>>>>>                 cache[job]._set(i, obj)
>>>>>>             except KeyError:
>>>>>>                 pass
>>>>>>             task = job = obj = None
       
   13:         if hasattr(outqueue, '_reader'):
   10:             debug('ensuring that outqueue is not full')
                   # If we don't make room available in outqueue then
                   # attempts to add the sentinel (None) to outqueue may
                   # block.  There is guaranteed to be no more than 2 sentinels.
   10:             try:
   10:                 for i in range(10):
   10:                     if not outqueue._reader.poll():
   10:                         break
>>>>>>                     get()
>>>>>>             except (IOError, EOFError):
>>>>>>                 pass
       
   13:         debug('result handler exiting: len(cache)=%s, thread._state=%s',
   13:               len(cache), thread._state)
       
    1:     @staticmethod
           def _get_tasks(func, it, size):
   25:         it = iter(it)
   25:         while 1:
  157:             x = tuple(itertools.islice(it, size))
  151:             if not x:
   17:                 return
  134:             yield (func, x)
       
    1:     def __reduce__(self):
>>>>>>         raise NotImplementedError(
>>>>>>               'pool objects cannot be passed between processes or pickled'
                     )
       
    1:     def close(self):
    9:         debug('closing pool')
    9:         if self._state == RUN:
    9:             self._state = CLOSE
    9:             self._worker_handler._state = CLOSE
       
    1:     def terminate(self):
    4:         debug('terminating pool')
    4:         self._state = TERMINATE
    4:         self._worker_handler._state = TERMINATE
    4:         self._terminate()
       
    1:     def join(self):
   11:         debug('joining pool')
   11:         assert self._state in (CLOSE, TERMINATE)
   11:         self._worker_handler.join()
   11:         self._task_handler.join()
   11:         self._result_handler.join()
   39:         for p in self._pool:
   28:             p.join()
       
    1:     @staticmethod
           def _help_stuff_finish(inqueue, task_handler, size):
               # task_handler may be blocked trying to put items on inqueue
   10:         debug('removing tasks from inqueue until task handler finished')
   10:         inqueue._rlock.acquire()
   10:         while task_handler.is_alive() and inqueue._reader.poll():
>>>>>>             inqueue._reader.recv()
>>>>>>             time.sleep(0)
       
    1:     @classmethod
           def _terminate_pool(cls, taskqueue, inqueue, outqueue, pool,
                               worker_handler, task_handler, result_handler, cache):
               # this is guaranteed to only be called once
   13:         debug('finalizing pool')
       
   13:         worker_handler._state = TERMINATE
   13:         task_handler._state = TERMINATE
       
   13:         debug('helping task handler/workers to finish')
   13:         cls._help_stuff_finish(inqueue, task_handler, len(pool))
       
   13:         assert result_handler.is_alive() or len(cache) == 0
       
   13:         result_handler._state = TERMINATE
   13:         outqueue.put(None)                  # sentinel
       
               # We must wait for the worker handler to exit before terminating
               # workers because we don't want workers to be restarted behind our back.
   13:         debug('joining worker handler')
   13:         if threading.current_thread() is not worker_handler:
   13:             worker_handler.join(1e100)
       
               # Terminate workers which haven't already finished.
   13:         if pool and hasattr(pool[0], 'terminate'):
   10:             debug('terminating workers')
   37:             for p in pool:
   27:                 if p.exitcode is None:
    4:                     p.terminate()
       
   13:         debug('joining task handler')
   13:         if threading.current_thread() is not task_handler:
   13:             task_handler.join(1e100)
       
   13:         debug('joining result handler')
   13:         if threading.current_thread() is not result_handler:
   13:             result_handler.join(1e100)
       
   13:         if pool and hasattr(pool[0], 'terminate'):
   10:             debug('joining pool workers')
   37:             for p in pool:
   27:                 if p.is_alive():
                           # worker has not yet exited
>>>>>>                     debug('cleaning up worker %d' % p.pid)
>>>>>>                     p.join()
       
       #
       # Class whose instances are returned by `Pool.apply_async()`
       #
       
    2: class ApplyResult(object):
       
    1:     def __init__(self, cache, callback):
  149:         self._cond = threading.Condition(threading.Lock())
  149:         self._job = job_counter.next()
  149:         self._cache = cache
  149:         self._ready = False
  149:         self._callback = callback
  149:         cache[self._job] = self
       
    1:     def ready(self):
>>>>>>         return self._ready
       
    1:     def successful(self):
>>>>>>         assert self._ready
>>>>>>         return self._success
       
    1:     def wait(self, timeout=None):
  147:         self._cond.acquire()
  147:         try:
  147:             if not self._ready:
   75:                 self._cond.wait(timeout)
               finally:
  147:             self._cond.release()
       
    1:     def get(self, timeout=None):
  147:         self.wait(timeout)
  147:         if not self._ready:
    2:             raise TimeoutError
  145:         if self._success:
  124:             return self._value
               else:
   21:             raise self._value
       
    1:     def _set(self, i, obj):
  134:         self._success, self._value = obj
  134:         if self._callback and self._success:
>>>>>>             self._callback(self._value)
  134:         self._cond.acquire()
  134:         try:
  134:             self._ready = True
  134:             self._cond.notify()
               finally:
  134:             self._cond.release()
  134:         del self._cache[self._job]
       
    1: AsyncResult = ApplyResult       # create alias -- see #17805
       
       #
       # Class whose instances are returned by `Pool.map_async()`
       #
       
    2: class MapResult(ApplyResult):
       
    1:     def __init__(self, cache, chunksize, length, callback):
   15:         ApplyResult.__init__(self, cache, callback)
   15:         self._success = True
   15:         self._value = [None] * length
   15:         self._chunksize = chunksize
   15:         if chunksize <= 0:
    6:             self._number_left = 0
    6:             self._ready = True
    6:             del cache[self._job]
               else:
    9:             self._number_left = length//chunksize + bool(length % chunksize)
       
    1:     def _set(self, i, success_result):
   51:         success, result = success_result
   51:         if success:
   50:             self._value[i*self._chunksize:(i+1)*self._chunksize] = result
   50:             self._number_left -= 1
   50:             if self._number_left == 0:
    6:                 if self._callback:
>>>>>>                     self._callback(self._value)
    6:                 del self._cache[self._job]
    6:                 self._cond.acquire()
    6:                 try:
    6:                     self._ready = True
    6:                     self._cond.notify()
                       finally:
    6:                     self._cond.release()
       
               else:
    1:             self._success = False
    1:             self._value = result
    1:             del self._cache[self._job]
    1:             self._cond.acquire()
    1:             try:
    1:                 self._ready = True
    1:                 self._cond.notify()
                   finally:
    1:                 self._cond.release()
       
       #
       # Class whose instances are returned by `Pool.imap()`
       #
       
    2: class IMapIterator(object):
       
    1:     def __init__(self, cache):
   24:         self._cond = threading.Condition(threading.Lock())
   24:         self._job = job_counter.next()
   24:         self._cache = cache
   24:         self._items = collections.deque()
   24:         self._index = 0
   24:         self._length = None
   24:         self._unsorted = {}
   24:         cache[self._job] = self
       
    1:     def __iter__(self):
   18:         return self
       
    1:     def next(self, timeout=None):
 2138:         self._cond.acquire()
 2138:         try:
 2138:             try:
 2138:                 item = self._items.popleft()
 1358:             except IndexError:
 1358:                 if self._index == self._length:
   10:                     raise StopIteration
 1348:                 self._cond.wait(timeout)
 1348:                 try:
 1348:                     item = self._items.popleft()
    4:                 except IndexError:
    4:                     if self._index == self._length:
    4:                         raise StopIteration
>>>>>>                     raise TimeoutError
               finally:
 2138:             self._cond.release()
       
 2124:         success, value = item
 2124:         if success:
 2114:             return value
   10:         raise value
       
    1:     __next__ = next                    # XXX
       
    1:     def _set(self, i, obj):
   80:         self._cond.acquire()
   80:         try:
   80:             if self._index == i:
   64:                 self._items.append(obj)
   64:                 self._index += 1
   80:                 while self._index in self._unsorted:
   16:                     obj = self._unsorted.pop(self._index)
   16:                     self._items.append(obj)
   16:                     self._index += 1
   64:                 self._cond.notify()
                   else:
   16:                 self._unsorted[i] = obj
       
   80:             if self._index == self._length:
    6:                 del self._cache[self._job]
               finally:
   80:             self._cond.release()
       
    1:     def _set_length(self, length):
   24:         self._cond.acquire()
   24:         try:
   24:             self._length = length
   24:             if self._index == self._length:
    4:                 self._cond.notify()
    4:                 del self._cache[self._job]
               finally:
   24:             self._cond.release()
       
       #
       # Class whose instances are returned by `Pool.imap_unordered()`
       #
       
    2: class IMapUnorderedIterator(IMapIterator):
       
    1:     def _set(self, i, obj):
 2050:         self._cond.acquire()
 2050:         try:
 2050:             self._items.append(obj)
 2050:             self._index += 1
 2050:             self._cond.notify()
 2050:             if self._index == self._length:
    8:                 del self._cache[self._job]
               finally:
 2050:             self._cond.release()
       
       #
       #
       #
       
    2: class ThreadPool(Pool):
       
    1:     from .dummy import Process
       
    1:     def __init__(self, processes=None, initializer=None, initargs=()):
    3:         Pool.__init__(self, processes, initializer, initargs)
       
    1:     def _setup_queues(self):
    3:         self._inqueue = Queue.Queue()
    3:         self._outqueue = Queue.Queue()
    3:         self._quick_put = self._inqueue.put
    3:         self._quick_get = self._outqueue.get
       
    1:     @staticmethod
           def _help_stuff_finish(inqueue, task_handler, size):
               # put sentinels at head of inqueue to make workers finish
    3:         inqueue.not_empty.acquire()
    3:         try:
    3:             inqueue.queue.clear()
    3:             inqueue.queue.extend([None] * size)
    3:             inqueue.not_empty.notify_all()
               finally:
    3:             inqueue.not_empty.release()
