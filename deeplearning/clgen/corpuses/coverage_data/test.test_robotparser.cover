    1: import os
    1: import robotparser
    1: import unittest
    1: from test import support
    1: from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
    1: import StringIO
    1: try:
    1:     import threading
>>>>>> except ImportError:
>>>>>>     threading = None
       
       
    2: class BaseRobotTest:
    1:     robots_txt = ''
    1:     agent = 'test_robotparser'
    1:     good = []
    1:     bad = []
       
    1:     def setUp(self):
   18:         lines = StringIO.StringIO(self.robots_txt).readlines()
   18:         self.parser = robotparser.RobotFileParser()
   18:         self.parser.parse(lines)
       
    1:     def get_agent_and_url(self, url):
   21:         if isinstance(url, tuple):
>>>>>>             agent, url = url
>>>>>>             return agent, url
   21:         return self.agent, url
       
    1:     def test_good_urls(self):
   17:         for url in self.good:
    8:             agent, url = self.get_agent_and_url(url)
    8:             self.assertTrue(self.parser.can_fetch(agent, url))
       
    1:     def test_bad_urls(self):
   22:         for url in self.bad:
   13:             agent, url = self.get_agent_and_url(url)
   13:             self.assertFalse(self.parser.can_fetch(agent, url))
       
       
    2: class UserAgentWildcardTest(BaseRobotTest, unittest.TestCase):
           robots_txt = """\
       User-agent: *
       Disallow: /cyberworld/map/ # This is an infinite virtual URL space
       Disallow: /tmp/ # these will soon disappear
       Disallow: /foo.html
    1:     """
    1:     good = ['/', '/test.html']
    1:     bad = ['/cyberworld/map/index.html', '/tmp/xxx', '/foo.html']
       
       
    2: class RejectAllRobotsTest(BaseRobotTest, unittest.TestCase):
           robots_txt = """\
       # go away
       User-agent: *
       Disallow: /
    1:     """
    1:     good = []
    1:     bad = ['/cyberworld/map/index.html', '/', '/tmp/']
       
       
    2: class UserAgentOrderingTest(BaseRobotTest, unittest.TestCase):
           # the order of User-agent should be correct. note
           # that this file is incorrect because "Googlebot" is a
           # substring of "Googlebot-Mobile"
           robots_txt = """\
       User-agent: Googlebot
       Disallow: /
       
       User-agent: Googlebot-Mobile
       Allow: /
    1:     """
    1:     agent = 'Googlebot'
    1:     bad = ['/something.jpg']
       
       
    2: class UserAgentGoogleMobileTest(UserAgentOrderingTest):
    1:     agent = 'Googlebot-Mobile'
       
       
    2: class GoogleURLOrderingTest(BaseRobotTest, unittest.TestCase):
           # Google also got the order wrong. You need
           # to specify the URLs from more specific to more general
           robots_txt = """\
       User-agent: Googlebot
       Allow: /folder1/myfile.html
       Disallow: /folder1/
    1:     """
    1:     agent = 'googlebot'
    1:     good = ['/folder1/myfile.html']
    1:     bad = ['/folder1/anotherfile.html']
       
       
    2: class DisallowQueryStringTest(BaseRobotTest, unittest.TestCase):
           # see issue #6325 for details
           robots_txt = """\
       User-agent: *
       Disallow: /some/path?name=value
    1:     """
    1:     good = ['/some/path']
    1:     bad = ['/some/path?name=value']
       
       
    2: class UseFirstUserAgentWildcardTest(BaseRobotTest, unittest.TestCase):
           # obey first * entry (#4108)
           robots_txt = """\
       User-agent: *
       Disallow: /some/path
       
       User-agent: *
       Disallow: /another/path
    1:     """
    1:     good = ['/another/path']
    1:     bad = ['/some/path']
       
       
    2: class EmptyQueryStringTest(BaseRobotTest, unittest.TestCase):
           # normalize the URL first (#17403)
           robots_txt = """\
       User-agent: *
       Allow: /some/path?
       Disallow: /another/path?
    1:     """
    1:     good = ['/some/path?']
    1:     bad = ['/another/path?']
       
       
    2: class DefaultEntryTest(BaseRobotTest, unittest.TestCase):
           robots_txt = """\
       User-agent: *
       Crawl-delay: 1
       Request-rate: 3/15
       Disallow: /cyberworld/map/
    1:     """
    1:     good = ['/', '/test.html']
    1:     bad = ['/cyberworld/map/index.html']
       
       
    2: class RobotHandler(BaseHTTPRequestHandler):
       
    1:     def do_GET(self):
    1:         self.send_error(403, "Forbidden access")
       
    1:     def log_message(self, format, *args):
    2:         pass
       
       
    2: @unittest.skipUnless(threading, 'threading required for this test')
    1: class PasswordProtectedSiteTestCase(unittest.TestCase):
       
    1:     def setUp(self):
    1:         self.server = HTTPServer((support.HOST, 0), RobotHandler)
       
    1:         self.t = threading.Thread(
    1:             name='HTTPServer serving',
    1:             target=self.server.serve_forever,
                   # Short poll interval to make the test finish quickly.
                   # Time between requests is short enough that we won't wake
                   # up spuriously too many times.
    1:             kwargs={'poll_interval':0.01})
    1:         self.t.daemon = True  # In case this function raises.
    1:         self.t.start()
       
    1:     def tearDown(self):
    1:         self.server.shutdown()
    1:         self.t.join()
    1:         self.server.server_close()
       
    1:     @support.reap_threads
           def testPasswordProtectedSite(self):
    1:         addr = self.server.server_address
    1:         url = 'http://' + support.HOST + ':' + str(addr[1])
    1:         robots_url = url + "/robots.txt"
    1:         parser = robotparser.RobotFileParser()
    1:         parser.set_url(url)
    1:         parser.read()
    1:         self.assertFalse(parser.can_fetch("*", robots_url))
       
       
    2: class NetworkTestCase(unittest.TestCase):
       
    1:     base_url = 'http://www.pythontest.net/'
    1:     robots_txt = '{}elsewhere/robots.txt'.format(base_url)
       
    1:     @classmethod
           def setUpClass(cls):
    1:         support.requires('network')
>>>>>>         with support.transient_internet(cls.base_url):
>>>>>>             cls.parser = robotparser.RobotFileParser(cls.robots_txt)
>>>>>>             cls.parser.read()
       
    1:     def url(self, path):
>>>>>>         return '{}{}{}'.format(
>>>>>>             self.base_url, path, '/' if not os.path.splitext(path)[1] else ''
               )
       
    1:     def test_basic(self):
>>>>>>         self.assertFalse(self.parser.disallow_all)
>>>>>>         self.assertFalse(self.parser.allow_all)
>>>>>>         self.assertGreater(self.parser.mtime(), 0)
       
    1:     def test_can_fetch(self):
>>>>>>         self.assertTrue(self.parser.can_fetch('*', self.url('elsewhere')))
>>>>>>         self.assertFalse(self.parser.can_fetch('Nutch', self.base_url))
>>>>>>         self.assertFalse(self.parser.can_fetch('Nutch', self.url('brian')))
>>>>>>         self.assertFalse(self.parser.can_fetch('Nutch', self.url('webstats')))
>>>>>>         self.assertFalse(self.parser.can_fetch('*', self.url('webstats')))
>>>>>>         self.assertTrue(self.parser.can_fetch('*', self.base_url))
       
    1:     def test_read_404(self):
>>>>>>         parser = robotparser.RobotFileParser(self.url('i-robot.txt'))
>>>>>>         parser.read()
>>>>>>         self.assertTrue(parser.allow_all)
>>>>>>         self.assertFalse(parser.disallow_all)
>>>>>>         self.assertEqual(parser.mtime(), 0)
       
       
    1: def test_main():
    1:     support.run_unittest(
    1:         UserAgentWildcardTest,
    1:         RejectAllRobotsTest,
    1:         UserAgentOrderingTest,
    1:         UserAgentGoogleMobileTest,
    1:         GoogleURLOrderingTest,
    1:         DisallowQueryStringTest,
    1:         UseFirstUserAgentWildcardTest,
    1:         EmptyQueryStringTest,
    1:         DefaultEntryTest,
    1:         PasswordProtectedSiteTestCase,
    1:         NetworkTestCase)
       
       
    1: if __name__ == "__main__":
>>>>>>     test_main()
