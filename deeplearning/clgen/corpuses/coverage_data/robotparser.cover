       """ robotparser.py
       
           Copyright (C) 2000  Bastian Kleineidam
       
           You can choose between two licenses when using this package:
           1) GNU GPLv2
           2) PSF license for Python 2.2
       
           The robots.txt Exclusion Protocol is implemented as specified in
           http://www.robotstxt.org/norobots-rfc.txt
       
    1: """
    1: import urlparse
    1: import urllib
       
    1: __all__ = ["RobotFileParser"]
       
       
    2: class RobotFileParser:
           """ This class provides a set of methods to read, parse and answer
           questions about a single robots.txt file.
       
    1:     """
       
    1:     def __init__(self, url=''):
   19:         self.entries = []
   19:         self.default_entry = None
   19:         self.disallow_all = False
   19:         self.allow_all = False
   19:         self.set_url(url)
   19:         self.last_checked = 0
       
    1:     def mtime(self):
               """Returns the time the robots.txt file was last fetched.
       
               This is useful for long-running web spiders that need to
               check for new robots.txt files periodically.
       
               """
>>>>>>         return self.last_checked
       
    1:     def modified(self):
               """Sets the time the robots.txt file was last fetched to the
               current time.
       
               """
   18:         import time
   18:         self.last_checked = time.time()
       
    1:     def set_url(self, url):
               """Sets the URL referring to a robots.txt file."""
   20:         self.url = url
   20:         self.host, self.path = urlparse.urlparse(url)[1:3]
       
    1:     def read(self):
               """Reads the robots.txt URL and feeds it to the parser."""
    1:         opener = URLopener()
    1:         f = opener.open(self.url)
   10:         lines = [line.strip() for line in f]
    1:         f.close()
    1:         self.errcode = opener.errcode
    1:         if self.errcode in (401, 403):
    1:             self.disallow_all = True
>>>>>>         elif self.errcode >= 400 and self.errcode < 500:
>>>>>>             self.allow_all = True
>>>>>>         elif self.errcode == 200 and lines:
>>>>>>             self.parse(lines)
       
    1:     def _add_entry(self, entry):
   24:         if "*" in entry.useragents:
                   # the default entry is considered last
   14:             if self.default_entry is None:
                       # the first default entry wins
   12:                 self.default_entry = entry
               else:
   10:             self.entries.append(entry)
       
    1:     def parse(self, lines):
               """parse the input lines from a robots.txt file.
                  We allow that a user-agent: line is not preceded by
                  one or more blank lines."""
               # states:
               #   0: start state
               #   1: saw user-agent line
               #   2: saw an allow or disallow line
   18:         state = 0
   18:         linenumber = 0
   18:         entry = Entry()
       
   18:         self.modified()
  104:         for line in lines:
   86:             linenumber += 1
   86:             if not line:
>>>>>>                 if state == 1:
>>>>>>                     entry = Entry()
>>>>>>                     state = 0
>>>>>>                 elif state == 2:
>>>>>>                     self._add_entry(entry)
>>>>>>                     entry = Entry()
>>>>>>                     state = 0
                   # remove optional comment and strip line
   86:             i = line.find('#')
   86:             if i >= 0:
    6:                 line = line[:i]
   86:             line = line.strip()
   86:             if not line:
   26:                 continue
   60:             line = line.split(':', 1)
   60:             if len(line) == 2:
   60:                 line[0] = line[0].strip().lower()
   60:                 line[1] = urllib.unquote(line[1].strip())
   60:                 if line[0] == "user-agent":
   24:                     if state == 2:
    6:                         self._add_entry(entry)
    6:                         entry = Entry()
   24:                     entry.useragents.append(line[1])
   24:                     state = 1
   36:                 elif line[0] == "disallow":
   24:                     if state != 0:
   24:                         entry.rulelines.append(RuleLine(line[1], False))
   24:                         state = 2
   12:                 elif line[0] == "allow":
    8:                     if state != 0:
    8:                         entry.rulelines.append(RuleLine(line[1], True))
    8:                         state = 2
   18:         if state == 2:
   18:             self._add_entry(entry)
       
       
    1:     def can_fetch(self, useragent, url):
               """using the parsed robots.txt decide if useragent can fetch url"""
   22:         if self.disallow_all:
    1:             return False
   21:         if self.allow_all:
>>>>>>             return True
       
               # Until the robots.txt file has been read or found not
               # to exist, we must assume that no url is allowable.
               # This prevents false positives when a user erroneously
               # calls can_fetch() before calling read().
   21:         if not self.last_checked:
>>>>>>             return False
       
               # search for given user agent matches
               # the first match counts
   21:         parsed_url = urlparse.urlparse(urllib.unquote(url))
   21:         url = urlparse.urlunparse(('', '', parsed_url.path,
   21:             parsed_url.params, parsed_url.query, parsed_url.fragment))
   21:         url = urllib.quote(url)
   21:         if not url:
>>>>>>             url = "/"
   21:         for entry in self.entries:
    4:             if entry.applies_to(useragent):
    4:                 return entry.allowance(url)
               # try the default entry last
   17:         if self.default_entry:
   17:             return self.default_entry.allowance(url)
               # agent not found ==> access granted
>>>>>>         return True
       
       
    1:     def __str__(self):
>>>>>>         return ''.join([str(entry) + "\n" for entry in self.entries])
       
       
    2: class RuleLine:
           """A rule line is a single "Allow:" (allowance==True) or "Disallow:"
    1:        (allowance==False) followed by a path."""
    1:     def __init__(self, path, allowance):
   32:         if path == '' and not allowance:
                   # an empty value means allow all
>>>>>>             allowance = True
   32:         path = urlparse.urlunparse(urlparse.urlparse(path))
   32:         self.path = urllib.quote(path)
   32:         self.allowance = allowance
       
    1:     def applies_to(self, filename):
   30:         return self.path == "*" or filename.startswith(self.path)
       
    1:     def __str__(self):
>>>>>>         return (self.allowance and "Allow" or "Disallow") + ": " + self.path
       
       
    2: class Entry:
    1:     """An entry has one or more user-agents and zero or more rulelines"""
    1:     def __init__(self):
   24:         self.useragents = []
   24:         self.rulelines = []
       
    1:     def __str__(self):
>>>>>>         ret = []
>>>>>>         for agent in self.useragents:
>>>>>>             ret.extend(["User-agent: ", agent, "\n"])
>>>>>>         for line in self.rulelines:
>>>>>>             ret.extend([str(line), "\n"])
>>>>>>         return ''.join(ret)
       
    1:     def applies_to(self, useragent):
               """check if this entry applies to the specified agent"""
               # split the name token and make it lower case
    4:         useragent = useragent.split("/")[0].lower()
    4:         for agent in self.useragents:
    4:             if agent == '*':
                       # we have the catch-all agent
>>>>>>                 return True
    4:             agent = agent.lower()
    4:             if agent in useragent:
    4:                 return True
>>>>>>         return False
       
    1:     def allowance(self, filename):
               """Preconditions:
               - our agent applies to this entry
               - filename is URL decoded"""
   36:         for line in self.rulelines:
   30:             if line.applies_to(filename):
   15:                 return line.allowance
    6:         return True
       
    2: class URLopener(urllib.FancyURLopener):
    1:     def __init__(self, *args):
    1:         urllib.FancyURLopener.__init__(self, *args)
    1:         self.errcode = 200
       
    1:     def prompt_user_passwd(self, host, realm):
               ## If robots.txt file is accessible only with a password,
               ## we act as if the file wasn't there.
>>>>>>         return None, None
       
    1:     def http_error_default(self, url, fp, errcode, errmsg, headers):
    1:         self.errcode = errcode
    1:         return urllib.FancyURLopener.http_error_default(self, url, fp, errcode,
    1:                                                         errmsg, headers)
