       """This file contains the definition of atomizers.
       An atomizer converts a block of text into a sequence of vocbulary tokens.
    1: """
    1: from collections import Counter
       
    1: import numpy as np
    1: import pathlib
    1: import pickle
    1: import typing
    1: from absl import flags
    1: import errors
       
       
    1: FLAGS = flags.FLAGS# pragma: no cover
       
       
    2: class AtomizerBase(object):# pragma: no cover
    1:   """The base class for implementing atomizers."""
       
    1:   def __init__(self, vocab: typing.Dict[str, int]):
           """Instantiate an atomizer.
           Args:
             vocab: A dictionary of mappings from character sequences (atoms) into
               indices.
           Raises:
             TypeError: If vocab is not a dictionary.
             InvalidVocab: If the dictionary of mappings includes any duplicate values.
           """
    3:     self.vocab = vocab
    3:     self._UpdateVocabulary()
       
    1:   @property
    1:   def atoms(self) -> typing.List[str]:
           """A list of atoms in the vocabulary."""
    3:     return list(sorted(self.vocab.keys()))
       
    1:   @property
    1:   def indices(self) -> typing.List[int]:
           """A list of vocabulary indices."""
>>>>>>     return list(sorted(self.vocab.values()))
       
    1:   def _UpdateVocabulary(self) -> None:
           """Private method which must be called if vocab is modified."""
    4:     if not isinstance(self.vocab, dict):
>>>>>>       raise TypeError('vocabulary must be a dict')
       
           # Each atom and index must be unique to ensure deterministic encoding.
    4:     if len(set(self.vocab.keys())) != len(self.vocab):
>>>>>>       raise errors.InvalidVocab('all atoms must be unique')
    4:     if len(set(self.vocab.values())) != len(self.vocab):
>>>>>>       raise errors.InvalidVocab('all indices must be unique')
       
    4:     self.vocab_size = len(self.vocab)
   37:     self.decoder = {val: key for key, val in self.vocab.items()}
       
    1:   def AtomizeString(self, text: str) -> np.array:
           """Atomize a text into an array of vocabulary indices.
           Args:
             text: Input text.
           Returns:
             An array of indices into vocabulary for all atoms in text.
           Raises:
             VocabError: If the input text contains elements not in the vocabulary.
           """
>>>>>>     raise NotImplementedError("abstract class")
       
    1:   def TokenizeString(self, text: str) -> typing.List[str]:
           """Split the text into atoms, but do not encode to indices.
           Args:
             text: Input text.
           Returns:
             A list of tokens.
           """
    1:     indices = self.AtomizeString(text)
    3:     return list(map(lambda x: self.decoder[x], indices))
       
    1:   def DeatomizeIndices(self, encoded: np.array) -> str:
           """Translate atomized code back into a string.
           Args:
             encoded: An nparray of encoded vocabulary indices.
           Returns:
             The decoded text.
           """
>>>>>>     try:
>>>>>>       return ''.join(list(map(lambda x: self.decoder[x], encoded)))
>>>>>>     except KeyError:
>>>>>>       raise errors.VocabError
       
    1:   def ToFile(self, path: pathlib.Path) -> None:
           """Save an atomizer to file."""
>>>>>>     with open(path, 'wb') as f:
>>>>>>       pickle.dump(self, f)
       
    1:   @classmethod
    1:   def FromText(cls, text: str) -> 'AtomizerBase':
           """Instantiate and specialize an atomizer from a corpus text.
           Args:
             text: Text corpus
           Returns:
             An atomizer instance.
           """
>>>>>>     raise NotImplementedError("abstract class")
       
    1:   @classmethod
    1:   def FromFile(cls, path: pathlib.Path) -> 'AtomizerBase':
           """Load an atomizer from file."""
>>>>>>     with open(path, 'rb') as infile:
>>>>>>       return pickle.load(infile)
       
       
    2: class AsciiCharacterAtomizer(AtomizerBase):# pragma: no cover
    1:   """An atomizer for character-level syntactic modelling."""
       
    1:   def AtomizeString(self, text: str) -> np.array:
           """Atomize a text into an array of vocabulary indices.
           Args:
             text: Input text.
           Returns:
             An array of indices into vocabulary for all atoms in text.
           """
>>>>>>     try:
>>>>>>       return np.array(list(map(lambda x: self.vocab[x], text)), dtype=np.int32)
>>>>>>     except KeyError:
>>>>>>       raise errors.VocabError
       
    1:   def __repr__(self) -> str:
>>>>>>     return f'AsciiCharacterAtomizer[{self.vocab_size} chars]'
       
    1:   @classmethod
    1:   def FromText(cls, text: str) -> 'AsciiCharacterAtomizer':
           """Instantiate and an atomizer from a corpus text.
           Args:
             text: Text corpus.
           Returns:
             An atomizer instance.
           """
>>>>>>     counter = Counter(text)
>>>>>>     count_pairs = sorted(counter.items(), key=lambda x: -x[1])
>>>>>>     atoms, _ = zip(*count_pairs)
>>>>>>     vocab = dict(zip(atoms, range(len(atoms))))
>>>>>>     return AsciiCharacterAtomizer(vocab)
       
       
    2: class GreedyAtomizer(AtomizerBase):
    1:   """A greedy atomizer supports multi-character tokens."""
       
    1:   def __init__(self, vocab: typing.Dict[str, int], determine_chars=False):# pragma: no cover
    3:     self.determine_chars = determine_chars
    3:     super(GreedyAtomizer, self).__init__(vocab)
       
   22:     multichars = set(k for k in self.atoms if len(k) > 1)
   17:     first_chars = set(a[0] for a in multichars)
    3:     self.lookup = dict(
  123:         (c, [a for a in multichars if a[0] == c]) for c in first_chars)
       
    1:   def AtomizeString(self, text: str) -> np.array:
           """Atomize a text into an array of vocabulary indices.
           Args:
             text: Input text.
           Returns:
             An array of indices into vocabulary for all atoms in text.
           """
       
    4:     def _AddToVocab(token: str) -> int:
             """Add a token to the vocabulary and return its index."""
   12:       if self.determine_chars and token not in self.vocab:
    2:         max_index = max(self.vocab.values())
    2:         self.vocab[token] = max_index + 1
   12:       return self.vocab[token]
       
    4:     indices = []
    4:     i = 0
    4:     j = 2
    4:     try:
   16:       while i < len(text):
   12:         if self.lookup.get(text[i]):
    1:           if j <= len(text) and any(
    3:               x.startswith(text[i:j]) for x in self.lookup[text[i]]):
>>>>>>             j += 1
                 else:
    2:             while j > i + 1:
    3:               if any(x == text[i:j] for x in self.lookup[text[i]]):
>>>>>>                 indices.append(self.vocab[text[i:j]])
>>>>>>                 i = j
>>>>>>                 j += 2
>>>>>>                 break
                     else:
    1:                 j -= 1
                   else:
    1:               indices.append(_AddToVocab(text[i]))
    1:               i += 1
    1:               j += 2
               else:
   11:           indices.append(_AddToVocab(text[i]))
   11:           i += 1
   11:           j += 2
>>>>>>     except KeyError:
>>>>>>       raise errors.VocabError
       
    4:     if self.determine_chars:
    1:       self._UpdateVocabulary()
       
    4:     return np.array(indices, dtype=np.int32)
       
    1:   def __repr__(self) -> str:# pragma: no cover
>>>>>>     return f'GreedyAtomizer[{self.vocab_size} tokens]'
       
    1:   @classmethod# pragma: no cover
    1:   def FromText(cls, text: str, atoms: typing.Set[str]) -> 'GreedyAtomizer':# pragma: no cover
           """Instantiate and an atomizer from a corpus text.
           Args:
             text: Text corpus
             atoms: A list of multi-character tokens.
           Returns:
             An atomizer instance.
           """
    1:     if not atoms:
>>>>>>       raise errors.UserError('No atoms specified')
       
           # Instantiate a greedy atomizer using the full vocabulary.
    1:     full_vocab = dict(zip(atoms, range(len(atoms))))
    1:     c = GreedyAtomizer(full_vocab, determine_chars=True)
           # Derive the subset of the vocabulary required to encode the given text.
    1:     tokens = sorted(list(set(c.TokenizeString(text))))
    1:     vocab_subset = dict(zip(tokens, range(len(tokens))))
           # Return a new atomizer using the subset vocabulary.
    1:     return GreedyAtomizer(vocab_subset)
