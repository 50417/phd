#!/usr/bin/env python3
#
# Given some training set, generate a random subset and evaluate it's
# effectiveness against a test set.
#
import csv
import sys
import os

import copy
import numpy as np
import pandas as pd
import math

from argparse import ArgumentParser
from random import random

import labm8
from labm8 import fs
from labm8 import math as labmath
from multiprocessing import cpu_count, Pool

import smith
from smith import features
from smith import cgo13


class MalformedDataException(smith.SmithException): pass


def mktraining(training, test):
    # Ratio of GPU oracle in test data:
    ngpu = sum([x == "GPU" for x in test["oracle"]])
    test_ratio = ngpu / (len(test["oracle"]))

    # Ratio of GPU oracle in training data:
    ngpu = sum([x == "GPU" for x in training["oracle"]])
    training_ratio = ngpu / (len(training["oracle"]))
    if ngpu > 200:
        target_ngpu = math.ceil(ngpu * random())
    else:
        target_ngpu = ngpu

    resized_training = int(round(target_ngpu / test_ratio))
    target_ncpu = resized_training - target_ngpu

    # Shuffle the training data:
    training = training.reindex(np.random.permutation(training.index))

    # print("Target:", target_ngpu, target_ncpu, resized_training)

    # Build the training data:
    out = []
    for i,d in enumerate(training.to_dict(orient="records")):
        if d["oracle"] == "GPU":
            if target_ngpu >= 1:
                out.append(d)
                target_ngpu -= 1
        elif target_ncpu >= 1:
            out.append(d)
            target_ncpu -= 1

        if not target_ncpu and not target_ngpu:
            break

    df = pd.DataFrame(out)

    # ngpu = sum([x == "GPU" for x in df["oracle"]])
    # ncpu = sum([x == "CPU" for x in df["oracle"]])
    # print("Generated:", ngpu, ncpu, len(out))

    return df


def _worker(job):
    training = mktraining(job["training"], job["test"])
    res = cgo13.classification(training, job["test"])
    return {"training": training, "res": res}

def main():
    parser = ArgumentParser()
    parser.add_argument('training', help='path to training data CSV')
    parser.add_argument('test', help='path to testing data CSV')
    args = parser.parse_args()

    training = cgo13.LabelledData.from_csv(smith.assert_exists(args.training))
    test = cgo13.LabelledData.from_csv(smith.assert_exists(args.test))

    best = 0

    if fs.exists("best.csv"):
        train2 = pd.DataFrame.from_csv("best.csv")
        res = cgo13.classification(train2, test)
        best = res.oracle
        print("loaded best of {:.0f}%".format(best * 100))

    # TODO: Parallelise into batches:
    # num_workers = 1 #round(cpu_count() * 1.5)
    # with Pool(num_workers) as pool:
    #     while True:
    #         jobs = [{"training": training.copy(), "test": test}
    #                 for i in range(num_workers)]

    #         print("spawning", num_workers, "worker threads to process", len(jobs),
    #               "jobs ...")
    #         results = pool.map(_worker, jobs)

    #         # if all(a == b for a,b in zip(results[0]["training"], results[1]["training"])):
    #         #     print("FUCK UP")
    #         # else:
    #         #     print("WORKING")

    #         batchbest = max([res["res"].oracle for res in results])
    #         if batchbest > best:
    #             print(batchbest, best)
    #         else:
    #             print(", ".join([str(res["res"].oracle) for res in results]))

    while True:
        train2 = mktraining(training, test)
        res = cgo13.classification(train2, test)
        print(len(train2), "instances", res, "current best {:.0f}%".format(best * 100))
        if res.oracle > best:
            best = res.oracle
            train2.to_csv("best.csv")


if __name__ == '__main__':
    main()
