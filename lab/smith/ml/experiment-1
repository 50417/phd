#!/usr/bin/env python3
#
#
#
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sklearn
import sys

from argparse import ArgumentParser
from math import sqrt,ceil
from functools import partial
from random import seed,random
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

import labm8
from labm8 import fmt
from labm8 import fs
from labm8 import viz
from labm8 import math as labmath

import smith
from smith import cgo13

# Seaborn configuration:
sns.set(style="ticks", color_codes=True)


def get_ex1_features(D):
    return np.array([
        (D["transfer"] / (D["comp"] + D["mem"])),
        (D["coalesced"] / D["mem"]),
        ((D["localmem"] / D["mem"]) * D["wgsize"]),
        (D["comp"] / D["mem"]),
    ]).T


def get_labels(D):
    return D["oracle"]


def leave_one_benchmark_out(D, benchmark):
    seed = 204
    clf = DecisionTreeClassifier(
        random_state=seed, criterion="entropy", splitter="best")
    # clf = KNeighborsClassifier(n_neighbors=1)

    # Create data masks to select all kernels from a given benchmark:
    # print("\r\033[Kleave-one-out on benchmark:", benchmark_name,
    #       "testing kernel:", kernel_name, end="")
    # sys.stdout.flush()

    # Create data masks. For training we exclude all results from
    # benchmark.
    test_mask = D["benchmark"].str.contains(r"^" + benchmark)
    train_mask = ~test_mask

    # Create training and testing data:
    X_train = get_ex1_features(D[train_mask])
    y_train = get_labels(D[train_mask])

    D_test = D[test_mask]
    # print(D_test)
    X_test = get_ex1_features(D_test)
    y_test = get_labels(D_test)

    # Train classifier:
    clf.fit(X_train, y_train)

    # Make predictions
    predicted = clf.predict(X_test)
    D_out = []
    for d,y,p in zip(D_test.to_dict('records'), y_test, predicted):
        d["p_correct"] = 1 if y == p else 0
        D_out.append(d)

    return D_out


def experiment1(platform_id):
    # Datasets:
    #
    #   B -  benchmarks
    #   S -  synthetics
    #   BS - benchmarks + synthetics
    #
    B = pd.read_csv("data/{platform_id}/benchmarks.csv"
                    .format(platform_id=platform_id.lower()))
    B["synthetic"] = np.zeros(len(B))

    S = pd.read_csv("data/{platform_id}/synthetics.csv"
                    .format(platform_id=platform_id.lower()))
    S["synthetic"] = np.ones(len(S))

    BS = pd.concat((B, S))

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith("npb-")
    ]))

    B_out, BS_out = [], []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(B, benchmark)
        BS_out += leave_one_benchmark_out(BS, benchmark)
        assert len(B_out) == len(BS_out)

    # Create results frame:
    R_out = []
    for b,bs in zip(B_out, BS_out):
        b["p_speedup"] = b["speedup"] if b["p_correct"] else b["penalty"]
        bs["p_speedup"] = bs["speedup"] if bs["p_correct"] else bs["penalty"]

        b["p_oracle"] = 1 if b["p_correct"] else b["penalty"]
        bs["p_oracle"] = 1 if bs["p_correct"] else bs["penalty"]

        b["benchmark_name"] = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                     b["benchmark"])
        bs["benchmark_name"] = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                      bs["benchmark"])

        b["training"] = "Baseline"
        bs["training"] = "w. Synthesized Kernels"

        R_out.append(b)
        R_out.append(bs)
    R = pd.DataFrame(R_out)

    b_mask = R["training"] == "Baseline"
    bs_mask = R["training"] == "w. Synthesized Kernels"

    B_acc = labmath.mean(R[b_mask]["p_correct"])
    BS_acc = labmath.mean(R[bs_mask]["p_correct"])

    B_oracle = labmath.mean(R[b_mask]["p_oracle"])
    BS_oracle = labmath.mean(R[bs_mask]["p_oracle"])

    B_speedup = labmath.mean(R[b_mask]["p_speedup"])
    BS_speedup = labmath.mean(R[bs_mask]["p_speedup"])

    # Print analytics:
    print("Experiment 1 on Platform {}:".format(platform_id.upper()))
    print(" Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
    print(" Accuracy w. synthesized: {:.1f} %".format(BS_acc * 100))
    print()
    print(" Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
    print(" Oracle w. synthesized:   {:.1f} %".format(BS_oracle * 100))
    print()
    print(" Speedup of baseline:     {:.2f} x".format(B_speedup))
    print(" Speedup w. synthesized:  {:.2f} x".format(BS_speedup))
    print()

    # Plot summary:
    ax = sns.barplot(x="benchmark_name", y="p_speedup", hue="training",
                     data=R, ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")
    # plt.ylim(0, 6)

    # No legend title
    ax.get_legend().set_title("")

    figsize = (4.5, 2)
    plottype = "pdf"
    viz.finalise("data/img/ex1-{}.{}".format(platform_id.upper(), plottype),
                 figsize=figsize, tight=True)


def experiment2():
    platform_id = "a"

    # Datasets:
    #
    #   B -  benchmarks
    #   S -  synthetics
    #   BS - benchmarks + synthetics
    #
    a_B = pd.read_csv("data/a/benchmarks.csv")
    a_B["synthetic"] = np.zeros(len(B))
    b_B = pd.read_csv("data/b/benchmarks.csv")
    b_B["synthetic"] = np.zeros(len(B))
    B = pd.concat((a_B, b_B))

    a_S = pd.read_csv("data/a/synthetics.csv")
    a_S["synthetic"] = np.ones(len(S))
    b_S = pd.read_csv("data/b/synthetics.csv")
    b_S["synthetic"] = np.ones(len(S))
    S = pd.concat((a_S, b_S))

    a_BS = pd.concat((a_B, a_S))
    b_BS = pd.concat((b_B, b_S))
    BS = pd.concat((a_BS, b_BS))

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b) for b in B["benchmark"]
        if b.startswith("npb-")
    ]))

    B_out, BS_out = [], []
    for benchmark in benchmark_names:
        print(benchmark)
        print(benchmark)
        B_out += leave_one_benchmark_out(B, benchmark)
        BS_out += leave_one_benchmark_out(BS, benchmark)
        assert(len(B_out) == len(BS_out))

    # Create results frame:
    R_out = []
    for b,bs in zip(B_out, BS_out):
        b["p_speedup"] = b["speedup"] if b["p_correct"] else b["penalty"]
        bs["p_speedup"] = bs["speedup"] if bs["p_correct"] else bs["penalty"]

        b["p_oracle"] = 1 if b["p_correct"] else b["penalty"]
        bs["p_oracle"] = 1 if bs["p_correct"] else bs["penalty"]

        b["benchmark_name"] = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                     b["benchmark"])
        bs["benchmark_name"] = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                      bs["benchmark"])

        b["training"] = "Baseline"
        bs["training"] = "w. Synthesized Kernels"

        R_out.append(b)
        R_out.append(bs)
    R = pd.DataFrame(R_out)

    b_mask = R["training"] == "Baseline"
    bs_mask = R["training"] == "w. Synthesized Kernels"

    B_acc = labmath.mean(R[b_mask]["p_correct"])
    BS_acc = labmath.mean(R[bs_mask]["p_correct"])

    B_oracle = labmath.mean(R[b_mask]["p_oracle"])
    BS_oracle = labmath.mean(R[bs_mask]["p_oracle"])

    B_speedup = labmath.mean(R[b_mask]["p_speedup"])
    BS_speedup = labmath.mean(R[bs_mask]["p_speedup"])

    # Print analytics:
    print("Experiment 1 on Platform {}:".format(platform_id.upper()))
    print(" Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
    print(" Accuracy w. synthesized: {:.1f} %".format(BS_acc * 100))
    print()
    print(" Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
    print(" Oracle w. synthesized:   {:.1f} %".format(BS_oracle * 100))
    print()
    print(" Speedup of baseline:     {:.2f} x".format(B_speedup))
    print(" Speedup w. synthesized:  {:.2f} x".format(BS_speedup))
    print()

    # Plot summary:
    ax = sns.barplot(x="benchmark_name", y="p_speedup", hue="training",
                     data=R, ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")
    # plt.ylim(0, 6)

    # No legend title
    ax.get_legend().set_title("")

    figsize = (4.5, 2)
    plottype = "pdf"
    viz.finalise("data/img/ex1-{}.{}".format(platform_id.upper(), plottype),
                 figsize=figsize, tight=True)


def main():
    experiment1("A")
    experiment1("B")

    # experiment2()


if __name__ == "__main__":
    main()
