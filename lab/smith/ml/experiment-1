#!/usr/bin/env python3
#
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sklearn
import sys

from argparse import ArgumentParser
from collections import Counter
from functools import partial
from math import sqrt,ceil
from random import seed,random
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

import labm8
from labm8 import fmt
from labm8 import fs
from labm8 import math as labmath
from labm8 import viz

import smith
from smith import cgo13

# Seaborn configuration:
sns.set(style="ticks", color_codes=True)


def get_ex1_features(D):
    return np.array([
        (D["transfer"] / (D["comp"] + D["mem"])),
        (D["coalesced"] / D["mem"]),
        ((D["localmem"] / D["mem"]) * D["wgsize"]),
        (D["comp"] / D["mem"]),
    ]).T


def get_labels(D):
    return D["oracle"]


def leave_one_benchmark_out(D, benchmark):
    seed = 204
    clf = DecisionTreeClassifier(
        random_state=seed, criterion="entropy", splitter="best")
    # clf = KNeighborsClassifier(n_neighbors=1)

    # Create data masks. For training we exclude all results from
    # benchmark.
    test_mask = D["benchmark"].str.contains(r"^" + benchmark)
    train_mask = ~test_mask

    # Create training and testing data:
    X_train = get_ex1_features(D[train_mask])
    y_train = get_labels(D[train_mask])

    D_test = D[test_mask]
    # print(D_test)
    X_test = get_ex1_features(D_test)
    y_test = get_labels(D_test)

    # Debugging printout:
    print("leave-one-out on benchmark:", benchmark,
          "train on:", len(X_train), "test on:", len(X_test))

    # Train classifier:
    clf.fit(X_train, y_train)

    # Make predictions
    predicted = clf.predict(X_test)
    D_out = []
    for d,y,p in zip(D_test.to_dict('records'), y_test, predicted):
        d["p"] = p
        d["p_correct"] = 1 if y == p else 0
        D_out.append(d)

    return D_out


def experiment1(platform_id):
    # Datasets:
    #
    #   B -  benchmarks
    #   S -  synthetics
    #   BS - benchmarks + synthetics
    #
    B = pd.read_csv("data/{platform_id}/benchmarks.csv"
                    .format(platform_id=platform_id.lower()))
    B["synthetic"] = np.zeros(len(B))

    S = pd.read_csv("data/{platform_id}/synthetics.csv"
                    .format(platform_id=platform_id.lower()))
    S["synthetic"] = np.ones(len(S))

    BS = pd.concat((B, S))

    # Find the ZeroR. This is the device which is most frequently
    # optimal:
    Bmask = B[B["benchmark"].str.contains("npb-3.3-")]

    zeror = Counter(Bmask["oracle"]).most_common(1)[0][0]
    zeror_freq = Counter(Bmask["oracle"]).most_common(1)[0][1]
    zeror_ratio = zeror_freq / len(Bmask)
    zeror_runtime = "runtime_" + zeror.lower()

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith("npb-")
    ]))

    B_out, BS_out = [], []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(B, benchmark)
        BS_out += leave_one_benchmark_out(BS, benchmark)
        assert len(B_out) == len(BS_out)

    # Create results frame:
    R_out = []
    for b,bs in zip(B_out, BS_out):
        # Get runtimes of device using predicted device.
        b_p_runtime = b["runtime_" + b["p"].lower()]
        bs_p_runtime = bs["runtime_" + bs["p"].lower()]

        # Speedup is the ratio of runtime using the predicted device
        # over runtime using ZeroR device.
        b["p_speedup"] = b_p_runtime / b[zeror_runtime]
        bs["p_speedup"] = bs_p_runtime / bs[zeror_runtime]

        # Oracle is the ratio of runtime using the best device vs
        # runtime using predicted device.
        b["p_oracle"] = b["runtime"] / b_p_runtime
        bs["p_oracle"] = bs["runtime"] / bs_p_runtime

        # Get the benchmark name:
        benchmark_name = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                b["benchmark"])
        b["benchmark_name"] = benchmark_name
        bs["benchmark_name"] = benchmark_name

        # Set the training data type.
        b["training"] = "Baseline"
        bs["training"] = "w. Synthesized Kernels"

        R_out.append(b)
        R_out.append(bs)
    R = pd.DataFrame(R_out)

    b_mask = R["training"] == "Baseline"
    bs_mask = R["training"] == "w. Synthesized Kernels"

    B_acc = labmath.mean(R[b_mask]["p_correct"])
    BS_acc = labmath.mean(R[bs_mask]["p_correct"])

    B_oracle = labmath.mean(R[b_mask]["p_oracle"])
    BS_oracle = labmath.mean(R[bs_mask]["p_oracle"])

    B_speedup = labmath.mean(R[b_mask]["p_speedup"])
    BS_speedup = labmath.mean(R[bs_mask]["p_speedup"])

    # Print analytics:
    print("Experiment 1 on Platform {}:".format(platform_id.upper()))
    print("  ZeroR device:            {} ({:.1f} %)".format(zeror, zeror_ratio * 100))
    print()
    print("  Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
    print("  Accuracy w. synthesized: {:.1f} %".format(BS_acc * 100))
    print()
    print("  Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
    print("  Oracle w. synthesized:   {:.1f} %".format(BS_oracle * 100))
    print()
    print("  Speedup of baseline:     {:.2f} x".format(B_speedup))
    print("  Speedup w. synthesized:  {:.2f} x".format(BS_speedup))
    print()

    # Plot summary:
    ax = sns.barplot(x="benchmark_name", y="p_speedup", hue="training",
                     data=R, ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")
    # plt.ylim(0, 6)
    plt.axhline(y=1)

    # No legend title
    ax.get_legend().set_title("")

    figsize = (4.5, 2)
    plottype = "pdf"
    viz.finalise("data/img/ex1-{}.{}".format(platform_id.upper(), plottype),
                 figsize=figsize, tight=True)


def main():
    experiment1("A")
    experiment1("B")

    # experiment2()


if __name__ == "__main__":
    main()
