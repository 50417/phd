#!/usr/bin/env python3
#
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sklearn
import sys

from argparse import ArgumentParser
from collections import Counter
from copy import copy
from functools import partial
from math import sqrt,ceil
from random import seed,random
from sklearn.base import clone
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


import labm8
from labm8 import fmt
from labm8 import fs
from labm8 import math as labmath
from labm8 import viz

import smith
from smith import cgo13

# Seaborn configuration:
sns.set(style="ticks", color_codes=True)


def get_cgo13_features(D):
    # return np.array([[0] * len(D), [0] * len(D)]).T
    return np.array([
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_our_features(D):
    # return np.array([[0] * len(D), [0] * len(D)]).T
    return np.array([
        D["comp"].values,
        D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        D["atomic"].values,
        D["transfer"].values,
        D["wgsize"].values,
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_labels(D):
    return D["oracle"]


def leave_one_benchmark_out(clf, get_features, D, benchmark):
    # Create data masks. For training we exclude all results from
    # benchmark.
    test_mask = D["benchmark"].str.contains(r"^" + benchmark)
    train_mask = ~test_mask

    # Create training and testing data:
    X_train = get_features(D[train_mask])
    y_train = get_labels(D[train_mask])

    D_test = D[test_mask]
    X_test = get_features(D_test)
    y_test = get_labels(D_test)

    # Debugging printout:
    print("benchmark:", benchmark,
          "train on:", len(X_train), "test on:", len(X_test))

    # Train classifier:
    clf.fit(X_train, y_train)

    # Make predictions
    predicted = clf.predict(X_test)
    D_out = []
    for d,y,p in zip(D_test.to_dict('records'), y_test, predicted):
        d["p"] = p
        d["p_correct"] = 1 if y == p else 0
        D_out.append(d)

    # Return a list of dicts
    return D_out


def motivation():
    clf = DecisionTreeClassifier(
        random_state=204, criterion="entropy", splitter="best")
    features = get_cgo13_features
    platform = "b"
    benchmark = "amd"


    B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
    test_mask = B["benchmark"].str.contains(r"^{}-".format(benchmark))
    B = B[test_mask]

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith(benchmark)
    ]))
    # print(benchmark_names)

    B_out = []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(clf, get_our_features, B, benchmark)
    B_out = pd.DataFrame(B_out)
    assert(len(B) == len(B_out))

    pca = PCA(n_components=2)
    X = pca.fit_transform(get_our_features(B))

    x_correct = [x[0] for x,b in zip(X,B_out.to_dict('records')) if b["p_correct"]]
    y_correct = [x[1] for x,b in zip(X,B_out.to_dict('records')) if b["p_correct"]]

    x_incorrect = [x[0] for x,b in zip(X,B_out.to_dict('records')) if not b["p_correct"]]
    y_incorrect = [x[1] for x,b in zip(X,B_out.to_dict('records')) if not b["p_correct"]]
    labels = [b["benchmark"].split('-')[-1]
              for b in B_out.to_dict('records')
              if not b["p_correct"]]

    plt.scatter(x_correct, y_correct, color="b", s=50)
    plt.scatter(x_incorrect, y_incorrect, color="r", s=50)

    for label, x, y in zip(labels, x_incorrect, y_incorrect):
        plt.annotate(
            label,
            xy = (x, y), xytext = (-20, 20),
            textcoords = 'offset points', ha = 'right', va = 'bottom',
            bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),
            arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))

    plt.show()


def experiment1(platform_id):
    #
    # Cross-validate CGO13 model across NPB benchmarks, with and
    # without synthetic benchmarks.
    #
    # Datasets:
    #
    #   B -  benchmarks
    #   S -  synthetics
    #   BS - benchmarks + synthetics
    #
    B = pd.read_csv("data/{platform_id}/benchmarks.csv"
                    .format(platform_id=platform_id.lower()))
    B["synthetic"] = np.zeros(len(B))

    S = pd.read_csv("data/{platform_id}/synthetics.csv"
                    .format(platform_id=platform_id.lower()))
    S["synthetic"] = np.ones(len(S))

    BS = pd.concat((B, S))

    # Find the ZeroR. This is the device which is most frequently
    # optimal:
    Bmask = B[B["benchmark"].str.contains("npb-3.3-")]

    zeror = Counter(Bmask["oracle"]).most_common(1)[0][0]
    zeror_freq = Counter(Bmask["oracle"]).most_common(1)[0][1]
    zeror_ratio = zeror_freq / len(Bmask)
    zeror_runtime = "runtime_" + zeror.lower()

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith("npb-")
    ]))

    B_out, BS_out = [], []
    for benchmark in benchmark_names:
        # CGO13 predictive model:
        clf = DecisionTreeClassifier(
            random_state=204, criterion="entropy", splitter="best")
        features = get_cgo13_features
        # clf = KNeighborsClassifier(n_neighbors=1)

        # Cross validate on baseline:
        B_out += leave_one_benchmark_out(clf, get_cgo13_features, B, benchmark)

        # Reset model.
        tmp = clone(clf)
        clf = tmp

        # Repeate cross-validation with synthetic kernels:
        BS_out += leave_one_benchmark_out(clf, get_cgo13_features, BS, benchmark)
        assert len(B_out) == len(BS_out)

    # Create results frame:
    R_out = []
    for b,bs in zip(B_out, BS_out):
        # Get runtimes of device using predicted device.
        b_p_runtime = b["runtime_" + b["p"].lower()]
        bs_p_runtime = bs["runtime_" + bs["p"].lower()]

        # Speedup is the ratio of runtime using the predicted device
        # over runtime using ZeroR device.
        b["p_speedup"] = b_p_runtime / b[zeror_runtime]
        bs["p_speedup"] = bs_p_runtime / bs[zeror_runtime]

        # Oracle is the ratio of runtime using the best device vs
        # runtime using predicted device.
        b["p_oracle"] = b["runtime"] / b_p_runtime
        bs["p_oracle"] = bs["runtime"] / bs_p_runtime

        # Get the benchmark name:
        benchmark_name = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                                b["benchmark"])
        b["benchmark_name"] = benchmark_name
        bs["benchmark_name"] = benchmark_name

        # Set the training data type.
        b["training"] = "Baseline"
        bs["training"] = "w. Synthesized Kernels"

        R_out.append(b)
        R_out.append(bs)
    R = pd.DataFrame(R_out)

    b_mask = R["training"] == "Baseline"
    bs_mask = R["training"] == "w. Synthesized Kernels"

    B_acc = labmath.mean(R[b_mask]["p_correct"])
    BS_acc = labmath.mean(R[bs_mask]["p_correct"])

    B_oracle = labmath.mean(R[b_mask]["p_oracle"])
    BS_oracle = labmath.mean(R[bs_mask]["p_oracle"])

    B_speedup = labmath.mean(R[b_mask]["p_speedup"])
    BS_speedup = labmath.mean(R[bs_mask]["p_speedup"])

    # Print analytics:
    print("Experiment 1 on Platform {}:".format(platform_id.upper()))
    print("  ZeroR device:            {} ({:.1f} %)"
          .format(zeror, zeror_ratio * 100))
    print()
    print("  Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
    print("  Accuracy w. synthesized: {:.1f} %".format(BS_acc * 100))
    print()
    print("  Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
    print("  Oracle w. synthesized:   {:.1f} %".format(BS_oracle * 100))
    print()
    print("  Speedup of baseline:     {:.2f} x".format(B_speedup))
    print("  Speedup w. synthesized:  {:.2f} x".format(BS_speedup))
    print()

    # Plot summary:
    ax = sns.barplot(x="benchmark_name", y="p_speedup", hue="training",
                     data=R, ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")
    # plt.ylim(0, 6)
    plt.axhline(y=1)

    # No legend title
    ax.get_legend().set_title("")

    figsize = (4.5, 2)
    plottype = "pdf"
    viz.finalise("data/img/ex1-{}.{}".format(platform_id.upper(), plottype),
                 figsize=figsize, tight=True)


def escape_benchmark_name(g):
    c = g.split('-')
    if c[0] == "amd":
        return "AMD SDK " + c[-2]
    elif c[0] == "nvidia":
        return "Nvidia SDK " + c[-2]
    elif c[0] == "shoc" or c[0] == "npb":
        return c[0].upper() + " " + c[-2]
    else:
        return c[0].capitalize() + " " + c[-2]


def experiment2():
    #
    # Cross-validate across all benchmarks using CGO13 model and our
    # own, with and without synthetic benchmarks. Report per-platform
    # speedup of our model over CGO13.
    #
    def compare_clfs(clf1, get_features1, clf2, get_features2, D1, D2, benchmark):
        test1_mask = D1["benchmark"].str.contains(r"^" + benchmark)
        test2_mask = D2["benchmark"].str.contains(r"^" + benchmark)
        assert(len(D1[test1_mask]) == len(D2[test2_mask]))

        # Create data masks. For training we exclude all results from
        # benchmark.
        train1_mask = ~test1_mask
        train2_mask = ~test2_mask

        # Create training and testing data:
        X1_train = get_features1(D1.loc[train1_mask])
        X2_train = get_features2(D2.loc[train2_mask])
        y1_train = get_labels(D1[train1_mask])
        y2_train = get_labels(D2[train2_mask])

        D1_test = D1[test1_mask]
        D2_test = D2[test2_mask]
        X1_test = get_features1(D1.loc[test1_mask])
        X2_test = get_features2(D2.loc[test2_mask])
        y1_test = get_labels(D1_test)
        y2_test = get_labels(D2_test)

        # Debugging printout:
        print("benchmark:", benchmark,
              "train on:", len(X1_train), "test on:", len(X1_test))

        # Train classifiers:
        clf1.fit(X1_train, y1_train)
        clf2.fit(X2_train, y2_train)

        # Make predictions
        predicted1 = clf1.predict(X1_test)
        predicted2 = clf2.predict(X2_test)

        D_out = []
        for d,y,p1,p2 in zip(D1_test.to_dict('records'), y1_test,
                             predicted1, predicted2):
            d["p1"], d["p2"] = p1, p2
            d["p1_correct"] = 1 if y == p1 else 0
            d["p2_correct"] = 1 if y == p2 else 0
            D_out.append(d)

        # Return a list of dicts
        return D_out


    aB = pd.read_csv("data/a/benchmarks.csv")
    aB["synthetic"] = np.zeros(len(aB))
    bB = pd.read_csv("data/b/benchmarks.csv")
    bB["synthetic"] = np.zeros(len(bB))
    B = pd.concat((aB, bB))

    aS = pd.read_csv("data/a/synthetics.csv")
    aS["synthetic"] = np.ones(len(aS))
    bS = pd.read_csv("data/b/synthetics.csv")
    bS["synthetic"] = np.ones(len(bS))
    S = pd.concat((aS, bS))

    aBS = pd.concat((aB, aS))
    bBS = pd.concat((bB, bS))
    BS = pd.concat((B, S))

    assert(len(B) == len(aB) + len(bB))
    assert(len(S) == len(aS) + len(bS))
    assert(len(BS) == len(aBS) + len(bBS))

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] # if b.startswith("parboil")
    ]))

    # Perform cross-validation:
    B_out = []
    for benchmark in benchmark_names:
        # CGO13 predictive model:
        cgo13_clf = DecisionTreeClassifier(
            random_state=204, criterion="entropy", splitter="best")
        cgo13_features = get_cgo13_features

        # our_clf = KNeighborsClassifier(3)
        # our_clf = SVC(kernel="linear", C=0.025)
        # our_clf = SVC(gamma=2, C=1)
        # our_clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
        # our_clf = AdaBoostClassifier()
        # our_clf = GaussianNB()
        # our_clf = LinearDiscriminantAnalysis()
        # our_clf = QuadraticDiscriminantAnalysis()
        our_clf = DecisionTreeClassifier(
            random_state=204, criterion="entropy", splitter="best")
        # clf = KNeighborsClassifier(n_neighbors=1)
        # our_features = get_our_features
        our_features = get_cgo13_features

        # Cross validate on baseline:
        tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                           aB, aBS, benchmark)
        for d in tmp:
            d["platform"] = "Platform A"
        B_out += tmp

        # Reset models.
        tmp = clone(cgo13_clf)
        cgo13_clf = tmp

        tmp = clone(our_clf)
        our_clf = tmp

        # Same as before, on other platform:
        tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                           bB, bBS, benchmark)
        for d in tmp:
            d["platform"] = "Platform B"
        B_out += tmp

        # Reset models.
        tmp = clone(cgo13_clf)
        cgo13_clf = tmp

        tmp = clone(our_clf)
        our_clf = tmp

    # Create results frame:
    R_out = []
    # Get runtimes of device using predicted device.
    for b in B_out:
        p1_runtime = b["runtime_" + b["p1"].lower()]
        p2_runtime = b["runtime_" + b["p2"].lower()]

        # Speedup is the ratio of runtime using our predicted device
        # over runtime using CGO13 predicted device.
        b["p_speedup"] = p2_runtime / p1_runtime

        # Oracle is the ratio of runtime using the best device vs
        # runtime using predicted device.
        b["p_oracle"] = b["runtime"] / p2_runtime

        # Get the benchmark name:
        b["benchmark_name"] = escape_benchmark_name(b["benchmark"])

        R_out.append(b)
    R = pd.DataFrame(R_out)

    print("Experiment 2:")

    improved = R[R["p_speedup"] > 1]

    print("Num Results:            ", len(R))
    print("Num matched or improved:", len(R[R["p_speedup"] >= 1]))
    print("Num improved:           ", len(R[R["p_speedup"] > 1]))
    print("Num worse:              ", len(R[R["p_speedup"] < 1]))

    Amask = R["platform"] == "Platform A"
    Bmask = R["platform"] == "Platform B"
    assert(len(R) == len(R[Amask]) + len(R[Bmask]))

    print("Num Instances:          ", len(R))

    a_matched_or_improved = R[Amask & R["p_speedup"] >= 1]
    b_matched_or_improved = R[Bmask & R["p_speedup"] >= 1]
    print("Matched or improved:    {:.1f} %"
          .format(((len(a_matched_or_improved) + len(b_matched_or_improved))
                   / float(len(R))) * 100))

    a_improved = R[Amask & R["p_speedup"] > 1]
    b_improved = R[Bmask & R["p_speedup"] > 1]
    print("Improved:               {:.1f} %"
          .format(((len(a_improved) + len(b_improved)) / float(len(R))) * 100))

    a_worsened = R[Amask & R["p_speedup"] < 1]
    b_worsened = R[Amask & R["p_speedup"] < 1]

    print("Worsened:               {:.1f} %"
          .format(((len(a_worsened) + len(b_worsened)) / float(len(R))) * 100))

    ax = sns.barplot(x="benchmark_name", y="p_speedup", hue="platform", data=R,
                     palette=sns.color_palette("Set1", n_colors=2, desat=.8),
                     ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")
    plt.axhline(y=1)
    plt.setp(ax.get_xticklabels(), rotation=90)

    plt.ylim(0,3)

    # No legend title
    ax.get_legend().set_title("")
    plt.legend(loc='upper right')
    ax.get_legend().draw_frame(True)

    plottype = "pdf"
    figsize = (10, 5)
    viz.finalise("data/img/ex2." + plottype, figsize=figsize, tight=True)


def main():
    motivation()
    experiment1("A")
    experiment1("B")
    experiment2()


if __name__ == "__main__":
    main()
