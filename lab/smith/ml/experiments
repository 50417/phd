#!/usr/bin/env python3
#
import matplotlib
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sklearn
import sys

from argparse import ArgumentParser
from collections import Counter
from copy import copy
from functools import partial
from itertools import combinations
from math import sqrt,ceil
from numpy.random import RandomState
from os import system
from random import seed,random
from sklearn.base import clone
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import normalize
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, export_graphviz


import labm8
from labm8 import fmt
from labm8 import fs
from labm8 import math as labmath
from labm8 import viz

import smith
from smith import cgo13

# Seaborn configuration:
sns.set(style="ticks", color_codes=True)
plt.style.use(["seaborn-white", "seaborn-paper"])


def get_cgo13_model():
    # return KNeighborsClassifier(1)
    return DecisionTreeClassifier(
        random_state=204,
        splitter="best",
        criterion="entropy",
        # max_depth=4,
        # min_samples_split=6,
        # min_samples_leaf=3
    )


def get_cgo13_features(D):
    return np.array([
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_our_model():
    return KNeighborsClassifier(1)
    # return SVC(kernel="linear", C=0.025)
    # return SVC(gamma=2, C=1)
    # return RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
    # return AdaBoostClassifier()
    # return GaussianNB()
    # return LinearDiscriminantAnalysis()
    # return QuadraticDiscriminantAnalysis()
    # return DecisionTreeClassifier(
    #     random_state=204, criterion="entropy", splitter="best")


def get_our_features(D):
    return np.array([
        D["comp"].values,
        D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        D["atomic"].values,
        D["transfer"].values,
        D["wgsize"].values,
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_raw_features(D):
    return np.array([
        D["comp"].values,
        D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        D["atomic"].values,
        D["transfer"].values,
        D["wgsize"].values,
    ]).T


def get_labels(D):
    return D["oracle"]


def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',
               lw=1, size=12, pad=8):
    """
    Credit: http://stackoverflow.com/a/11417222
    """
    for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],
                              ['left', 'bottom', 'right', 'top']):
        if c_spine != 'none':
            ax.spines[spine].set_color(c_spine)
            ax.spines[spine].set_linewidth(lw)
        else:
            ax.spines[spine].set_color('none')
    if (c_bottom == 'none') & (c_top == 'none'): # no bottom and no top
        ax.xaxis.set_ticks_position('none')
    elif (c_bottom != 'none') & (c_top != 'none'): # bottom and top
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom != 'none') & (c_top == 'none'): # bottom but not top
        ax.xaxis.set_ticks_position('bottom')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_bottom, labelsize=size, pad=pad)
    elif (c_bottom == 'none') & (c_top != 'none'): # no bottom but top
        ax.xaxis.set_ticks_position('top')
        ax.tick_params(axis='x', direction='out', width=lw, length=7,
                       color=c_top, labelsize=size, pad=pad)
    if (c_left == 'none') & (c_right == 'none'): # no left and no right
        ax.yaxis.set_ticks_position('none')
    elif (c_left != 'none') & (c_right != 'none'): # left and right
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left != 'none') & (c_right == 'none'): # left but not right
        ax.yaxis.set_ticks_position('left')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_left, labelsize=size, pad=pad)
    elif (c_left == 'none') & (c_right != 'none'): # no left but right
        ax.yaxis.set_ticks_position('right')
        ax.tick_params(axis='y', direction='out', width=lw, length=7,
                       color=c_right, labelsize=size, pad=pad)


def leave_one_benchmark_out(clf, get_features, D, benchmark):
    # Create data masks. For training we exclude all results from
    # the test benchmark.
    test_mask = D["benchmark"].str.contains(r"^" + benchmark)
    train_mask = ~test_mask

    # Create training and testing data:
    X_train = get_features(D[train_mask])
    y_train = get_labels(D[train_mask])

    D_test = D[test_mask]
    X_test = get_features(D_test)
    y_test = get_labels(D_test)

    # Train classifier:
    clf.fit(X_train, y_train)

    # Make predictions
    predicted = clf.predict(X_test)
    D_out = []
    for d,y,p in zip(D_test.to_dict('records'), y_test, predicted):
        d["p"] = p
        d["p_correct"] = 1 if y == p else 0
        D_out.append(d)

    # Return a list of dicts
    return D_out


def train_on_synthetics(clf, get_features, D, benchmark):
    # Create data masks. For training we exclude all results from
    # the test benchmark.
    test_mask = D["benchmark"].str.contains(r"^" + benchmark)
    train_mask = D["synthetic"] == 1

    # Create training and testing data:
    X_train = get_features(D[train_mask])
    y_train = get_labels(D[train_mask])

    D_test = D[test_mask]
    X_test = get_features(D_test)
    y_test = get_labels(D_test)

    print(len(X_train), len(X_test))

    # Train classifier:
    clf.fit(X_train, y_train)

    # Make predictions
    predicted = clf.predict(X_test)
    D_out = []
    for d,y,p in zip(D_test.to_dict('records'), y_test, predicted):
        d["p"] = p
        d["p_correct"] = 1 if y == p else 0
        D_out.append(d)

    # Return a list of dicts
    return D_out


def rand_jitter(arr, factor=0.01, randomstate=RandomState(204)):
    stdev = factor * (max(arr) - min(arr))
    return arr + randomstate.randn(len(arr)) * stdev


def scatter_with_jitter(plt, x, y, **kwargs):
    jitter_opts = kwargs.get("jitter_opts", {})
    if "jitter_opts" in kwargs: kwargs.pop("jitter_opts")

    jitter_factor = kwargs.get("jitter", None)
    if jitter_factor is not None:
        jitter_opts["factor"] = jitter_factor
        kwargs.pop("jitter")

    return plt.scatter(rand_jitter(x, **jitter_opts),
                       rand_jitter(y, **jitter_opts), **kwargs)

def motivation():
    #######################
    # Motivation Baseline #
    #######################
    clf = get_cgo13_model()
    platform = "b"
    suite = "parboil"

    # Load data and mask off the benchmark suite in use:
    B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
    test_mask = B["benchmark"].str.contains(r"^{}-".format(suite))
    B = B[test_mask]

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)", b).group(1)
        for b in B["benchmark"] if b.startswith(suite)
    ]))

    B_out = []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(clf, get_cgo13_features, B, benchmark)
    B_out = pd.DataFrame(B_out)
    assert(len(B) == len(B_out))

    pca = PCA(n_components=2)
    pca.fit(get_raw_features(B))

    X = pca.transform(get_raw_features(B))
    jitter = .075

    # Apply jitter and repack
    x,y = zip(*X)
    x = rand_jitter(x, jitter, RandomState(204))
    y = rand_jitter(y, jitter, RandomState(205))
    X = list(zip(x, y))

    correct = [x for x,b in zip(X,B_out.to_dict('records')) if b["p_correct"]]
    incorrect = [x for x,b in zip(X,B_out.to_dict('records')) if not b["p_correct"]]

    print()
    print("Motivation:")
    print("  #. correct:   ", len(correct),
          "({:.1f}%)".format((len(correct) / len(X)) * 100))
    print("  #. incorrect: ", len(incorrect))
    print()

    # Scatter type:
    plot_opts = { "s": 85, "alpha": .65 }
    plt.scatter(*zip(*incorrect),
                color="r", marker="v", label='Incorrect', **plot_opts)
    plt.scatter(*zip(*correct),
                color="b", marker="^", label='Correct', **plot_opts)

    ax = plt.gca()
    # No tick labels:
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    # customaxis(ax)

    # Axis labels
    plt.xlabel(r"$e_1 \rightarrow$", ha="right")
    plt.ylabel(r"$e_2 \rightarrow$", ha="right")

    # Position axis labels at end of axis
    ax.xaxis.set_label_coords(1, -.025)
    ax.yaxis.set_label_coords(-.025, 1)

    # Show legend
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles[::-1], labels[::-1])
    ax.get_legend().draw_frame(True)

    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    figsize = (2.5,2.5)
    viz.finalise(fs.path("~/phd/docs/wip-smith/img/motivation-a.pdf"),
                 figsize=figsize, tight=True)

    # Reset classifier
    clf = clone(clf)


    ##############################
    # With additional benchmarks #
    ##############################

    # Load data and mask off the benchmark suite in use:
    B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
    B["synthetic"] = np.zeros(len(B))
    S = pd.read_csv("data/{}/synthetics.csv".format(platform))
    S["synthetic"] = np.ones(len(S))
    BS = pd.concat((B, S))

    train_mask = (BS["benchmark"].str.contains(r"^" + suite) |
                  BS["synthetic"] == 1)
    test_mask = BS["benchmark"].str.contains(r"^" + suite)
    other_mask = BS["synthetic"] == 1

    Btrain = BS[train_mask]
    Btest = BS[test_mask]
    Bother = BS[other_mask]

    B_out = []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(
            clf, get_cgo13_features, Btrain, benchmark)
    B_out = pd.DataFrame(B_out)
    assert(len(Btest) == len(B_out))

    X = pca.transform(get_raw_features(Btest))

    # Apply jitter
    x,y = zip(*X)
    x = rand_jitter(x, jitter, RandomState(204))
    y = rand_jitter(y, jitter, RandomState(205))
    X = list(zip(x, y))

    correct = [
        x for x,b in zip(X,B_out.to_dict('records')) if b["p_correct"]]
    incorrect = [
        x for x,b in zip(X,B_out.to_dict('records')) if not b["p_correct"]]
    other = pca.transform(get_raw_features(Bother))

    print()
    print("Motivation w. other suite:")
    print("  #. correct:   ", len(correct),
          "({:.1f}%)".format((len(correct) / len(X)) * 100))
    print("  #. incorrect: ", len(incorrect))
    print("  #. additional:", len(Bother))
    print()

    scatter_with_jitter(plt, *zip(*other),
                        color="g", marker="o", label="Additional",
                        jitter=jitter, **plot_opts)
    plt.scatter(*zip(*incorrect),
                color="r", marker="v", label='Incorrect', **plot_opts)
    plt.scatter(*zip(*correct),
                color="b", marker="^", label='Correct', **plot_opts)

    ax = plt.gca()
    # No tick labels:
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    # customaxis(ax)

    # Axis labels
    plt.xlabel(r"$e_1 \rightarrow$", ha="right")
    plt.ylabel(r"$e_2 \rightarrow$", ha="right")

    # Position axis labels at end of axis
    ax.xaxis.set_label_coords(1, -.025)
    ax.yaxis.set_label_coords(-.025, 1)

    # Show legend
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles[::-1], labels[::-1])
    ax.get_legend().draw_frame(True)

    # Set same axis limits as before
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)

    # plt.show()
    viz.finalise(fs.path("~/phd/docs/wip-smith/img/motivation-b.pdf"),
                 figsize=figsize, tight=True)


    ##########################
    # Survey of GPGPU papers #
    ##########################

    # Load data from literature review:
    data = pd.read_csv("data/motivation-survey.csv", delimiter="\t")

    benchmark_counts = [x[1] for x in data.groupby("Paper").size().iteritems()]
    median_benchmark_counts = labmath.median(benchmark_counts)

    suites = pd.DataFrame(
        [(x[0], x[1] / median_benchmark_counts)
         for x in data.groupby(["Benchmark Suite"]).size().iteritems()],
        columns=["Suite", "#. benchmarks"])

    # To sort by frequency count:
    suites.sort_values("#. benchmarks", inplace=True, ascending=False)
    # To sort by alphabetical:
    # suites.sort_values("Suite", inplace=True)

    print(suites)

    ax = sns.barplot(x="Suite", y="#. benchmarks", data=suites)

    # customaxis(ax)

    # Add threshold line:
    plt.axhline(y=1, color="k", lw=1)

    # Rotate x labels:
    plt.setp(ax.get_xticklabels(), rotation=90)

    plt.xlabel("")
    plt.ylabel("#. benchmarks")

    figsize = (4, 2.35)
    viz.finalise(fs.path("~/phd/docs/wip-smith/img/motivation-c.pdf"),
                 figsize=figsize, tight=True)


counter = 0
def check_for_subset(candidates):
    global counter
    counter += 1
    print("\r\033[Ktest #.", counter, end="")
    sys.stdout.flush()

    clf = get_cgo13_model()
    platform = "b"
    suite = "parboil"

    # Load data and mask off the benchmark suite in use:

    # Scatter type:
    opts = { "s": 85, "alpha": .65 }

    # Reset classifier
    clf = clone(clf)

    # Load data and mask off the benchmark suite in use:
    B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
    S = pd.read_csv("data/{}/synthetics.csv".format(platform))
    BS = pd.concat((B, S))

    train_mask = BS["benchmark"].str.contains(r"^" + suite + "|" +
                                              "|".join(candidates))
    test_mask = BS["benchmark"].str.contains(r"^" + suite)
    other_mask = BS["benchmark"].str.contains("|".join(candidates))

    Btrain = BS[train_mask]
    Btest = BS[test_mask]
    Bother = BS[other_mask]
    assert(len(Bother) + len(Btest) == len(Btrain))

    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith(suite)
    ]))

    B_out = []
    for benchmark in benchmark_names:
        B_out += leave_one_benchmark_out(clf, get_cgo13_features, Btrain,
                                         benchmark)
    B_out = pd.DataFrame(B_out)
    assert(len(Btest) == len(B_out))

    num_incorrect = len(B_out) - sum(B_out["p_correct"])
    if num_incorrect > 3:
        return

    jitter = .05
    pca = PCA(n_components=2)
    pca.fit(get_raw_features(Btrain))
    X = pca.transform(get_raw_features(Btest))

    # Apply jitter
    x,y = zip(*X)
    x = rand_jitter(x, jitter, RandomState(204))
    y = rand_jitter(y, jitter, RandomState(205))
    X = list(zip(x, y))

    correct = [
        x for x,b in zip(X,B_out.to_dict('records')) if b["p_correct"]]
    incorrect = [
        x for x,b in zip(X,B_out.to_dict('records')) if not b["p_correct"]]

    other = pca.transform(get_raw_features(Bother))

    scatter_with_jitter(plt, *zip(*other), color="g", label='Correct',
                        jitter=jitter, **opts)
    plt.scatter(*zip(*incorrect), color="r", label='Incorrect', **opts)
    plt.scatter(*zip(*correct), color="b", label='Correct', **opts)

    ax = plt.gca()
    # No ticks
    ax.xaxis.set_major_locator(plt.NullLocator())
    ax.yaxis.set_major_locator(plt.NullLocator())

    # Axis labels
    plt.xlabel("e1 →", ha="right")
    plt.ylabel("e2 →", ha="right")

    # Position axis labels at end of axis
    ax.xaxis.set_label_coords(1, -.025)
    ax.yaxis.set_label_coords(-.025, 1)

    # Show legend
    # handles, labels = ax.get_legend_handles_labels()
    # ax.legend(handles, labels)
    # ax.get_legend().draw_frame(True)

    # Set same axis limits as before

    figsize = (2.5,2.5)

    with open("motivation-{}-{}-{}.txt".format(num_incorrect, len(candidates),
                                               counter), "w") as outfile:
        print("\n".join(candidates), file=outfile)

    viz.finalise("motivation-{}-{}-{}.png".format(num_incorrect,
                                                  len(candidates), counter),
                 figsize=figsize)


def speedup_with_synthetic_benchmarks(platform_id):
    #
    # Cross-validate CGO13 model across NPB benchmarks, with and
    # without synthetic benchmarks.
    #
    # Datasets:
    #
    #   B -  benchmarks
    #   S -  synthetics
    #   BS - benchmarks + synthetics
    #
    B = pd.read_csv("data/{platform_id}/benchmarks.csv"
                    .format(platform_id=platform_id.lower()))
    B["synthetic"] = np.zeros(len(B))

    S = pd.read_csv("data/{platform_id}/synthetics.csv"
                    .format(platform_id=platform_id.lower()))
    S["synthetic"] = np.ones(len(S))

    BS = pd.concat((B, S))

    # Find the ZeroR. This is the device which is most frequently
    # optimal:
    Bmask = B[B["benchmark"].str.contains("npb-3.3-")]

    zeror = Counter(Bmask["oracle"]).most_common(1)[0][0]
    zeror_freq = Counter(Bmask["oracle"]).most_common(1)[0][1]
    zeror_ratio = zeror_freq / len(Bmask)
    zeror_runtime = "runtime_" + zeror.lower()

    # Get the names of the benchmarks, in the form: $suite-$version-$benchmark
    benchmark_names = sorted(set([
        re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
        for b in B["benchmark"] if b.startswith("npb-")
    ]))

    B_out, BS_out = [], []
    for benchmark in benchmark_names:
        # CGO13 predictive model:
        clf = get_cgo13_model()
        features = get_cgo13_features
        # clf = KNeighborsClassifier(n_neighbors=1)

        # Cross validate on baseline:
        B_out += leave_one_benchmark_out(clf, get_cgo13_features, B, benchmark)

        # Reset model.
        clf = get_cgo13_model()

        # Repeate cross-validation with synthetic kernels:
        # BS_out += train_on_synthetics(clf, get_cgo13_features, BS, benchmark)
        BS_out += leave_one_benchmark_out(clf, get_cgo13_features, BS, benchmark)
        assert len(B_out) == len(BS_out)

    # Create results frame:
    R_out = []
    for b,bs in zip(B_out, BS_out):
        # Get runtimes of device using predicted device.
        b_p_runtime = b["runtime_" + b["p"].lower()]
        bs_p_runtime = bs["runtime_" + bs["p"].lower()]

        # Speedup is the ratio of runtime using the predicted device
        # over runtime using ZeroR device.
        b["p_speedup"] = b_p_runtime / b[zeror_runtime]
        bs["p_speedup"] = bs_p_runtime / bs[zeror_runtime]

        # Oracle is the ratio of runtime using the best device vs
        # runtime using predicted device.
        b["p_oracle"] = b["runtime"] / b_p_runtime
        bs["p_oracle"] = bs["runtime"] / bs_p_runtime

        # Get the group label, in the form $benchmark.$dataset:
        group = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                       b["benchmark"]) + "." + b["dataset"]
        b["group"] = group
        bs["group"] = group

        # Set the training data type.
        b["training"] = "Baseline"
        bs["training"] = "w. Synthesized"

        R_out.append(b)
        R_out.append(bs)
    R = pd.DataFrame(R_out)

    b_mask = R["training"] == "Baseline"
    bs_mask = R["training"] == "w. Synthesized"

    B_acc = labmath.mean(R[b_mask].groupby(["group"])["p_correct"].mean())
    BS_acc = labmath.mean(R[bs_mask].groupby(["group"])["p_correct"].mean())

    B_oracle = labmath.mean(R[b_mask].groupby(["group"])["p_oracle"].mean())
    BS_oracle = labmath.mean(R[bs_mask].groupby(["group"])["p_oracle"].mean())

    B_speedup = labmath.mean(R[b_mask].groupby(["group"])["p_speedup"].mean())
    BS_speedup = labmath.mean(R[bs_mask].groupby(["group"])["p_speedup"].mean())

    # Print analytics:
    print()
    print("Experiment 1 on Platform {}:".format(platform_id.upper()))
    print("  ZeroR device:            {} ({:.1f} %)"
          .format(zeror, zeror_ratio * 100))
    print()
    print("  Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
    print("  Accuracy w. synthesized: {:.1f} %".format(BS_acc * 100))
    print()
    print("  Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
    print("  Oracle w. synthesized:   {:.1f} %".format(BS_oracle * 100))
    print()
    print("  Speedup of baseline:     {:.2f} x".format(B_speedup))
    print("  Speedup w. synthesized:  {:.2f} x".format(BS_speedup))
    print()

    # Plot summary:
    ax = sns.barplot(x="group", y="p_speedup", hue="training",
                     data=R, ci=None)
    plt.ylabel("Speedup")
    plt.xlabel("")

    # FIXME: Apply hatch pattern to alternate bars:
    # for i,bar in enumerate(ax.patches):
    #     if not i % 2:
    #         bar.set_hatch("////")

    # Speedup line
    plt.axhline(y=1, color="k", lw=1)

    # No legend title
    ax.get_legend().set_title("")

    figsize = (9, 2.2)
    if platform_id == "B":
        plt.ylim(0, 15)
    plt.setp(ax.get_xticklabels(), rotation=90)

    # plt.show()
    viz.finalise(fs.path("~/phd/docs/wip-smith/img/ex1-{}.png"
                         .format(platform_id.upper())),
                 figsize=figsize, tight=True)


def eigenspace(platform_id):
    B = pd.read_csv("data/{}/benchmarks.csv".format(platform_id))
    S = pd.read_csv("data/{}/synthetics.csv".format(platform_id))
    BS = pd.concat((B, S))

    features = get_raw_features
    pca = PCA(n_components=2)
    pca.fit(features(BS))

    # benchmarks = pd.DataFrame([x for x in pca.transform(features(B))][:100],
    #                           columns=["E1", "E2"])
    # benchmarks["synthetic"] = np.zeros(len(benchmarks))
    # synthetics = pd.DataFrame([x for x in pca.transform(features(S))][:100],
    #                           columns=["E1", "E2"])
    # synthetics["synthetic"] = np.ones(len(synthetics))
    # eigens = pd.concat((benchmarks, synthetics))
    #
    # sns.pairplot(eigens, hue="synthetic")

    benchmarks = pca.transform(features(B))
    synthetics = pca.transform(features(S))[:100]

    plot_opts = { "s": 35, "alpha": .65 }
    plt.scatter(*zip(*benchmarks),
                color="r", marker="v", label='Benchmarks', **plot_opts)
    plt.scatter(*zip(*synthetics),
                color="b", marker="^", label='Synthetics', **plot_opts)
    # plt.xlim(3200000, 3700000)
    plt.ylim(2,5)

    plt.show()


def predictive_models():
    B = pd.read_csv("data/b/benchmarks.csv")
    S = pd.read_csv("data/b/synthetics.csv")
    BS = pd.concat((B, S))

    model1 = get_cgo13_model()
    model2 = get_cgo13_model()
    features = get_cgo13_features

    Btrain_mask = ~B["benchmark"].str.contains(r"^npb-3.3-CG")
    BStrain_mask = ~BS["benchmark"].str.contains(r"^npb-3.3-CG")

    BX_train = features(B[Btrain_mask])
    By_train = get_labels(B[Btrain_mask])

    BSX_train = features(BS[BStrain_mask])
    BSy_train = get_labels(BS[BStrain_mask])

    print("")
    model1.fit(BX_train, By_train)

    def visualize_tree(model, path, max_depth=None):
        export_graphviz(model, label=None, filled=True,
                        feature_names=["F1", "F2", "F3", "F4"],
                        impurity=False, proportion=True,
                        rounded=True, max_depth=max_depth)
        # Remove the 2nd and 3rd lines from split nodes:
        system(r"sed -r 's/\\n[0-9\.]+%\\n\[[0-9\.]+, [0-9\.]+\]//' -i tree.dot")

        # Convert .dot to .ps:
        system("dot -Tpng tree.dot -o " + fs.path(path))

        # Clean up
        system("rm tree.dot")


    model1.fit(BX_train, By_train)
    visualize_tree(model1, "~/phd/docs/wip-smith/img/tree-a.png", 2)

    model2.fit(BSX_train, BSy_train)
    visualize_tree(model2, "~/phd/docs/wip-smith/img/tree-b.png", 2)



def experiment1():
    # eigenspace("B")
    # predictive_models()
    speedup_with_synthetic_benchmarks("A")
    speedup_with_synthetic_benchmarks("B")


def escape_benchmark_name(g):
    c = g.split('-')
    if (c[0] == "shoc" or
        c[0] == "npb" or
        c[0] == "nvidia" or
        c[0] == "amd"):
        return c[0].upper() + "." + c[-2]
    else:
        return c[0].capitalize() + "." + c[-2]


def experiment2():
    #
    # Cross-validate across all benchmarks using CGO13 model and our
    # own, with and without synthetic benchmarks. Report per-platform
    # speedup of our model over CGO13.
    #
    def compare_clfs(clf1, get_features1, clf2, get_features2, D1, D2, benchmark):
        test1_mask = D1["benchmark"].str.contains(r"^" + benchmark)
        test2_mask = D2["benchmark"].str.contains(r"^" + benchmark)
        assert(len(D1[test1_mask]) == len(D2[test2_mask]))

        # Create data masks. For training we exclude all results from
        # benchmark.
        train1_mask = ~test1_mask
        train2_mask = ~test2_mask

        # Create training and testing data:
        X1_train = get_features1(D1.loc[train1_mask])
        X2_train = get_features2(D2.loc[train2_mask])
        y1_train = get_labels(D1[train1_mask])
        y2_train = get_labels(D2[train2_mask])

        D1_test = D1[test1_mask]
        D2_test = D2[test2_mask]
        X1_test = get_features1(D1.loc[test1_mask])
        X2_test = get_features2(D2.loc[test2_mask])
        y1_test = get_labels(D1_test)
        y2_test = get_labels(D2_test)

        # Train classifiers:
        clf1.fit(X1_train, y1_train)
        clf2.fit(X2_train, y2_train)

        # Make predictions
        predicted1 = clf1.predict(X1_test)
        predicted2 = clf2.predict(X2_test)

        D_out = []
        for d,y,p1,p2 in zip(D1_test.to_dict('records'), y1_test,
                             predicted1, predicted2):
            d["p1"], d["p2"] = p1, p2
            d["p1_correct"] = 1 if y == p1 else 0
            d["p2_correct"] = 1 if y == p2 else 0
            D_out.append(d)

        # Return a list of dicts
        return D_out


    aB = pd.read_csv("data/a/benchmarks.csv")
    aB["synthetic"] = np.zeros(len(aB))
    bB = pd.read_csv("data/b/benchmarks.csv")
    bB["synthetic"] = np.zeros(len(bB))
    B = pd.concat((aB, bB))

    aS = pd.read_csv("data/a/synthetics.csv")
    aS["synthetic"] = np.ones(len(aS))
    bS = pd.read_csv("data/b/synthetics.csv")
    bS["synthetic"] = np.ones(len(bS))
    S = pd.concat((aS, bS))

    aBS = pd.concat((aB, aS))
    bBS = pd.concat((bB, bS))
    BS = pd.concat((B, S))

    assert(len(B) == len(aB) + len(bB))
    assert(len(S) == len(aS) + len(bS))
    assert(len(BS) == len(aBS) + len(bBS))

    benchmark_names = sorted(set(B["benchmark"]))

    # Perform cross-validation:
    B_out = []
    for benchmark in benchmark_names:
        cgo13_clf = get_cgo13_model()
        our_clf = get_our_model()

        cgo13_features = get_cgo13_features
        our_features = get_our_features

        # Cross validate on baseline:
        tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                           aBS, aBS, benchmark)
        for d in tmp:
            d["platform"] = "Platform A"
        B_out += tmp

        # Reset models.
        cgo13_clf = get_cgo13_model()
        our_clf = get_our_model()

        # Same as before, on other platform:
        tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                           bBS, bBS, benchmark)
        for d in tmp:
            d["platform"] = "Platform B"
        B_out += tmp


    # Create results frame:
    R_out = []
    # Get runtimes of device using predicted device.
    for b in B_out:
        p1_runtime = b["runtime_" + b["p1"].lower()]
        p2_runtime = b["runtime_" + b["p2"].lower()]

        # Speedup is the ratio of runtime using our predicted device
        # over runtime using CGO13 predicted device.
        b["p_speedup"] = p2_runtime / p1_runtime

        # Oracle is the ratio of runtime using the best device vs
        # runtime using predicted device.
        b["p_oracle"] = b["runtime"] / p2_runtime

        # Get the benchmark name:
        b["group"] = escape_benchmark_name(b["benchmark"])

        R_out.append(b)
    R = pd.DataFrame(R_out)

    improved = R[R["p_speedup"] > 1]

    Amask = R["platform"] == "Platform A"
    Bmask = R["platform"] == "Platform B"
    assert(len(R) == len(R[Amask]) + len(R[Bmask]))

    a_matched_or_improved = R[Amask & R["p_speedup"] >= 1]
    b_matched_or_improved = R[Bmask & R["p_speedup"] >= 1]

    a_improved = R[Amask & R["p_speedup"] > 1]
    b_improved = R[Bmask & R["p_speedup"] > 1]

    a_worsened = R[Amask & R["p_speedup"] < 1]
    b_worsened = R[Amask & R["p_speedup"] < 1]

    print()
    print("Experiment 2:")
    print("  Num Results:            ", len(R))
    print("  Num matched or improved:", len(R[R["p_speedup"] >= 1]))
    print("  Num improved:           ", len(R[R["p_speedup"] > 1]))
    print("  Num worse:              ", len(R[R["p_speedup"] < 1]))
    print()
    print("  Matched or improved:     {:.1f} %"
          .format(((len(a_matched_or_improved) + len(b_matched_or_improved))
                   / float(len(R))) * 100))
    print("  Improved:                {:.1f} %"
          .format(((len(a_improved) + len(b_improved)) / float(len(R))) * 100))
    print("  Worsened:                {:.1f} %"
          .format(((len(a_worsened) + len(b_worsened)) / float(len(R))) * 100))
    print()

    ax = sns.barplot(x="group", y="p_speedup", hue="platform", data=R,
                     palette=sns.color_palette("Set1", n_colors=2, desat=.8),
                     ci=None)

    # FIXME: Apply hatch pattern to alternate bars:
    # for i,bar in enumerate(ax.patches):
    #     if not i % 2:
    #         bar.set_hatch("////")

    plt.ylabel("Speedup")
    plt.xlabel("")
    plt.axhline(y=1, color="k", lw=1)

    # Rotate x ticks
    plt.setp(ax.get_xticklabels(), rotation=90)

    # Set plot limit:
    plt.ylim(0,4)

    # No legend title
    ax.get_legend().set_title("")
    plt.legend(loc='upper right')
    ax.get_legend().draw_frame(True)

    figsize = (9, 4.5)
    viz.finalise(fs.path("~/phd/docs/wip-smith/img/ex2.pdf"),
                 figsize=figsize, tight=True)



def main():
    S = pd.read_csv("data/b/benchmarks.csv")
    # Smask = ~S["benchmark"].str.contains("parboil")
    Smask = S["benchmark"].str.contains("shoc")
    candidates = sorted(set(S[Smask]["benchmark"]))

    for n in range(1, 10):
        print()
        print("Trying all combinations of", n, "additions")
        for combination in combinations(candidates, n):
            check_for_subset(combination)
    print("Best results:", best, best_combination)

    motivation()
    # experiment1()
    # experiment2()


if __name__ == "__main__":
    main()
