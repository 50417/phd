% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\input{preamble}

% Document starts
\begin{document}

% Page heads
\markboth{C. Cummins et al.}{Autotuning Stencils Codes with Algorithmic Skeletons}

\title{Collaborative Performance Tuning for Algorithmic Skeletons}

\author{CHRIS CUMMINS, PAVLOS PETOUMENOS, MICHEL STEUWER, and HUGH LEATHER
\affil{University of Edinburgh}}

\begin{abstract}
  The physical limitations of microprocessor design have forced the
  industry towards increasingly heterogeneous designs to extract
  performance. This trend has not been matched with adequate software
  tools, leading to a growing disparity between the availability of
  parallelism and the ability for application developers to exploit
  it. Algorithmic skeletons simplify parallel programming by providing
  high-level, reusable patterns of computation. Achieving performant
  skeleton implementations is a difficult task; skeleton authors must
  attempt to anticipate and tune for a wide range of architectures and
  use cases. This results in implementations that target the general
  case and cannot provide the performance advantages that are gained
  from tuning low level optimization parameters. Autotuning combined
  with machine learning offers promising performance benefits in these
  situations, but the high cost of training and lack of available
  tools limits the practicality of autotuning for real world
  programming. We believe that performing autotuning at the level of
  the skeleton library can overcome these issues.

  In this work, we present OmniTune --- an extensible and distributed
  framework for dynamic autotuning of optimization parameters at
  runtime. OmniTune uses a client-server model with a flexible API to
  support machine learning enabled autotuning. Training data is shared
  across a network of cooperating systems, using a collective approach
  to performance tuning. We demonstrate the practicality of OmniTune
  in a case study using the algorithmic skeleton library SkelCL. We
  present three methodologies for predicting workgroup sizes. The
  first, using classifiers to select the optimal workgroup size. The
  second and third proposed methodologies employ the novel use of
  regressors for performing classification by predicting the runtime
  of kernels and the relative performance of different workgroup
  sizes, respectively. We evaluate the effectiveness of each technique
  in an empirical study of 429 combinations of architecture, kernel,
  and dataset, comparing an average of 629 different workgroup sizes
  for each. We find that autotuning provides a median $3.79\times$
  speedup over the best possible fixed workgroup size, achieving 94\%
  of the maximum performance. OmniTune achieves this without
  introducing a significant runtime overhead, and enables portable,
  cross-device and cross-program tuning.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10011007.10010940.10011003.10011002</concept_id>
        <concept_desc>Software and its engineering~Software performance</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10011007.10011006.10011041</concept_id>
        <concept_desc>Software and its engineering~Compilers</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
    <concept>
        <concept_id>10010520.10010575.10010577</concept_id>
        <concept_desc>Computer systems organization~Reliability</concept_desc>
        <concept_significance>100</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software performance}
\ccsdesc[300]{Software and its engineering~Compilers}
\ccsdesc[100]{Computer systems organization~Reliability}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

\keywords{%
  Autotuning,
  algorithmic skeletons,
  stencils,
  machine learning,
  collective tuning,
  adaptive optimization,
  GPGPU,
  OpenCL,
  workgroup size%
}

\acmformat{%
  Chris Cummins, Pavlos Petoumenos, Michel Steuwer, and Hugh Leather,
  2015. Autotuning Stencils Codes with Algorithmic Skeletons.%
}

\begin{bottomstuff}
  Corresponding Author: C. Cummins, University of Edinburgh; email: \url{c.cummins@ed.ac.uk}.\\*
  This work was supported by the UK Engineering and Physical Sciences
  Research Council under grants EP/L01503X/1 for the University of
  Edinburgh School of Informatics Centre for Doctoral Training in
  Pervasive Parallelism
  (\url{http://pervasiveparallelism.inf.ed.ac.uk/}), EP/H044752/1
  (ALEA), and EP/M015793/1 (DIVIDEND).
\end{bottomstuff}

\maketitle


\section{Introduction}\label{sec:introduction}

General purpose programming with GPUs has been shown to provide huge
parallel throughput, but poses a significant programming challenge,
requiring application developers to master an unfamiliar programming
model (such as provided by CUDA or OpenCL) and architecture (SIMD with
a multi-level memory hierarchy). As a result, GPGPU programming is
often considered beyond the realm of everyday development. If steps
are not taken to increase the accessibility of such parallelism, the
gap between potential and utilized performance will continue to widen
as hardware core counts increases.

Algorithmic skeletons offer a solution to this this
\emph{programmability challenge} by raising the level of
abstraction. This simplifies parallel programming, allowing developers
to focus on solving problems rather than coordinating parallel
resources. Skeleton frameworks provide robust parallel implementations
of common patterns of computation which developers parameterise with
their application-specific code~\cite{Gonzalez2010}. This greatly
reduces the challenge of parallel programming, allowing users to
structure their problem-solving logic sequentially, while offloading
the cognitive cost of parallel coordination to the skeleton library
author. The rising number of skeleton frameworks supporting graphics
hardware illustrates the demand for high level abstractions for GPGPU
programming~\cite{Enmyren2010,Marques2013}. The challenge is in
maintaining portable performance across the breadth of devices in the
rapidly developing GPU and heterogeneous architecture landscape.


\subsection{The Performance Portability Challenge}

There are many factors --- or \emph{parameters} --- which influence
the behavior of parallel programs. For example, setting the number of
threads to launch for a particular algorithm. The performance of
parallel programs is sensitive to the values of these parameters, and
when tuning to maximize performance, one size does not fit all. The
suitability of parameter values depends on the program implementation,
the target hardware, and the dataset that is operated upon. Iterative
compilation and autotuning have been shown to help in these cases by
automating the process of tuning parameter values to match individual
execution environments~\cite{Kisuki}. However, there have been few
attempts to develop general mechanisms for these techniques, and the
time taken to develop ad-hoc autotuning solutions and gather
performance data is often prohibitively expensive.

We believe that by embedding autotuning at the skeletal level, it is
possible to achieve performance with algorithmic skeletons that is
competitive with --- and in some cases, exceeds --- that of hand tuned
parallel implementations which traditionally came at the cost of many
man hours of work from expert programmers to develop. Incorporating
autotuning into algorithmic skeleton libraries has two key benefits:
first, it minimizes development effort by requiring only a
modification to the skeleton implementation rather than to every user
program; and second, by targeting a library, it enables a broader and
more substantive range of performance data to be gathered than with
ad-hoc tuning of individual programs.


\subsection{Contributions}

The key contributions of this work are:

\begin{itemize}
\item The design and implementation of a generic toolset for
  autotuning: \emph{OmniTune} is a novel and extensible framework for
  collaborative autotuning of optimization parameters across the life
  cycle of programs.
\item The integration of OmniTune with an established skeleton library
  for CPU and multi-GPU parallelism, SkelCL~\cite{Steuwer2011}. We
  extend SkelCL to provide autotuning for the selection of OpenCL
  workgroup size for Stencil skeletons, achieving 94\% of the maximum
  performance in an evaluation of 429 combinations of architectures
  and stencil programs.
\item A comparison of multiple approaches for runtime autotuning:
  using classifiers to predict optimal parameter values, using
  regressors to predict the absolute runtime of programs, and using
  regressors to predict the relative performance of different
  parameter values.
\end{itemize}

\section{Motivation}

\begin{figure}
\centering
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_1}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-1}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_2}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-2}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same stencil
      program on two different devices: (\subref{fig:motivation-1})
      Intel CPU, (\subref{fig:motivation-2}) NVIDIA GPU. Selecting an
      appropriate workgroup size depends on the execution device.%
    }
    \label{fig:motivation-arch}
  \end{minipage}%
}%
\hspace{2.5mm}
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_3}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-3}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_4}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-4}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same device
      executing two different stencil programs. Selecting an appropriate
      workgroup size depends on the program.%
    }
    \label{fig:motivation-prog}
  \end{minipage}%
}
\end{figure}

In this section we will briefly examine the performance impact of
selecting workgroup size for the SkelCL Stencil skeleton. A full
explanation of SkelCL and the workgroup size parameter space is given
Section~\ref{sec:omnitune-skelcl}.

SkelCL uses OpenCL to parallelise skeleton operations across many
threads. In OpenCL, multiple threads are grouped into
\emph{workgroups}. The shape and size of these groups is known to have
a big impact on performance. For the SkelCL stencil skeleton, the
selection of workgroup size presents a two dimensional parameter
space, consisting of a number of rows and columns ($w_r \times w_c$).
Measuring and plotting the runtime of stencil programs using different
workgroup sizes allows us to compare the performance of different
workgroup sizes for different combinations of architecture and
program. Figure~\ref{fig:motivation-arch} shows this performance
comparison for a single stencil program on two different devices,
demonstrating that a good choice of workgroup size is device
dependent. The optimization space of the same stencil benchmark on
different devices is radically different: not only does the optimal
workgroup size change between devices, but the performance of
suboptimal workgroup sizes is also dissimilar. The optimization space
of~\ref{fig:motivation-1} has a grid-like structure, with clear
performance advantages of workgroup sizes at multiples of 8 for
$w_c$. A developer specifically targeting this device would learn to
select workgroup sizes following this pattern. This domain specific
knowledge clearly does not transfer to the device shown
in~\ref{fig:motivation-2}.

In Figure~~\ref{fig:motivation-prog}, we compare the performance of
two different stencil programs on the \emph{same} device, showing that
workgroup size choice is also program dependent. In each of these four
examples, the optimal workgroup size changes, as does the relative
performance of suboptimal parameters. The average speedup of the best
over the worst workgroup size is $37.0\times$, and the best average
performance that can be achieved using a single fixed workgroup size
is only 63\% of the maximum.

SkelCL uses a fixed workgroup size by default. Since both the
execution device and the user-provided stencil code are not known
until runtime, selection of workgroup size should be made
dynamically. To the best of our knowledge, there is currently no such
generic system which meets our requirements for lightweight runtime
machine learning autotuning with distributed training sets, and as a
result, a variety of autotuners have been developed ad-hoc and on a
per-case basis.


\section{The OmniTune Framework}\label{sec:autotune}

\begin{figure}
\centering
\includegraphics[width=.6\columnwidth]{img/omnitune-system-overview.pdf}
\caption{%
  OmniTune system architecture, showing the separate components and
  the one to many relationship between servers to client applications,
  and remotes to servers.%
  % \vspace{-2em}%
}
\label{fig:omnitune-system-overview}
\end{figure}

OmniTune is a novel framework for extensible, distributed autotuning
of parameter values at runtime using machine learning. It serves as a
generic platform for developing autotuning solutions, aiming to reduce
both the engineering time required to target new optimization
parameters, and the time to deploy on new systems.

It emphasizes collaborative, online learning of optimization spaces. A
client-server architecture with clearly delineated separation of
concerns minimizes the code footprint in client applications, enabling
quick re-purposing for autotuning targets. OmniTune provides a
lightweight interface for communication between each of the
components, and aims to strike a balance between offering a fully
featured environment for quickly implementing autotuning, while
providing enough flexibility to cater to a wide range of use
cases. First, we describe the overall structure of OmniTune and the
rationale for the design, followed by the interfaces and steps
necessary to apply OmniTune.


\subsection{System Architecture}

Common implementations of autotuning in the literature either embed
the autotuning logic within each target application
(e.g.~\cite{Chen2014}), or take a standalone approach in which the
autotuner is a program which must be externally invoked by the user to
tune a target application (e.g.~\cite{Lutz2013}). Embedding the
autotuner within each target application has the advantage of
providing ``always-on'' behavior, but is infeasible for complex
systems in which the cost of building machine learning models must be
added to each program run. The standalone approach separates the
autotuning logic, at the expense of adding one additional step to the
build process. The approach taken in OmniTune aims to combine the
advantages of both techniques by implementing autotuning \emph{as a
  service}, in which a standalone autotuning server performs the heavy
lifting of managing training data and machine learning models, with a
minimal set of lightweight communication logic to be embedded in
target applications.

OmniTune is built around a three tier client-server model, shown in
Figure~\ref{fig:omnitune-system-overview}. The applications which are
to be autotuned are the \emph{clients}. These clients communicate with
a system-wide \emph{server}, which handles autotuning requests. The
server communicates and caches data sourced from a \emph{remote}
server, which maintains a global store of all autotuning data. There
is a many to one relationship between clients, servers, and remotes,
such that a single remote may handle connections to multiple servers,
which in turn may accept connections from multiple clients. This
design has two primary advantages: the first is that it decouples the
autotuning logic from that of the client program, allowing developers
to easily repurpose the autotuning framework to target additional
optimization parameters without a significant development overhead for
the target applications; the second advantage is that this enables
collective tuning, in which training data gathered from a range of
devices can be accessed and added to by any OmniTune server.

The OmniTune framework is implemented as a set of Python classes which
are extended to target specific parameters. The generic implementation
of OmniTune's server and remote components consists of 8987 lines of
Python and MySQL code. No client logic is provided, since that is use
case dependent (See Section~\ref{sec:omnitune-skelcl} for an example
implementation for SkelCL). Inter-process communication between client
programs and the server uses the D-Bus protocol. D-Bus is
cross-platform, and bindings are available for most major programming
languages, allowing flexibility for use with a range of
clients. Communication between servers and remotes uses TCP/IP (we
used an Amazon Web Services database instance for development).


\subsection{Autotuning Behavior}

The goal of machine learning enabled autotuning is to build models
from empirical performance data of past programs to select parameter
values for new \emph{unseen} programs. Instead of an iterative process
of trial and improvement, parameter values are \emph{predicted}, by
building correlations between performance, and \emph{features}
(explanatory variables). The data used to build such models is called
training data. OmniTune supports autotuning using a separate offline
training phase, online training, or a mixture of both. For each
autotuning-capable machine, an OmniTune server acts as an intermediary
between training data and the client application, and hosts the
autotuning logic. On launch, a server requests the latest training
data from the remote, which it uses to build the relevant models for
performing prediction of optimization parameter values. If additional
training data is gathered by the server, this can be uploaded to the
remote.

While the data types of the autotuning interface are
application-specific (e.g. a binary flag or one or more numeric
values), the general pattern is that a client application will request
parameter values from an OmniTune server by sending it a set of
explanatory variables. The server will then use machine learning
models to form a prediction for the optimal parameter values and
return these. Crucially, there is a mechanism provided for the client
to \emph{refuse} parameter values. This functionality is provided for
cases where the predicted parameter values are in some way invalid and
do not lead to a valid program. % TODO: citation

The server contains a library of machine learning tools to perform
parameter prediction, interfacing with the popular datamining software
suite Weka\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}} using
its Java Native Interface. The provided tools include classifiers,
regressors, and a selection of meta-learning algorithms.

OmniTune servers may perform additional feature extraction of
explanatory variables supplied by incoming client requests. The reason
for performing feature extraction on the server as opposed to on the
client side is that this allows the results of expensive operations
(for example, analyzing source code of target applications) to be
cached for use across the lifespan of client applications. The
contents of these local caches are periodically and asynchronously
synced with the remote to maintain a global store of lookup tables for
expensive operations.


\subsection{Interfaces}

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{img/omnitune-system-flow.pdf}
\caption{%
  Predicting parameter values and collecting training data with
  OmniTune.%
}
\label{fig:omnitune-system-flow}
\end{figure}

Key design elements of OmniTune are the interfaces exposed by the
server and remote components.

\paragraph{Client-Server} An OmniTune server exposes a public
interface over D-Bus with four operations. Client applications invoke
these methods to request parameter values, submit new training
observations, and refuse suggested parameters:
%
\begin{itemize}
\item \textsc{Request}$(x) \to p$\\*Given explanatory variables $x$,
  request the parameter values $p$ which are expected to provide
  maximum performance.
\item \textsc{RequestTraining}$(x) \to p$\\*Given explanatory
  variables $x$, allow the server to select parameter values $p$ for
  evaluating their fitness.
\item \textsc{Submit}$(x, p, y)$\\*Submit an observed measurement of
  fitness $y$ for parameter values $p$, given explanatory variables
  $x$.
\item \textsc{Refuse}$(x, p)$\\*Refuse parameter values $p$, given a
  set of explanatory variables $x$. Once refused, those parameters are
  blacklisted and will not be returned by any subsequent calls to
  \textsc{Request()} or \textsc{RequestTraining()} for the same
  explanatory variables $x$.
\end{itemize}
%
% This set of operations enables the core functionality of an autotuner,
% while providing flexibility for the client to control how and when
% training data is collected.

\paragraph{Server-Remote} The role of the remote is to provide
bookkeeping of training data for machine learning. Remotes allow
shared access to data from multiple servers using a transactional
communication pattern, supported by two methods:
%
\begin{itemize}
\item \textsc{Push}$(\bf{x}, \bf{p}, \bf{y})$\\*Asynchronously submit
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\item \textsc{Pull}$() \to (\bf{x}, \bf{p}, \bf{y})$\\*Request
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\end{itemize}


\subsection{Extensibility}

To extend OmniTune to target an optimization parameter, a developer
extends the server class to implement response handlers for the four
public interface operations, and then inserts client code into the
target application to call these operations. The implementation of
these response handlers and invoking client code determines the type
of autotuning methods supported. Figure~\ref{fig:omnitune-system-flow}
shows the flow diagram for an example OmniTune implementation. The
call to \textsc{RequestTraining()} is matched with a response call of
\textsc{Submit()}, showing the client recording a training
observation. In the next Section, we will detail the steps required to
apply OmniTune to SkelCL.


\section{Integration of OmniTune with SkelCL}\label{sec:omnitune-skelcl}

In this section we demonstrate the practicality of OmniTune by
integrating the framework into an established algorithmic skeleton
library. Introduced in~\cite{Steuwer2011}, SkelCL enables users to
easily harness the power of GPUs and CPUs for data parallel computing,
offering a set of OpenCL implementations of data parallel skeletons in
an object oriented C++ library. Skeletons are parameterised with user
functions which are compiled into OpenCL kernels for execution on
device hardware.

\subsection{The Stencil Skeleton}

\begin{figure}
  \centering
  \begin{subfigure}[h]{.55\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{img/lena-stencil}
  \end{subfigure}
  \begin{subfigure}[h]{.4\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{stencil}
  \end{subfigure}
  \caption{%
    Application of a Gaussian blur stencil operation to an image: the
    input pixels are decomposed into workgroups, consisting of
    $w_r \times w_c$ elements. Each element is mapped to a
    work-item. Each work-item operates on its corresponding element
    and a surrounding border region. In a Gaussian blur, pixel values
    are interpolated with neighboring pixels, producing a smoothed
    effect.%
  }
  \label{fig:stencil-img}
\end{figure}


Stencils are patterns of computation which operate on uniform grids of
data, where the value of each grid element (cell) is updated based on
its current value and the value of one or more neighboring elements,
called the \emph{border region}. Figure~\ref{fig:stencil-img} shows
the use of a stencil to apply a Gaussian blur to an image. SkelCL
provides a 2D stencil skeleton which allows users to provide a
function which updates a cell's value, while SkelCL orchestrates the
parallel execution of this function across all
cells~\cite{Steuwer2014a}. The border region is described by a
\emph{stencil shape}, which defines an $i \times j$ rectangular region
around each cell which is used to update the cell value. Stencil
shapes may be asymmetrical, and are defined in terms of the number of
cells in the border region to the north, east, south, and west of each
cell. Given a function $f$, a stencil shape $S$, and an $n \times m$
matrix with elements $x_{ij}$:
%
\begin{equation}
\scriptsize
% \begin{split}
\stencil \left( f, S,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix} \right)
\to
\begin{bmatrix}
  z_{11} & \cdots & z_{1m} \\
  \vdots & \ddots & \vdots \\
  z_{n1} & \cdots & z_{nm}
\end{bmatrix}
% \end{split}
\end{equation}
%
where:
%
\begin{equation}
\scriptsize
z_{ij} = f \left(
\begin{bmatrix}
  x_{i-S_n,j-S_w} & \cdots & x_{i-S_n,j+S_e} \\
  \vdots & \ddots & \vdots \\
  x_{i+S_s,j-S_w} & \cdots & x_{i+S_s,j+S_e}
\end{bmatrix} \right)
\end{equation}
%
For border region elements outside the bounds of the matrix, values
are substituted from either a predefined padding value, or the value
of the nearest element within the matrix, depending on user
preference. A popular usage of Stencil codes is for iterative problem
solving, whereby a stencil operation is repeated over a range of
discrete time steps $0 \le t \le t_{max}$, and $t \in \mathbb{N}$. An
iterative stencil operation $g$ accepts a customizing function $f$, a
Stencil shape $S$, and a matrix $M$ with initial values
$M_{init}$. The value of an iterative stencil can be defined
recursively as:
%
\begin{equation}
\scriptsize
g(f, S, M, t) =
\begin{cases}
  \stencil \left( f, S, g(f, S, M, t-1) \right),& \text{if } t \geq 1\\
  M_{init}, & \text{otherwise}
\end{cases}
\end{equation}
%
Examples of iterative stencils include cellular automata and partial
differential equation solvers.

In the implementation of the SkelCL stencil skeleton, each element in
the matrix is mapped to a unique thread (known as a \emph{work item}
in OpenCL) which applies the user-specified function. The work items
are then divided into \emph{workgroups} for execution on the target
hardware. Each work-item reads the value of its corresponding matrix
element and the surrounding elements defined by the border
region. Since the border regions of neighboring elements overlap, the
value of all elements within a workgroup are copied into a
\emph{tile}, allocated as a contiguous region of the fast, but small
local memory. As local memory access times are much faster than that
of global device memory, this greatly reduces the latency of the
border region memory accesses performed by each work item. Changing
the size of workgroups thus affects the amount of local memory
required for each workgroup, and in turn affects the number of
workgroups which may be simultaneously active on the device. While the
user defines the data size and type, the shape of the border region,
and the function being applied to each element, it is the
responsibility of the SkelCL stencil implementation to select an
appropriate workgroup size to use.


\subsection{Optimization Parameters}\label{subsec:op-params}

SkelCL stencil kernels are parameterised by a workgroup size $w$,
which consists of two integer values to denote the number of rows and
columns in a workgroup. The space of optimization parameter values is
subject to hard constraints, and these constraints cannot conveniently
be statically determined. Contributing factors are architectural
limitations, kernel constraints, and parameters which are refused for
other reasons.  Each OpenCL device imposes a maximum workgroup size
which can be statically checked. These are defined by architectural
limitations of how code is mapped to the underlying execution
hardware. Typical values are powers of two, e.g.\ 1024, 4096, 8192. At
runtime, once an OpenCL program has been compiled to a kernel, users
can query the maximum workgroup size supported by that particular
kernel dynamically. This value cannot easily be obtained statically as
there is no mechanism to determine the maximum workgroup size for a
given source code and device without first compiling it, which in
OpenCL does not occur until runtime.

Factors which affect a kernel's maximum workgroup size include the
number of registers required for a kernel, and the available number of
SIMD execution units for each type of instructions in a kernel. In
addition to satisfying the constraints of the device and kernel, not
all points in the workgroup size optimization space are guaranteed to
provide working programs. A \emph{refused parameter} is a workgroup
size which satisfies the kernel and architectural constraints, yet
causes a \texttt{CL\_OUT\_OF\_RESOURCES} error to be thrown when the
kernel is enqueued. Note that in many OpenCL implementations, this
error type acts as a generic placeholder and may not necessarily
indicate that the underlying cause of the error was due to finite
resources constraints. We define a \emph{legal} workgroup size as one
which, for a given \emph{scenario} $s$ (a combination of program,
device, and dataset), satisfies the architectural and kernel
constraints, and is not refused. The subset of all possible workgroup
sizes $W_{legal}(s) \subset W$ that are legal for a given scenario $s$
is then:
%
\begin{equation}
  W_{legal}(s) = \left\{w | w \in W, w < W_{\max}(s) \right\} - W_{refused}(s)
\end{equation}
%
Where $W_{\max}(s)$ can be determined at runtime prior to the kernels
execution, but the set $W_{refused}(s)$ can only be determined
experimentally.

The \emph{oracle} workgroup size $\Omega(s) \in W_{legal}(s)$ of a
scenario $s$ is the $w$ value which provides the lowest mean
runtime. The relative performance $p(s,w)$ of a particular workgroup
against the maximum available performance for that scenario, within
the range $0 \le p(s,w) \le 1$, is the ratio of the runtime of a
program with workgroup size $w$ over the oracle workgroup size
$\Omega(s)$. For a given workgroup size, the average performance
$\bar{p}(w)$ across a set of scenarios $S$ can be found using the
geometric mean of performance relative to the oracle:
%
\begin{equation}
  \bar{p}(w) =
  \left(
    \prod_{s \in S} p(s, w)
  \right)^{1/|S|}
\end{equation}
%
The \emph{baseline} workgroup size $\bar{w}$ is the value which
provides the best average case performance across a set of
scenarios. Such a baseline value represents the \emph{best} possible
performance which can be achieved using a single, statically chosen
workgroup size. By defining $W_{safe} \in W$ as the intersection of
legal workgroup sizes, the baseline can be found using:
%
\begin{align}
W_{safe} &= \cap \left\{ W_{legal}(s) | s \in S \right\}\\
\bar{w} &= \argmax_{w \in W_{safe}} \bar{p}(w)
\end{align}


\subsection{Machine Learning}

This section presents three contrasting methods for predicting
workgroup sizes of \emph{unseen} scenarios.


\subsubsection{Predicting Oracle Workgroup Sizes}

\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{scenario $s$}
  \KwResult{workgroup size $w$}
  \SetKwRepeat{Do}{do}{while}

  $w \leftarrow \text{classify}(f(s))$\;
  $W \leftarrow \left\{ w | w < W_{\max}(s) \right\} - W_{refused}(s)$\;
  \While{$w \not\in W_{legal}(s)$}{%
    $W = W - \{w\}$\;
    \eIf{random}{%
      $w \leftarrow $ random selection $w \in W$\;
    }{%
      $d_{min} \leftarrow \infty$; $w_{closest} \leftarrow \text{null}$\;
      \For{$c \in W$}{
        $d \leftarrow \sqrt{\left(c_r - w_r\right)^2 + \left(c_c - w_c\right)^2}$\;
        \If{$d < d_{min}$}{%
          $d_{min} \leftarrow d$\;
          $w_{closest} \leftarrow c$\;
        }
      }
      $w \leftarrow w_{closest}$\;
    }
  }
\caption{Prediction using classifiers}
\label{alg:autotune-classification}
\end{algorithm}

The first approach is detailed in
Algorithm~\ref{alg:autotune-classification}. By considering the set of
possible workgroup sizes as a hypothesis space, we train a classifier
to predict, for a given set of features, the \emph{oracle} workgroup
size. The oracle workgroup size $\Omega(s)$ is the workgroup size
which provides the lowest mean runtime $t(s,w)$ for a scenario $s$:
%
\begin{equation}
  \Omega(s) = \argmin_{w \in W_{legal}(s)} t(s,w)
\end{equation}
%
Training a classifier for this purpose requires pairs of stencil
features $f(s)$ to be labeled with their oracle workgroup size for a
set of training scenarios $S_{training}$:
%
\begin{equation}
  D_{training} = \left\{ \left(f(s), \Omega(s)\right) | s \in S_{training} \right\}
\end{equation}
%
After training, the classifier predicts workgroup sizes for unseen
scenarios from the set of oracle workgroup sizes from the training
set. This is a common and intuitive approach to autotuning, in that a
classifier predicts the best parameter value based on what worked well
for the training data. However, given the constrained space of
workgroup sizes, this presents the problem that future scenarios may
have different sets of legal workgroup sizes to that of the training
data, i.e.:
%
\begin{equation}
  \bigcup_{\forall s \in S_{future}} W_{legal}(s) \nsubseteq \left\{ \Omega(s) | s \in S_{training} \right\}
\end{equation}
%
This results in an autotuner which may predict workgroup sizes that
are not legal for all scenarios, either because they exceed
$W_{\max}(s)$, or because parameters are refused,
$w \in W_{refused}(s)$. For these cases, we evaluate the effectiveness
of three \emph{fallback strategies}:
%
\begin{enumerate}
\item \emph{Baseline} --- select the workgroup size which provides the
  highest average case performance from the set of safe workgroup sizes.
\item \emph{Random} --- select a random workgroup size which is
  expected from prior observations to be legal.
\item \emph{Nearest Neighbor} --- select the workgroup size which
  from prior observations is expected to be legal, and has the lowest
  Euclidian distance to the prediction.
\end{enumerate}


\subsubsection{Predicting Kernel Runtimes}

\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{scenario $s$, regressor $R(x, w)$, fitness function $\Delta(x)$}
  \KwResult{workgroup size $w$}
  \SetKwRepeat{Do}{do}{while}

  $W \leftarrow \left\{ w | w < W_{\max}(s) \right\} - W_{refused}(s)$\;
  % \Comment Candidates.
  \Do{$w \not\in W_{legal}(s)$}{%
    $w \leftarrow \underset{w \in W}{\argmax} \Delta(R(f(s), w))$\;
    % \Comment Select best candidate.
    $W \leftarrow W - \left\{ w \right\}$\;
    % \Comment Remove candidate from selection.
  }
\caption{Prediction using regressors}
\label{alg:autotune-regression}
\end{algorithm}

A problem of predicting oracle workgroup sizes is that, for each
training instance, an exhaustive search of the optimization space must
be performed in order to find the oracle workgroup size. An
alternative approach is to instead predict the expected \emph{runtime}
of a kernel given a specific workgroup size. Given training data
consisting of $(f(s),w,t)$ tuples, where $f(s)$ are scenario features,
$w$ is the workgroup size, and $t$ is the observed runtime, we train a
regressor $R(f(s), w)$ to predict the runtime of scenario and
workgroup size combinations. The selected workgroup size
$\bar{\Omega}(s)$ is then the workgroup size from a pool of candidates
which minimizes the output of the
regressor. Algorithm~\ref{alg:autotune-regression} formalizes this
approach of autotuning with regressors. A fitness function $\Delta(x)$
computes the reciprocal of the predicted runtime so as to favor
shorter over longer runtimes. Note that the algorithm is self
correcting in the presence of refused parameters --- if a workgroup
size is refused, it is removed from the candidate pool, and the next
best candidate is chosen. This removes the need for fallback
handlers. Importantly, this technique allows for training on data for
which the oracle workgroup size is unknown, and has the secondary
advantage that this allows for an additional training instance to be
gathered every time a kernel is evaluated.


\subsubsection{Predicting Relative Performance}

Accurately predicting the runtime of arbitrary code is a difficult
problem. It may instead be more effective to predict the relative
performance of two different workgroup sizes for the same kernel. To
do this, we predict the \emph{speedup} of a workgroup size over a
baseline. This baseline is the workgroup size which provides the best
average case performance across all scenarios and is known to be
safe. Such a baseline value represents the \emph{best} possible
performance which can be achieved using a single, fixed workgroup
size. As when predicting runtimes, this approach performs
classification using regressors
(Algorithm~\ref{alg:autotune-regression}). We train a regressor
$R(f(s), w)$ to predict the relative performance of workgroup size $w$
over a baseline parameter for scenario $s$. The fitness function
returns output of the regressor, so the selected workgroup size
$\bar{\Omega}(s)$ is the workgroup size from a pool of candidates
which is predicted to provide the best relative performance. This has
the same advantageous properties as predicting runtimes, but by
training using relative performance, we negate the challenges of
predicting dynamic code behavior.


\section{Experimental Setup}

To evaluate the performance of the presented autotuning techniques, an
exhaustive enumeration of the workgroup size optimization space for
429 combinations of architecture, program, and dataset was performed.

Table~\ref{tab:hw} describes the experimental platforms and OpenCL
devices used. Each platform was unloaded, frequency governors
disabled, and benchmark processes set to the highest priority
available to the task scheduler. Datasets and programs were stored in
an in-memory file system.

In addition to the synthetic stencil benchmarks described in
Section~\ref{subsec:training}, six stencil kernels taken from four
reference implementations of standard stencil applications from the
fields of image processing, cellular automata, and partial
differential equation solvers are used: Canny Edge Detection, Conway's
Game of Life, Heat Equation, and Gaussian Blur. For each program,
dataset sizes of size $512\times512$, $1024\times1024$,
$2048\times2048$, and $4096\times4096$ were used.

All runtimes were recorded with millisecond precision using OpenCL's
Profiling API to record the kernel execution time. The workgroup size
space was enumerated for each combination of $w_r$ and $w_c$ values in
multiples of 2, up to the maximum workgroup size. A minimum of 30
samples were recorded for each scenario and workgroup size.

Program behavior is validated by comparing program output against a
gold standard output collected by executing each of the real-world
benchmarks programs using the baseline workgroup size. The output of
real-world benchmarks with other workgroup sizes is compared to this
gold standard output to test for correct program execution.

Five different classification algorithms are used to predict oracle
workgroup sizes, chosen for their contrasting properties: Naive Bayes,
SMO, Logistic Regression, J48 Decision tree, and Random
Forest~\cite{Han2011}. For regression, a Random Forest with regression
trees is used, chosen because of its efficient handling of large
feature sets compared to linear models~\cite{Breiman1999}. The
autotuning system is implemented in Python as a system daemon. SkelCL
stencil programs request workgroup sizes from this daemon, which
performs feature extraction and classification.

\begin{table}
\tiny
\centering
\begin{tabular}{l R{1.1cm} | l R{1.1cm} r R{1.2cm} R{1.2cm} R{1.2cm}}
\toprule
Host & Host Memory &  OpenCL Device &  Compute units & Frequency & Local Memory & Global Cache & Global Memory \\
\midrule
Intel i5-2430M & 8 GB  & CPU              &              4 &   2400 Hz &        32 KB &       256 KB &       7937 MB \\
Intel i5-4570  & 8 GB  & CPU              &              4 &   3200 Hz &        32 KB &       256 KB &       7901 MB \\
Intel i7-3820  & 8 GB  & CPU              &              8 &   1200 Hz &        32 KB &       256 KB &       7944 MB \\
Intel i7-3820  & 8 GB  & AMD Tahiti 7970  &             32 &   1000 Hz &        32 KB &        16 KB &       2959 MB \\
Intel i7-3820  & 8 GB  & Nvidia GTX 590   &              1 &   1215 Hz &        48 KB &       256 KB &       1536 MB \\
Intel i7-2600K & 16 GB & Nvidia GTX 690   &              8 &   1019 Hz &        48 KB &       128 KB &       2048 MB \\
Intel i7-2600  & 8 GB  & Nvidia GTX TITAN &             14 &    980 Hz &        48 KB &       224 KB &       6144 MB \\
\bottomrule
\end{tabular}
\caption{Specification of experimental platforms and OpenCL devices.}
\label{tab:hw}
\end{table}


\section{Performance Results}\label{sec:evaluation}

This section describes the performance results of enumerating the
workgroup size optimization space. The effectiveness of autotuning
techniques for exploiting this space are examined in
Section~\ref{sec:evaluation}. The experimental results consist of
measured runtimes for a set of \emph{test cases}, where a test case
$\tau_i$ consists of a scenario, workgroup size pair
$\tau_i = (s_i,w_i)$, and is associated with a \emph{sample} of
observed runtimes of the program. A total of 269813 test cases were
evaluated, which represents an exhaustive enumeration of the workgroup
size optimization space for 429 scenarios. For each scenario, runtimes
for an average of 629 (max 7260) unique workgroup sizes were
measured. The average sample size for each test case is 83 (min 33,
total 16917118).

The workgroup size optimization space is non-linear and complex, as
shown in Figure~\ref{fig:oracle-wgsizes}, which plots the distribution
of optimal workgroup sizes. Across the 429 scenarios, there are 135
distinct optimal workgroup sizes (31.5\%). The average speedup of the
oracle workgroup size over the worst workgroup size for each scenario
is $15.14\times$ (min $1.03\times$, max $207.72\times$).

Of the 8504 unique workgroup sizes tested, 11.4\% were refused in one
or more test cases, with an average of 5.5\% test cases leading to
refused parameters. There are certain patterns to the refused
parameters: for example, workgroup sizes which contain $w_c$ and $w_r$
values which are multiples of eight are less frequently refused, since
eight is a common width of SIMD vector
operations~\cite{IntelCorporation2012}. However, a refused parameter
is an obvious inconvenience to the user, as one would expect that any
workgroup size within the specified maximum should generate a working
program, if not a performant one.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/oracle_param_space}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:oracle-wgsizes}
  \end{subfigure}
  ~%
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/coverage_space}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:coverage}
  \end{subfigure}
  \vspace{-.5em}
  \caption{%
    Log frequency counts for: (\subref{fig:oracle-wgsizes})
    optimality, and (\subref{fig:coverage}) legality for a subset of
    the aggregated workgroup size optimization space,
    $w_c \le 100, w_r \le 100$. The space of oracle workgroup size
    frequencies is highly irregular and uneven, with a most frequently
    optimal workgroup size of $w_{(64 \times 4)}$. Legality
    frequencies are highest for smaller row and column counts (where
    $w < W_{\max}(s) \forall s \in S$), and $w_c$ and $w_r$ values
    which are multiples of 8.%
  }
\label{fig:oracle-wgsizes}
\end{figure}

Experimental results suggest that the problem is vendor --- or at
least device --- specific. Figure~\ref{fig:refused-params} shows the
ratio of refused test cases, grouped by device. We see many more
refused parameters for test cases on Intel CPU devices than any other
type, while no workgroup sizes were refused by the AMD GPU. The exact
underlying cause for these refused parameters is unknown, but can
likely by explained by inconsistencies or errors in specific OpenCL
driver implementations. Note that the ratio of refused parameters
decreases across the three generations of Nvidia GPUs: GTX 590 (2011),
GTX 690 (2012), and GTX TITAN (2013). For now, it is imperative that
any autotuning system is capable of adapting to these refused
parameters by suggesting alternatives when they occur.

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{img/num_params_oracle.pdf}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  unique workgroup sizes. The greatest accuracy that can be achieved
  using a single statically chosen workgroup size is 15\%. Achieving
  50\% oracle accuracy requires a minimum of 14 distinct workgroup
  sizes.%
}
\label{fig:oracle-accuracy}
\end{figure}

The baseline parameter is the workgroup size providing the best
overall performance while being legal for all scenarios. Because of
refused parameters, only a \emph{single} workgroup size
$w_{(4 \times 4)}$ from the set of experimental results is found to
have a legality of 100\%, suggesting that an adaptive approach to
setting workgroup size is necessary not just for the sake of
maximizing performance, but also for guaranteeing program
execution. The utility of the baseline parameter is that it represents
the best performance that can be achieved through static tuning of the
workgroup size parameter; however, compared to the oracle workgroup
size for each scenario, the baseline parameter achieves only 24\% of
the optimal performance.


We find that the \emph{human expert} selected workgroup size is
invalid for 2.6\% of scenarios, as it is refused by 11 test cases. By
device, these are: 3 on the GTX 690, 6 on the i5-2430M, and 2 on the
i5-4570. For the purpose of comparing performance against human
experts, we will ignore these test cases, but it demonstrates the
utility of autotuning not just for maximizing performance, but
ensuring program reliability. For the scenarios where the human expert
workgroup size \emph{is} legal, it achieves an impressive geometric
mean of 79.2\% of the oracle performance. The average speedup of
oracle workgroup sizes over the workgroup size selected by a human
expert is $1.37\times$ (min $1.0\times$, max $5.17\times$).

The utility of the \emph{baseline} workgroup size is that it
represents the best performance that can be achieved through static
tuning. The baseline workgroup size achieves only 24\% of the maximum
performance. Figures~\ref{fig:performance-wgsizes}
and~\ref{fig:performances} show box plots for the performance of all
workgroup sizes using different groupings: ratio of maximum workgroup
size, kernel, device, and dataset. The plots show the median
performance, interquartile range, and outliers. What is evident is
both the large range of workgroup size performances (i.e. the high
performance upper bounds), and the lack of obvious correlations
between any of the groupings and performance.

\begin{figure}
\centering
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_wgsize}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-max-wgsize}
    \end{subfigure}
    \\
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_c}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-wg-c}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_r}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-wg-r}
    \end{subfigure}
    \caption{%
      Comparing performance of workgroup sizes relative to the oracle as
      a function of: (\subref{fig:performance-max-wgsize})~maximum legal
      size, (\subref{fig:performance-wg-c})~number of columns, and
      (\subref{fig:performance-wg-r})~number of rows. Each workgroup
      size is normalized to the maximum allowed for that scenario,
      $W_{\max}(s)$. There is no clear correlation between workgroup
      size and performance.%
    }
    \label{fig:performance-wgsizes}
  \end{minipage}%
}%
\hspace{2.5mm}
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_kernels.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-kernels}
    \end{subfigure}
    \\
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_devices.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-devices}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_datasets.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-datasets}
    \end{subfigure}
    \caption{%
      Performance relative to the oracle of workgroup sizes across all
      test cases, grouped by:
      (\subref{fig:performance-kernels})~kernels,
      (\subref{fig:performance-devices})~devices, and
      (\subref{fig:performance-datasets})~datasets. The performance
      impact is not consistent across kernels, devices, or datasets. The
      Intel i7-3820 has the lowest performance gains from tuning
      workgroup size.%
    }
    \label{fig:performances}
  \end{minipage}%
}
\end{figure}


\begin{figure}
  \centering
  \vspace{-1em}
  \includegraphics[width=.4\columnwidth]{refused_params_by_device}
  \vspace{-1em}
  \caption{%
    The ratio of test cases with refused workgroup sizes, grouped by
    OpenCL device ID. No parameters were refused by the AMD device.
    \vspace{-1em}%
  }
\label{fig:refused-params}
\end{figure}


\section{Autotuning Evaluation}

In this section we evaluate the effectiveness of the three proposed
autotuning techniques for predicting performant workgroup sizes. For
each autotuning technique, we partition the experimental data into
training and testing sets. Three strategies for partitioning the data
are used: the first is a 10-fold cross-validation; the second is to
divide the data such that only data collected from synthetic
benchmarks are used for training and only data collected from the
real-world benchmarks are used for testing; the third strategy is to
create leave-one-out partitions for each unique device, kernel, and
dataset. For each combination of autotuning technique and testing
dataset, we evaluate each of the workgroup sizes predicted for the
testing data using the following metrics:
%
\begin{itemize}
\item time (real) --- the time taken to make the autotuning
  prediction. This includes classification time and any communication
  overheads.
\item accuracy (binary) --- whether the predicted workgroup size is
  the true oracle, $w = \Omega(s)$.
\item validity (binary) --- whether the predicted workgroup size
  satisfies the workgroup size constraints constraints,
  $w < W_{\max}(s)$.
\item refused (binary) --- whether the predicted workgroup size is
  refused, $w \in W_{refused}(s)$.
\item performance (real) --- the performance of the predicted
  workgroup size relative to the oracle for that scenario.
\item speedup (real) --- the relative performance of the predicted
  workgroup size relative to the baseline workgroup size
  $w_{(4 \times 4)}$.
\end{itemize}
%
The \emph{validty} and \emph{refused} metrics measure how often
fallback strategies are required to select a legal workgroup size
$w \in W_{legal}(s)$. This is only required for the classification
approach to autotuning, since the process of selecting workgroup sizes
using regressors respects workgroup size constraints.


\subsection{Predicting Oracle Workgroup Size}

Figure~\ref{fig:class-syn} shows the results when classifiers are
trained using data from synthetic benchmarks and tested using
real-world benchmarks. With the exception of the ZeroR, a dummy
classifier which ``predicts'' only the baseline workgroup size
$w_{\left( 4 \times 4 \right)}$, the other classifiers achieve good
speedups over the baseline, ranging from $4.61\times$ to $5.05\times$
when averaged across all test sets. The differences in speedups
between classifiers is not significant, with the exception of
SimpleLogistic, which performs poorly when trained with synthetic
benchmarks and tested against real-world programs. This suggests the
model over-fitting to features of the synthetic benchmarks which are
not shared by the real-world tests. Of the three fallback handlers,
\textsc{NearestNeighbour} provides the best performance, indicating
that it successfully exploits structure in the optimization space.


\subsection{Predicting with Regressors}

Figure~\ref{fig:regression-class} shows a summary of results for
autotuning using regressors to predict kernel runtimes
(\ref{fig:runtime-class-xval}) and speedups
(\ref{fig:speedup-class-xval}). Of the two regression techniques,
predicting the \emph{speedup} of workgroup sizes is much more
successful than predicting the \emph{runtime}. This is most likely
caused by the inherent difficulty in predicting the runtime of
arbitrary code, where dynamic factors such as flow control and loop
bounds are not captured by the instruction counts which are used as
features for the machine learning models. The average speedup achieved
by predicting runtimes is $4.14\times$. For predicting speedups, the
average is $5.57\times$, the highest of all of the autotuning
techniques.


\subsection{Autotuning Overheads}

Comparing the classification times of Figures~\ref{fig:class-syn}
and~\ref{fig:regression-class} shows that the prediction overhead of
regressors is significantly greater than classifiers. This is because,
while a classifier makes a single prediction, the number of
predictions required of a regressor grows with the size of
$W_{\max}(s)$, since classification with regression requires making
predictions for all $w \in \left\{ w | w < W_{\max}(s) \right\}$. The
fastest classifier is the J48 decision tree, due to the it's
simplicity --- it can be implemented as a sequence of nested
\texttt{if} and \texttt{else} statements. The measured overhead of
autotuning using this classifier is 2.5ms, of which only 0.3ms is
required for classification using Weka, although an optimized decision
tree implementation could reduce this further. The remaining 2.2ms is
required for feature extraction and the inter-process round trip
between the OmniTune server and client.

\begin{figure}
\centering
\includegraphics[width=.55\columnwidth]{classification-syn-real}
\vspace{-.7em}
\caption{%
  Autotuning performance using classifiers and synthetic benchmarks. Each
  classifier is trained on data collected from synthetic stencil
  applications, and tested for prediction quality using data from 6
  real-world benchmarks. Each of the different values correspond to a
  different data partitioning strategy, e.g.\ cross-kernel
  partitioning, 10-fold validation, etc. 95\% confidence intervals are
  shown where appropriate.
}
\label{fig:class-syn}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[h]{.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{runtime-class-xval}
\vspace{-2em}
\caption{}
\label{fig:runtime-class-xval}
\end{subfigure}
\begin{subfigure}[h]{.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{speedup-class-xval}
\vspace{-2em}
\caption{}
\label{fig:speedup-class-xval}
\end{subfigure}
\vspace{-.5em}
\caption{%
  Autotuning performance for each type of test dataset using
  regressors to predict: (\subref{fig:runtime-class-xval}) kernel
  runtimes, and (\subref{fig:speedup-class-xval}) relative performance
  of workgroup sizes.%
}
\label{fig:regression-class}
\end{figure}


\subsection{Comparison with Human Expert}

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{speedup-distributions}
\vspace{-1.2em}
\caption{%
  Violin plot of speedups over \emph{human expert}, ignoring cases
  where the workgroup size selected by human experts is
  invalid. Classifiers are using \textsc{NearestNeighbour} fallback
  handlers. Horizontal dashed lines show the median, Q1, and
  Q3. Kernel Density Estimates show the distribution of results. The
  speedup axis is fixed to the range 0--2.5 to highlight the IQRs,
  which results in some outlier speedups > 2.5 being clipped.%
}
\label{fig:speedup-distributions}
\end{figure}

In the original implementation of the SkelCL stencil
pattern~\cite{Steuwer2014a}, a workgroup size of $w_{(32 \times 4)}$
was selected in an evaluation of 4 stencil operations on a Tesla S1070
system. In our evaluation of 429 combinations of kernel, architecture,
and dataset, we found that this workgroup size is refused by 2.6\% of
scenarios, making it unsuitable for use as a baseline. However, if we
remove the scenarios for which $w_{(32 \times 4)}$ is \emph{not} a
legal workgroup size, we can directly compare the performance against
the autotuning predictions.

Figure~\ref{fig:speedup-distributions} plots the distributions and
Interquartile Range (IQR) of all speedups over the human expert
parameter for each autotuning technique. The distributions show
consistent classification results for the five classification
techniques, with the speedup at Q1 for all classifiers being
$\ge 1.0\times$. The IQR for all classifiers is $< 0.5$, but there are
outliers with speedups both well below $1.0\times$ and well above
$2.0\times$. In contrast, the speedups achieved using regressors to
predict runtimes have a lower range, but also a lower median and a
larger IQR. Clearly, this approach is the least effective of the
evaluated autotuning techniques. Using regressors to predict relative
performance is more successful, achieving the highest median speedup
of all the techniques ($1.33\times$).


\subsection{OmniTune Extensibility}

The client-server architecture OmniTune neatly separates the
autotuning logic from the target application. This makes adjusting the
autotuning methodology a simple process. To demonstrate this, we
changed the machine learning algorithm from a J48 decision tree to a
Naive Bayes classifier, and duplicated the evaluation. This required
only a single line of source code in the OmniTune server extension to
be changed. Figure~\ref{fig:class-hmaps} visualizes the differences in
autotuning predictions when changing between these two
classifiers. While the average performances of the two classifiers is
comparable, the distribution of predictions is not. For example, the
Naive Bayes classifier predicted the human expert selected workgroup
size of $w_{(32 \times 4)}$ more frequently than it was optimal, while
the decision tree predicted it less frequently. Selection of machine
learning algorithms has a large impact on the effectiveness of
autotuning, and the OmniTune client-server design allows for low cost
experimenting with different approaches. In future work we will
investigate meta-tuning techniques for selecting autotuning
algorithms.

\begin{figure}
\centering
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-1}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-2}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-3}
\end{subfigure}
\\
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_5}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-4}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_runtime_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-5}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_speedup_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-6}
\end{subfigure}
\caption[Classification error heatmaps]{%
  Heatmaps of classification errors for 10-fold cross-validation,
  showing a subset of the optimization space. The shading in each
  cells indicates if it is predicted less frequently (blue), ore more
  frequently (red) than it is optimal. Color gradients are normalized
  across plots.%
}
\label{fig:class-hmaps}
\end{figure}


\subsection{Summary}

In this section we have explored the performance impact of the
workgroup size optimization space, and the effectiveness of autotuning
using OmniTune to exploit this. By comparing the relative performance
of an average of 629 workgroup sizes for each of 429 scenarios, the
following conclusions can be drawn:

%
\begin{itemize}
\item The performance gap between the best and workgroup sizes for a
  particular combination of hardware, software, and dataset is up to
  $207.72\times$.
\item Not all workgroup sizes are legal, and the space of legal
  workgroup sizes cannot statically be determined. Adaptive tuning is
  required to ensure reliable performance.
\item Statically tuning workgroup size fails to extract the potential
  performance across a range of programs, architectures, and
  datasets. The best statically chosen workgroup size achieves only
  26\% of the optimal performance.
\item Workgroup size prediction using a decision tree achieves an
  average 90\% of the optimal performance.
\item Autotuning provides performance portability across programs,
  devices, and datasets. The performance of predicted workgroup sizes
  for unseen devices is within 8\% of the performance for known
  devices.
\end{itemize}


\section{Related Work}\label{sec:related}

Early work in autotuning applied iterative search techniques to the
space of compiler optimisations~\cite{Bodin1998,Kisuki}. Since then,
machine learning techniques have been successfully employed to reduce
the cost of iterative
compilation~\cite{Agakov,Stephenson2003,Fursin2011}. However,
optimizing GPGPU programs presents different challenges to that of
traditional CPU programming. Speedups of up to $432\times$ for matrix
multiplication in CUDA were demonstrated in~\cite{Ryoo2008a} through
the appropriate use of zero-overhead thread scheduling, memory
bandwidth, and thread grouping. The importance of proper exploitation
of local shared memory and synchronization costs is explored
in~\cite{Lee2010}. In~\cite{Chen2014}, data locality optimisations are
automated using a description of the hardware and a
memory-placement-agnostic compiler. \cite{Magni2014} use a machine
learning model to predict optimal thread coarsening factors of OpenCL
kernels, demonstrating speedups between $1.11\times$ and $1.33\times$.

\cite{Ganapathi2009} demonstrated early attempts at autotuning
multicore stencil codes, drawing upon the successes of statistical
machine learning techniques in the compiler community. They use Kernel
Canonical Correlation Analysis to build correlations between stencil
features and optimization parameters. Their use of KCCA restricts the
scalability of their system, as the complexity of model building grows
exponentially with the number of features. A code generator and
autotuner for 3D Jacobi stencil codes is presented
in~\cite{Zhang2013a}, although their approach requires a full
enumeration of the parameter space for each new program, and has no
cross-program learning. A DSL and CUDA code generator for stencils is
presented in~\cite{Kamil2010}. Unlike the SkelCL stencil pattern, the
generated stencil codes do not exploit fast local device
memory. Autotuning memory transfers for multi-GPU stencils is explored
in~\cite{Lutz2013} using a combination of offline tuning and runtime
parameter search. A comparison of tuning for CPU and GPU architectures
is made in~\cite{Christen2011}, although the authors did not compare
differences between GPUs. The automatic generation of synthetic
benchmarks using parameterised template substitution is presented
in~\cite{Chiu2015}. The authors describe an application of their tool
for generating OpenCL stencil kernels for machine learning, but do not
report any performance results.

OpenTuner~\cite{Ansel2013} and CLTune~\cite{Nugteren2015} are examples
of generic autotuning frameworks which use iterative search techniques
to explore optimization spaces. In OpenTuner, using ensemble search
techniques; in CLTune, using simulated annealing or particle
swarms. Since OpenTuner and CLTune do not \emph{learn} optimization
spaces as OmniTune does, performance data is not shared across
devices. This means that the search for performant parameter values
must be performed by each new device to be autotuned. Our approach
combines machine learning with distributed training sets so that new
users automatically benefit from the collective tuning experience of
other users, which reduces the time to deployment.

% A generic autotuner for OpenCL kernels is presented
% in~\cite{Nugteren2015}, using iterative search techniques to find
% optimal parameters on a per-case basis, rather than \emph{learning}
% parameter spaces.

A ``big data'' driven approach to autotuning is presented
in~\cite{Fursin2014}. The authors propose the use of ``Collective
optimization'' to leverage training experience across devices, by
sharing performance data, datasets and additional metadata about
experimental setups. In addition to the mechanism for sharing training
datasets, our system provides the capabilities of performing
autotuning at runtime using a lightweight inter-process communication
interface. Additionally, Collective Mind uses a NoSQL JSON format for
storing datasets. The relational schema used in OmniTune offers
greater scaling performance and lower storage overhead as the amount
of performance data grows. The authors do not provide an empirical
evaluation of their technique.


\section{Conclusions and Future Work}\label{sec:conclusions}

As the trend towards increasingly programmable heterogeneous
architectures continues, the need for high level, accessible
abstractions to manage such parallelism will continue to
grow. Autotuning proves to be a valuable aid for achieving these
goals, provided that the burden of development and collecting
performance data is lifted from the user. The system presented in this
paper aims to solve this issue by providing a generic interface for
implementing machine learning-enabled autotuning.

OmniTune is a novel framework for autotuning which has the benefits of
a fast, ``always-on'' interface for client applications, while being
able to synchronize data with global repositories of knowledge which
are built up across devices. To demonstrate the utility of this
framework, we implemented a frontend for predicting the workgroup size
of OpenCL kernels for SkelCL stencil codes. This optimization space is
complex, non linear, and critical for the performance of stencil
kernels.  Selecting the correct workgroup size is difficult ---
requiring a knowledge of the kernel, dataset, and underlying
architecture. To the best of our knowledge, this work constitutes the
first attempt to autotune the workgroup size of high-level stencil
patterns. The autotuning techniques proposed in this paper achieve up
to 94\% of the maximum performance, providing average speedups of
$5.57\times$ over static tuning, while providing robust fallbacks in
the presence of unexpected behavior of OpenCL driver
implementations. Of the three techniques proposed, predicting the
relative performances of workgroup sizes using regressors provides the
highest median speedup, whilst predicting the oracle workgroup size
using decision tree classifiers adds the lowest runtime overhead. This
presents a trade-off between classification time and training time,
which could be explored in future work using a hybrid of the
classifier and regressor techniques presented in this paper.

In future work we will further explore the transition towards online
machine learning, developing methods for the collaborative exploration
of optimization spaces in parallel across multiple cooperating
devices. This will be combined with the use of adaptive sampling plans
to minimize the number of observations required to distinguish bad
from good parameter values.


% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{../../../library}


\end{document}
