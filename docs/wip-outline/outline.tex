% The outline proposal needs to identify the research area as
% something rather more specific than (say) Neuroinformatics, Natural
% Language Processing or Automated Reasoning. For those students who
% started the programme with no topic, one would expect an outline
% proposal of around 2-3 pages, detailing the specific research area
% and problem to be addressed, plus a very brief description of the
% idea the student will pursue for solving it.
%
% For those students who started their PhD programme with a specific
% research proposal already, this proposal should be much more fleshed
% out (e.g., 15 pages), providing more details of both the research
% problem and research methods, a more detailed justification of the
% approach and how it will be evaluated, and a scheduled work plan. If
% the student is in a position to write this more detailed proposal at
% this point in the programme, then this can be used as part of the
% deliverable (perhaps with minor amendments) for the formal First
% Year Review in month 9, together with a separate document that
% describes the research achievements to date and how they relate to
% the work plan described in the proposal. Details of the documents to
% be submitted for the First Year Review are given below under “Month
% 9”.
%
% The literature review that forms a part of this submission must be
% relevant to the research area defined in the proposal and serve to
% justify the approach taken in the proposed thesis. The course notes
% for the MSc "Informatics Research Review" course has some notes on
% the form and content of a literature review and how to go about
% tracking down relevant publications.
%
\input{preamble}

% Document starts
\begin{document}

\title{Proposal}

\author{Chris Cummins}

\maketitle

\begin{abstract}
  The physical limitations of microprocessor design have forced the
  industry towards increasingly heterogeneous designs to extract
  performance, with an an increasing pressure to offload traditionally
  CPU based workloads to the GPU. This trend has not been matched with
  adequate software tools; the popular languages OpenCL and CUDA
  provide a very low level model with little abstraction above the
  hardware. Programming at this level requires expert knowledge of
  both the domain and the target hardware, and achieving performance
  requires laborious hand tuning of each program. This has led to a
  growing disparity between the availability of parallelism in modern
  hardware, and the ability for application developers to exploit it.

  The goal of this work is to bring the performance of hand tuned
  heterogeneous code to high level programming, by incorporating
  autotuning into \textit{Algorithmic Skeletons}. Algorithmic
  Skeletons simplify parallel programming by providing reusable,
  high-level, patterns of computation. However, achieving performant
  skeleton implementations is a difficult task; skeleton authors must
  attempt to anticipate and tune for a wide range of architectures and
  use cases. This results in implementations that target the general
  case and cannot provide the performance advantages that are gained
  from tuning low level optimization parameters for individual
  programs and architectures. Autotuning combined with machine
  learning offers promising performance benefits by tailoring
  parameter values to individual cases, but the high cost of training
  and the ad-hoc nature of autotuning tools limits the practicality of
  autotuning for real world programming. We believe that performing
  autotuning at the level of the skeleton library can overcome these
  issues.
\end{abstract}

\tableofcontents


\section{Introduction}

Tuning parallel programs is hard, and consumes untold man-hours of
labor. The automation of this task is a much sought after goal, but
current approaches are ad-hoc, impractical, and unable to generalize
across the broad space of hardware and software. Developers are no
longer able to rely on increasing clock speeds to increase the
performance of their programs, and must instead rely on
paralellization using an increasingly exotic range of heterogeneous
architectures and accelerators. The shape of this landscape is
changing faster than developers ability to tune for it. A more robust
technique that is capable of automatically tuning parallel programs is
required in order to

the application of machine learning to

interesting because of the implications of ``cloud computing'' meets
autotuning


\subsection{Motivation}

% Applicable to everything from smartphones to big data processors.

\subsection{Contributions}

\subsection{Algorithmic Skeletons}

Introduced by \citeauthor{Cole1989} in \citeyear{Cole1989},
algorithmic skeletons simplify the task of parallel programming by
abstracting common patterns of communication, providing parallel
implementations of higher order functions~\cite{Cole1989}. The
interfaces to generic parallel algorithms exposed by algorithmic
skeletons are parameterised by the user with \emph{muscle functions}
that implement problem specific logic. The idea is that this allows
the user to focus on solving the problem at hand, affording greater
ease of use by automating the coordination of parallel resources.

Algorithmic skeletons are categorised as either \emph{data} parallel
or \emph{task} parallel. In data parallel skeletons, data are
distributed across nodes for parallel processing, where each parallel
node executes the same code on a unique subset of the data. Examples
of data parallel algorithmic skeletons include \emph{map}, \emph{zip},
and \emph{reduce}.

Task parallel skeletons treat the data as a singular object and
instead parallelise the execution of multiple tasks. Tasks are
assigned to threads, which can communicate with each other by passing
data between threads. Examples of task parallel algorithmic skeletons
include \emph{pipe}, \emph{task farm}, and \emph{for loops}.

Algorithmic Skeleton Frameworks (ASkFs) provide concrete parallel
implementations of parallel patterns, which are parameterised by the
user to generate specific problem solving programs. The interfaces
exposed by frameworks must be sufficiently generic to allow users to
express a range of problems.

Implementations of algorithmic skeletons abound, targeting a range of
different use cases and host languages. Notable examples include:
eSkel~\cite{Benoit2005a}, Skandium~\cite{Leyton2010}, and
FastFlow~\cite{Aldinucci2011}. The most prevalent form of ASkF is that
of the standalone library which exposes a set of public APIs, shown in
Figure~\ref{fig:askf}. See~\cite{Gonzalez2010} for a more exhaustive
review of ASkFs in the research literature.

In industry, Google's MapReduce~\cite{Dean2008} and Intel's Thread
Building Blocks~\cite{IntelTBB} have utilised a similar approach to
abstracting the coordination of parallel resources as in algorithmic
skeletons, to great commercial success, although they do not advertise
themselves as such.

The Orléans Skeleton Library for C++ has exceptions~\cite{Legaux2013}.


\section{Literature Review}


Evaluating GPGPU benchmarks~\cite{Ryoo2015}.

\subsection{Skeleton-aware compilation}

What are the limits of compiler optimisations for parallel code?

\paragraph{Limits of static analysis} Sequential consistency in
shared-memory parallel
programming~\cite{Krishnamurthy1995,Shasha1988,Sura2005}. Sequential
consistency and caches~\cite{Goodman}.

\paragraph{Parallelism in programming languages} Programming languages
have taken a variety of approaches in adopting parallelism. In C, a
threading model was retrofitted using the \textit{POSIX pthreads}
standard~\cite{Sura2005}. \citeauthor{Boehm2005} contends this
approach in~\cite{Boehm2005}, describing issues in which a compiler
developed independently of threading issues cannot be guaranteed to
produce correct code. These issues were circumvented in C++11 by the
introduction of a concurrency model~\cite{Boehm2008}, based off the
success of the Java memory model~\cite{Bash2015a}. C++ and Java are
two of the most popular imperative programming languages, and in both
cases, the languages specify memory models which guarantee sequential
consistency across threads, subject to some restrictions.

Sequential consistency for Java incurs a 10\%
slowdown~\cite{Sura2005}.

Titanium is a dialect of Java targeting scientific computing. It
extends the Java feature set, adding multidimensional arrays,
immutable classes, and a PGAS programming model~\cite{Yelick1998}. In
contrast, Deterministic Parallel Java adds a type and effect system to
in order to provide a provably sound language
core~\cite{Bocchino2009}.

Parallelism in the modern functional languages: Haskell, Clojure and
Erlang~\cite{Pierro2012}.

Scala actors~\cite{Haller2009a}, and~\cite{Haller2012}.

% TODO: Go, Fortran


\paragraph{Library level parallelism}

Optimising MapReduce for multi-core processors~\cite{Kaashoek2010}.

% OpenMP, MPI

% Compiler optimisations for parallel programs

% * Optimisations for skeletons
%   * “skeletons" - Intel TBB, std::algorithm, FastFlow, MapReduce
%   * Composition and nesting.
%   * Rewrite rules:

%     Patterns and Rewrite Rules for Systematic Code Generation From
%     High-Level Functional Patterns to High-Performance OpenCL Code

%       A

% \subsubsection{Autotuning for parallel programs}

%   * Autotuning for skeletons


\paragraph{Autotuning Algorithmic Skeletons}
\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to predict the optimal execution device (i.e.\ CPU, GPU) for a
given program by predicting execution time and memory copy overhead
based on problem size. The autotuner only supports vector operations,
and there is limited cross-architecture
evaluation. In~\cite{Dastgeer2015a}, the authors extend SkePU to
improve the data consistency and transfer overhead of container types,
reporting up to a $33.4\times$ speedup over the previous
implementation.

\citeauthor{Contreras2008} enumerated the optimisation space of Intel
Thread Building Blocks in~\cite{Contreras2008}, showing that runtime
knowledge of the available parallel hardware can have a significant
impact on program performance. \citeauthor{Collins2012} exploit this
in~\cite{Collins2012}, using Principle Components Analysis to reduce
the dimensionality of the parameter optimisation space and then search
across it. In~\cite{Collins2013} they extend this using static feature
extraction and nearest neighbour classification to further prune the
search space.

Autotuning Stencils on GPUs using synthetically generated
kernels~\cite{Garvey2015b}.

Automated GPU kernel transformations~\cite{Wahib2015a}.


\subsection{Skeleton-aware debugging}

General purpose, platform-specific debuggers: Intel Debugger for
Linux~\cite{Blair-chappell}, Microsoft Windows Debugger.


\subsubsection{Debugging Parallel Frameworks and Libraries}

Allinea DDT~\cite{K2010}, a scalable parallel debugger.

TotalView~\cite{Cownie2000}, an OpenMP debugger.

Jumbune~\cite{ImpetusTechnologies}, an open source debugger for
Hadoop.

The parallel pipeline library FlumeJava~\cite{Chambers2010} provides
debugging support through regular tools using a sequential execution
mode,r and routing of exceptions and output from remote workers back to
a central host.


\subsubsection{Debugging GPGPU programs}

CUDA-GDB~\cite{NVIDIA2016}, Intel OpenCL Debugger for Linux
OS~\cite{IntelCorporation2016}.

Oclgrind~\cite{Price2015}.

GPUverify~\cite{Betts2012}.


\subsubsection{Debugging Algorithmic Skeletons}

SkIE~\cite{Bacci1999} is based on a coordination language, but
provides advanced features such as debugging tools, performance
analysis, visualization, and graphical user interface. Instead of
directly using the coordination language, programmers interact with a
graphical tool, where parallel modules based on skeletons can be
composed

SKiPPER~\cite{Cezeaux1999} supports sequential execution for
debugging.


\subsection{Skeleton-aware profiling}

Profiling sequential programs~\cite{Ball1994}.

\paragraph{Profiling parallel programs} Profiling parallel programs
using KOJAK~\cite{Mohr2003}.


\section{Summary of Progress}

ADAPT~\cite{Cummins2015a}, HLPGPU~\cite{Cummins2016}.


\section{Methodology}

Use geometric mean for speedups~\cite{Fleming1986}. Statistical
rigour\cite{Georges2007}. Execution times and
variance~\cite{Box}. Benchmarking parallel computing
systems~\cite{Belli2015}.


\subsection{Skeleton-aware compiler optimizations}

First steps:
%
\begin{enumerate}
\item Write a test case program
\item Compile to bytecode and check optimisation is not applied
\item Implement optimisation by hand to source code
\item Measure performance improvement
\item Implement compiler pass to apply optimisation to bytecode
\item Test on real world benchmarks
\end{enumerate}
%
Example: constant propagation across muscle functions in pipe:
%
\begin{enumerate}
\item Write a pipeline of 100 "+1” functions.
\item Compile to bytecode and check that pipeline isn’t collapsed.
\item Collapse pipeline to single stage.
\item Measure performance improvement.
\item Implement compiler pass to apply optimisation to bytecode.
\item Test on real world benchmarks.
\end{enumerate}
%
Next step: load balancing across pipelines - stream partitioning?

Related work: dataflow analysis across skeletons for scheduling, what
kind of optimisations are applied in TLB/actor frameworks, actors in
scala


\subsection{Skeleton-aware debugging}
debugging+profiling for checkpointing

TBB debugging

exoskeleton

SkePU related workx


\section{Work Plan}

Paper 1, Paper 2, Paper 3.


\section{Conclusions}


\newpage
\label{bibliography}
\printbibliography


\end{document}
