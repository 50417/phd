% The outline proposal needs to identify the research area as
% something rather more specific than (say) Neuroinformatics, Natural
% Language Processing or Automated Reasoning. For those students who
% started the programme with no topic, one would expect an outline
% proposal of around 2-3 pages, detailing the specific research area
% and problem to be addressed, plus a very brief description of the
% idea the student will pursue for solving it.
%
% For those students who started their PhD programme with a specific
% research proposal already, this proposal should be much more fleshed
% out (e.g., 15 pages), providing more details of both the research
% problem and research methods, a more detailed justification of the
% approach and how it will be evaluated, and a scheduled work plan. If
% the student is in a position to write this more detailed proposal at
% this point in the programme, then this can be used as part of the
% deliverable (perhaps with minor amendments) for the formal First
% Year Review in month 9, together with a separate document that
% describes the research achievements to date and how they relate to
% the work plan described in the proposal. Details of the documents to
% be submitted for the First Year Review are given below under “Month
% 9”.
%
% The literature review that forms a part of this submission must be
% relevant to the research area defined in the proposal and serve to
% justify the approach taken in the proposed thesis. The course notes
% for the MSc "Informatics Research Review" course has some notes on
% the form and content of a literature review and how to go about
% tracking down relevant publications.
%
\input{preamble}

% Document starts
\begin{document}

\title{Outline}

\author{Chris Cummins}

\maketitle

\begin{abstract}
  The physical limitations of microprocessor design have forced the
  industry towards increasingly heterogeneous designs to extract
  performance. This trend has not been matched with adequate software
  tools, leading to a growing disparity between the availability of
  parallelism and the ability for application developers to exploit
  it. Algorithmic skeletons simplify parallel programming by providing
  high-level, reusable patterns of computation. Achieving performant
  skeleton implementations is a difficult task; skeleton authors must
  attempt to anticipate and tune for a wide range of architectures and
  use cases. This results in implementations that target the general
  case and cannot provide the performance advantages that are gained
  from tuning low level optimization parameters. Autotuning combined
  with machine learning offers promising performance benefits in these
  situations, but the high cost of training and lack of available
  tools limits the practicality of autotuning for real world
  programming. We believe that performing autotuning at the level of
  the skeleton library can overcome these issues.
\end{abstract}


\section{Introduction}

\subsection{Motivation}

% Applicable to everything from smartphones to big data processors.

\section{Background}


\section{Literature Review}


\subsection{Skeleton-aware compilation}

What are the limits of compiler optimisations for parallel code?

\paragraph{Limits of static analysis} Sequential consistency in
shared-memory parallel
programming~\cite{Krishnamurthy1995,Shasha1988,Sura2005}. Sequential
consistency and caches~\cite{Goodman}.

\paragraph{Parallelism in programming languages} Programming languages
have taken a variety of approaches in adopting parallelism. In C, a
threading model was retrofitted using the \textit{POSIX pthreads}
standard~\cite{Sura2005}. \citeauthor{Boehm2005} contend this approach
in~\cite{Boehm2005}, describing issues in which a compiler developed
independently of threading issues cannot be guaranteed to produce
correct code. These issues were circumvented in C++ by the
introduction of a concurrency model~\cite{Boehm2008}, based off the
success of the Java Memory Model~\cite{Bash2015a}. C++ and Java are
two of the most popular imperative programming languages, and in both
cases, .

In both C++ and Java,

Deterministic Parallel Java~\cite{Bocchino2009}.

Go

Functional: Clojure, Haskell, Scala


% * Compiler optimisations for parallel programs

% * Optimisations for skeletons
%   * “skeletons" - Intel TBB, std::algorithm, FastFlow, MapReduce
%   * Composition and nesting.
%   * Rewrite rules:

%     Patterns and Rewrite Rules for Systematic Code Generation From
%     High-Level Functional Patterns to High-Performance OpenCL Code

%       A

% \subsubsection{Autotuning for parallel programs}

%   * Autotuning for skeletons

\paragraph{Optimising MapReduce} Optimisations for multi-core
processors~\cite{Kaashoek2010}.


\paragraph{Autotuning Algorithmic Skeletons}
\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to predict the optimal execution device (i.e.\ CPU, GPU) for a
given program by predicting execution time and memory copy overhead
based on problem size. The autotuner only supports vector operations,
and there is limited cross-architecture
evaluation. In~\cite{Dastgeer2015a}, the authors extend SkePU to
improve the data consistency and transfer overhead of container types,
reporting up to a $33.4\times$ speedup over the previous
implementation.

\citeauthor{Contreras2008} enumerated the optimisation space of Intel
Thread Building Blocks in~\cite{Contreras2008}, showing that runtime
knowledge of the available parallel hardware can have a significant
impact on program performance. \citeauthor{Collins2012} exploit this
in~\cite{Collins2012}, using Principle Components Analysis to reduce
the dimensionality of the parameter optimisation space and then search
across it. In~\cite{Collins2013} they extend this using static feature
extraction and nearest neighbour classification to further prune the
search space.


\subsection{Skeleton-aware debugging}

General purpose, platform-specific debuggers: Intel Debugger for
Linux~\cite{Blair-chappell}, Microsoft Windows Debugger.


\subsubsection{Debugging Parallel Frameworks and Libraries}

Allinea DDT~\cite{K2010}, a scalable parallel debugger.

TotalView~\cite{Cownie2000}, an OpenMP debugger.

Jumbune~\cite{ImpetusTechnologies}, an open source debugger for
Hadoop.

The parallel pipeline library FlumeJava~\cite{Chambers2010} provides
debugging support through regular tools using a sequential execution
mode,r and routing of exceptions and output from remote workers back to
a central host.


\subsubsection{Debugging GPGPU programs}

CUDA-GDB~\cite{NVIDIA2016}, Intel OpenCL Debugger for Linux
OS~\cite{IntelCorporation2016}.

Oclgrind~\cite{Price2015}.


\subsubsection{Debugging Algorithmic Skeletons}

SkIE~\cite{Bacci1999} is based on a coordination language, but
provides advanced features such as debugging tools, performance
analysis, visualization, and graphical user interface. Instead of
directly using the coordination language, programmers interact with a
graphical tool, where parallel modules based on skeletons can be
composed

SKiPPER~\cite{Cezeaux1999} supports sequential execution for
debugging.


\subsection{Skeleton-aware profiling}

Profiling sequential programs~\cite{Ball1994}.

\paragraph{Profiling parallel programs} Profiling parallel programs
using KOJAK~\cite{Mohr2003}.


\section{Methodology}

Use geometric mean for speedups~\cite{Fleming1986}. Statistical
rigour\cite{Georges2007}.


\subsection{Skeleton-aware compiler optimizations}

First steps:
%
\begin{enumerate}
\item Write a test case program
\item Compile to bytecode and check optimisation is not applied
\item Implement optimisation by hand to source code
\item Measure performance improvement
\item Implement compiler pass to apply optimisation to bytecode
\item Test on real world benchmarks
\end{enumerate}
%
Example: constant propagation across muscle functions in pipe:
%
\begin{enumerate}
\item Write a pipeline of 100 "+1” functions.
\item Compile to bytecode and check that pipeline isn’t collapsed.
\item Collapse pipeline to single stage.
\item Measure performance improvement.
\item Implement compiler pass to apply optimisation to bytecode.
\item Test on real world benchmarks.
\end{enumerate}
%
Next step: load balancing across pipelines - stream partitioning?

Related work: dataflow analysis across skeletons for scheduling, what
kind of optimisations are applied in TLB/actor frameworks, actors in
scala


\subsection{Skeleton-aware debugging}
debugging+profiling for checkpointing

TBB debugging

exoskeleton

SkePU related workx


\section{Conclusions}


\label{bibliography}
\printbibliography


\end{document}
