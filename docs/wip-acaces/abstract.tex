\documentclass[hidelinks]{acaces}


%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document and Layout %%
%%%%%%%%%%%%%%%%%%%%%%%%%

% Fix for multiple "No room for a new \dimen" errors.
%
% See: http://tex.stackexchange.com/questions/38607/no-room-for-a-new-dimen
%
\usepackage{etex}

% Fix for "'babel/polyglossia' detected but 'csquotes' missing"
% warning. NOTE: Include after inputenc.
%
\usepackage{csquotes}
\usepackage{booktabs}

% Required for full page-width tables.
\usepackage{tabularx}

\usepackage{wrapfig}

\usepackage{adjustbox}

% Define column types L, C, R with known text justification and fixed
% widths:
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% Make internal macro definitions accessible,
% e.g. \@title, \@date \@author.
\makeatletter

%%%%%%%%%%%%%%%%%%%%%
% Table of Contents %
%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\newcommand{\fix}[1]{\textcolor{red}{\em\footnotesize#1}}


%%%%%%%%%%%%%%%%
% Bibliography %
%%%%%%%%%%%%%%%%
\usepackage[%
    backend=biber,%bibtex,
    style=numeric-comp,
    % style=numeric-comp,  % numerical-compressed
    sorting=none,        % nty,nyt,nyvt,anyt,anyvt,ynt,ydnt,none
    sortcites=true,      % sort \cite{b a d c}: true,false
    block=none,          % space between blocks: none,space,par,nbpar,ragged
    indexing=false,      % indexing options: true,false,cite,bib
    citereset=none,      % don't reset cites
    isbn=false,          % print ISBN?
    url=true,            % print URL?
    doi=false,           % print DOI?
    natbib=true,         % natbib compatability
    maxbibnames=99       % no 'et-al' in the bibliography pls
  ]{biblatex}

\addbibresource{../../../library.bib}

% Reduce the font size of the bibliography:
\renewcommand{\bibfont}{\normalfont\scriptsize}

\usepackage{setspace}
\onehalfspacing


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Figures, footnotes and listings %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Pre-requisites for rendering upquotes in listings package.
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}

% Pseudo-code listings.
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\Break}{\State \textbf{break} }
\algblockdefx[Loop]{Loop}{EndLoop}[1][]{\textbf{Loop} #1}{\textbf{End
    Loop}}

\algrenewcommand\ALG@beginalgorithmic{\footnotesize}


%%%%%%%%%%%%%%%%%%%%%%%%
%% Graphics and maths %%
%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}

% Vector notation, e.g. \vv{x}:
%
\usepackage{esvect}

% Additional amsmath symbols, see:
%
% http://texblog.org/2007/08/27/number-sets-prime-natural-integer-rational-real-and-complex-in-latex/
%
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{mathtools}

% Provide bold font face in maths.
\usepackage{bm}

\usepackage{subcaption}
\expandafter\def\csname ver@subfig.sty\endcsname{}

% Define an 'myalignat' command which behave as 'alignat' without the
% vertical top and bottom padding. See:
%     http://www.latex-community.org/forum/viewtopic.php?f=5&t=1890
\newenvironment{myalignat}[1]{%
  \setlength{\abovedisplayskip}{-.7\baselineskip}%
  \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
  \start@align\z@\st@rredtrue#1
}%
{\endalign}

% Define additional operators:
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator*{\gain}{Gain}

% Skeleton operators.
\DeclareMathOperator*{\map}{Map}
\DeclareMathOperator*{\reduce}{Reduce}
\DeclareMathOperator*{\scan}{Scan}
\DeclareMathOperator*{\stencil}{Stencil}
\DeclareMathOperator*{\zip}{Zip}
\DeclareMathOperator*{\allpairs}{All\,Pairs}

% Define a command to allow word breaking.
\newcommand*\wrapletters[1]{\wr@pletters#1\@nil}
\def\wr@pletters#1#2\@nil{#1\allowbreak\if&#2&\else\wr@pletters#2\@nil\fi}

% Define a command to create centred page titles.
\newcommand{\centredtitle}[1]{
  \begin{center}
    \large
    \vspace{0.9cm}
    \textbf{#1}
  \end{center}}

% Provide generic commands \degree, \celsius, \perthousand, \micro
% and \ohm which work both in text and maths mode.
\usepackage{gensymb}

\begin{document}

\title{Autotuning OpenCL Workgroup Sizes}

\author{
Chris~Cummins\addressnum{1},
Pavlos~Petoumenos\addressnum{1},
Michel~Steuwer\addressnum{1},
Hugh~Leather\addressnum{1}
}

\address{1}{
University of Edinburgh,
UK
}

\extra{1}{% Contacts
  E-mail: c.cummins@ed.ac.uk, ppetoume@inf.ed.ac.uk,
  michel.steuwer@ed.ac.uk, hleather@inf.ed.ac.uk%
}%
\extra{2}{% Acknowledgements
  This work was supported by the UK EPSRC under grants EP/L01503X/1
  (CDT in Pervasive Parallelism), EP/H044752/1 (ALEA), and
  EP/M015793/1 (DIVIDEND).%
}

\pagestyle{empty}


\begin{abstract}
  Selecting appropriate workgroup sizes for OpenCL is critical for
  program performance, and requires knowledge of the underlying
  hardware, the data being operated on, and the kernel
  implementation. Ensuring performance portability of workgroup sizes
  is challenging, since simple heuristics and statically chosen values
  fail to exploit the available performance. To address this, we
  propose the use of machine learning-enabled autotuning to
  automatically predict workgroup sizes for stencil patterns on CPUs
  and multi-GPUs. Using the Algorithmic Skeleton library SkelCL, we
  show that static tuning of workgroup size achieves only $26\%$ of
  the optimal performance across 429 combinations of architecture,
  kernel, and dataset. Using machine learning and synthetically
  generated stencil programs, we achieve $92\%$ of this maximum,
  demonstrating a median $3.79\times$ speedup over the best possible
  fixed workgroup size.
\end{abstract}

\vspace{-.5em}
\keywords{%
  OpenCL; %
  Autotuning; %
  Stencil codes; %
  GPUs; %
  Machine learning; %
  Synthetic benchmarking%
}

\vspace{-2em}
\section{Introduction}

Efficient, tuned implementations of stencil codes are highly sought
after, with a variety of applications from fluid dynamics to quantum
mechanics. OpenCL enables the application of a range of heterogeneous
devices to parallelise stencils; however, achieving portable
performance is a hard task --- OpenCL kernels are sensitive to
properties of the underlying hardware, to the implementation, and even
to the dataset that is operated upon. This forces developers to
laboriously hand tune performance on a case-by-case basis, since
simple heuristics fail to exploit the available performance. In this
work we demonstrate how machine learning-enabled autotuning can be
applied to Algorithmic Skeletons to achieve near optimal performance
of stencils operations, without requiring hand tuning.

\section{The SkelCL Stencil Pattern}

\begin{wrapfigure}{r}{0.38\textwidth}
% \vspace{-3em}
\includegraphics[width=.38\textwidth]{stencil}
\caption[Stencil border region]{%
  The components of a stencil: an input matrix is decomposed into
  workgroups, and elements within workgroups are mapped to work-items.
  The values used by workgroups are mapped to local memory.%
  \vspace{-.75em} }
\label{fig:stencil-shape}
\end{wrapfigure}

SkelCL is an Algorithmic Skeleton library which provides OpenCL
implementations of data parallel patterns for heterogeneous
parallelism using CPUs and
multi-GPUs~\cite{Steuwer2011}. Figure~\ref{fig:stencil-shape} shows
the components of the SkelCL stencil pattern, which applies a
user-provided \emph{customising function} to each element of a 2D
matrix. The value of each element is updated based on its current
value and the value of one or more neighbouring elements, called the
\emph{border region}. When a SkelCL stencil pattern is executed, each
of the matrix elements are mapped to OpenCL work-items; and this
collection of work-items is divided into \emph{workgroups} for
execution on the target hardware. A \emph{tile} of elements the size
of the workgroup and the perimeter border region is allocated in local
memory for use by work-items. Changing the workgroup size affects both
the number of workgroups which can be active simultaneously, and the
amount of local memory required for each workgroup. While the user
defines the size, type, and border region of the matrix being operated
upon, it is the responsibility of the SkelCL implementation to select
an appropriate workgroup size to use.

\section{OpenCL Workgroup Size Optimization Space}

Preliminary experiments demonstrated that selection of an appropriate
workgroup size $w$ from the space of all possible workgroup sizes
$w \in W$ depends on properties of the kernel, hardware, and dataset,
illustrated in Figures~\ref{fig:motivation-arch}
and~\ref{fig:motivation-prog}. Additionally, the optimisation space is
subject to hard constraints which cannot conveniently be statically
determined. Each OpenCL device and kernel imposes a maximum workgroup
size determined by the hardware resources and resource requirements of
a program. Preliminary experiments found that not all points in the
workgroup size space provide correct programs, even if they satisfy
the kernel and architectural constraints. We call these points in the
space \emph{refused parameters}. For a given \emph{scenario} $s$ (a
combination of program, device, and dataset), we define a
\textit{legal} workgroup size as one which satisfies the architectural
and kernel constraints $w < W_{max}(s)$ and is not refused
$w \notin W_{refused}(s)$. The subset of legal workgroup sizes for a
given scenario $W_{legal}(s) \subset W$ is then:
%
\begin{equation}
  W_{legal}(s) \subset W = \left\{w | w \in W, w < W_{\max}(s) \right\} - W_{refused}(s)
\end{equation}


\begin{figure}
\centering
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{motivation_1}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-1}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{motivation_2}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-2}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same
      stencil program on two different devices:
      (\subref{fig:motivation-1}) Intel CPU,
      (\subref{fig:motivation-2}) NVIDIA GPU.%
    }
    \label{fig:motivation-arch}
  \end{minipage}%
}%
\hspace{2.5mm}
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{motivation_3}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-3}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{motivation_4}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-4}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same device
      executing two different stencil programs.%
    }
    \label{fig:motivation-prog}
  \end{minipage}%
}
\end{figure}


\section{Machine Learning Method}

The goal of machine learning enabled autotuning is to predict, for a
given scenario, the workgroup size that will provide the shortest
runtime.
%
% The goal of machine learning enabled autotuning is to predict, for a
% given scenario $s$, the workgroup size $f(s)$ that will provide the
% shortest runtime $t(s, w)$:
% %
% \begin{equation}
%   f(s) = \underset{w \in W_{legal(s)}}{\argmin} t(s,w)
% \end{equation}
% %
The OpenCL workgroup size optimization space is large, complex, and
non-linear. Successfully applying machine learning to this problem
requires plentiful training data, the careful selection of explanatory
variables, and appropriate machine learning methods:
\begin{itemize}
\item \textbf{Explanatory variables} 102 variables are extracted to
  capture information about the device, program, and dataset of a
  scenario. Device variables information about the host and execution
  device (e.g.\ number of compute units, local memory size, global
  caches). Program variables include static instruction densities, the
  total number of basic blocks, and the total instruction
  count. Dataset variables include the data types (input and output),
  and dimensions of the input matrix and stencil region.
\item \textbf{Training Data} A parameterised template substitution
  engine is used to generate synthetic stencil programs. To generate
  training data, these synthetically generated programs are executed
  on a range of different execution devices, using different datasets,
  and exhaustively enumerating the legal workgroup sizes. The
  workgroup size which gives the best performance for each particular
  device, program, and dataset is combined with the explanatory
  variables to create training data.
\item \textbf{Classification} Our approach uses a J48 Decision Tree
  classifier~\cite{Han2011} to correlate patterns between explanatory
  variables and the workgroup sizes which provide optimal
  performance. The classifier is chosen due to its low runtime cost
  and ability to efficiently handle large dimensionality training
  data. The classifier predicts optimal workgroup sizes based on the
  extracted explanatory variables. If the predicted workgroup size is
  not legal for that scenario, the \emph{nearest neighbor} workgroup
  size is iteratively selected based on Euclidian distance to the
  predicted value.
\end{itemize}

\section{Evaluation}

An exhaustive enumeration of the workgroup size optimisation space for
429 combinations of architecture, program, and dataset was performed.
Seven architectures were used (3 NVIDIA GPUs, 3 Intel CPUs, and 1 AMD
GPU). Synthetically generated stencil programs and six reference
kernels taken from image processing, cellular automata, and partial
differential equation solving were used. Dataset sizes used were
$512\times512$, $1024\times1024$, $2048\times2048$, and
$4096\times4096$. The workgroup size space was enumerated for each
combination of $w_r$ and $w_c$ values in multiples of 2, up to the
maximum workgroup size. A minimum of 30 runtimes were recorded for
each scenario and the average execution time taken.

\section{Conclusions}

We present a novel methodology for autotuning the workgroup size of
stencil patterns using the established open source library
SkelCL. This technique achieves 94\% of the maximum performance, while
providing robust fallbacks in the presence of unexpected behaviour in
OpenCL driver implementations. In future work we will use machine
learning to perform a directed search through the program space by
guiding the generation of synthetic benchmark programs.


\label{bibliography}

\begingroup
\setstretch{0.8}
\setlength\bibitemsep{1pt}
\printbibliography
\endgroup

\end{document}
