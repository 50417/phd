{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import typing\n",
    "import collections\n",
    "\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import humanize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from experimental.deeplearning.polyglot import get_instances\n",
    "from deeplearning.clgen import clgen\n",
    "from deeplearning.clgen import errors\n",
    "from deeplearning.clgen.corpuses import corpuses\n",
    "from deeplearning.clgen.proto import corpus_pb2\n",
    "from deeplearning.clgen.proto import clgen_pb2\n",
    "from deeplearning.clgen.proto import model_pb2\n",
    "from deeplearning.clgen.proto import sampler_pb2\n",
    "from lib.labm8 import bazelutil\n",
    "from lib.labm8 import pbutil\n",
    "from lib.labm8 import viz\n",
    "\n",
    "FLAGS = flags.FLAGS(['argv[0]', '--logtostderr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 instances\n"
     ]
    }
   ],
   "source": [
    "instances = get_instances.GetInstances()\n",
    "print(\"Loaded {} instances\".format(len(instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 output corpuses\n"
     ]
    }
   ],
   "source": [
    "def GetOutputCorpus(instance: clgen.Instance) -> corpuses.Corpus:\n",
    "    with instance.Session():\n",
    "        out_dir = instance.model.SamplerCache(instance.sampler)\n",
    "        if not out_dir.is_dir():\n",
    "            return None\n",
    "        output_corpus_config = corpus_pb2.Corpus()\n",
    "        output_corpus_config.CopyFrom(instance.model.corpus.config)\n",
    "        output_corpus_config.local_directory = str(out_dir) + '.contentfiles'\n",
    "        if not pathlib.Path(output_corpus_config.local_directory).is_dir():\n",
    "            return None\n",
    "        return corpuses.Corpus(output_corpus_config)\n",
    "\n",
    "output_corpuses = [GetOutputCorpus(i) for i in instances]\n",
    "print(\"Loaded {} output corpuses\".format(len([x for x in output_corpuses if x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (138 of 11927) |                    | Elapsed Time: 0:01:23 ETA:   2:07:49"
     ]
    }
   ],
   "source": [
    "def InstanceStats(instance: clgen.Instance, output_corpus: corpuses.Corpus) -> typing.Dict[str, typing.Any]:\n",
    "    stats = collections.OrderedDict()\n",
    "    stats['Language'] = {\n",
    "        'opencl': 'OpenCL',\n",
    "        'java': 'Java',\n",
    "    }[pathlib.Path(instance.model.corpus.config.local_directory).stem]\n",
    "    with instance.Session():\n",
    "        instance.model.corpus.Create()\n",
    "    stats['Encoding'] = 'Character' if 'Ascii' in str(instance.model.corpus.atomizer) else 'Token'\n",
    "    stats['Vocab size'] = instance.model.corpus.atomizer.vocab_size\n",
    "    stats['Corpus size'] = '{:.1f}M'.format(instance.model.corpus.encoded.token_count / 1e6)\n",
    "    # stats['Embedding'] = instance.model.config.architecture.embedding_size\n",
    "    stats['Model size'] = f'{instance.model.config.architecture.neurons_per_layer}x{instance.model.config.architecture.num_layers}'\n",
    "    # stats['Dropout'] = instance.model.config.architecture.post_layer_dropout_micros / 1e6\n",
    "    if instance.model.config.training.HasField('adam_optimizer'):\n",
    "        stats['Optimizer'] = 'Adam'\n",
    "        stats['Learning rate'] = instance.model.config.training.adam_optimizer.initial_learning_rate_micros / 1e6\n",
    "        stats['Decay'] = instance.model.config.training.adam_optimizer.learning_rate_decay_per_epoch_micros / 1e6\n",
    "    else:\n",
    "        stats['Optimizer'] = 'RMSProp'\n",
    "        stats['Learning rate'] = instance.model.config.training.rmsprop_optimizer.initial_learning_rate_micros / 1e6\n",
    "        stats['Decay'] = instance.model.config.training.rmsprop_optimizer.learning_rate_decay_per_epoch_micros / 1e6\n",
    "    telemetry = instance.model.TrainingTelemetry()\n",
    "    if telemetry:\n",
    "        stats['Epochs'] = len(telemetry)\n",
    "        stats['Final Loss'] = '{:.3f}'.format(telemetry[-1].loss)\n",
    "        stats['Training time'] = humanize.naturaldelta(\n",
    "            datetime.timedelta(seconds=sum(t.epoch_wall_time_ms for t in telemetry) / 1e3))\n",
    "        stats['Time / epoch'] = humanize.naturaldelta(\n",
    "            datetime.timedelta(seconds=np.array([t.epoch_wall_time_ms for t in telemetry]).mean() / 1e3))\n",
    "\n",
    "    if output_corpus:\n",
    "#         with instance.Session():\n",
    "#             try:\n",
    "#                 output_corpus.Create()\n",
    "#             except errors.EmptyCorpusException:\n",
    "#                 pass\n",
    "        samples_dir = instance.model.SamplerCache(instance.sampler)\n",
    "        sample_times = np.array([\n",
    "            pbutil.FromFile(samples_dir / f, model_pb2.Sample, uninitialized_okay=True).wall_time_ms for f in samples_dir.iterdir()\n",
    "        ], dtype=np.int32)\n",
    "        # TODO(cec): Use the number of extracted kernels, not the number of samples themselves.\n",
    "        # Sample times is in milliseconds, and we want time per thousand, so they cancel out.\n",
    "        # Average sample time in seconds.\n",
    "        sample_time_seconds = sample_times.mean() / 1000\n",
    "        stats['Sample temperature'] = humanize.intcomma(instance.sampler.temperature)\n",
    "        stats['Output samples'] = humanize.intcomma(output_corpus.preprocessed.input_size)\n",
    "        stats['Output vocab size'] = humanize.intcomma(output_corpus.vocab_size)\n",
    "        stats['Time / sample (ms)'] = int(round(sample_times.mean()))\n",
    "        sample_throughput = (24 * 3600) / sample_time_seconds\n",
    "        stats['Samples / day'] = '{:.1f}k'.format(sample_throughput / 1000)\n",
    "        # stats['Time / 1k samples'] = humanize.naturaldelta(\n",
    "        #     datetime.timedelta(seconds=samples_time_seconds * 1000))\n",
    "        if output_corpus.preprocessed.size:\n",
    "            efficiency = (output_corpus.preprocessed.size / \n",
    "                          (output_corpus.preprocessed.input_size or 1))\n",
    "            good_sample_throughput = efficiency * sample_throughput\n",
    "            stats['Efficiency'] = '{:.2%}'.format(efficiency)\n",
    "            stats['Throughput / day'] = '{:.1f}k'.format(good_sample_throughput / 1000)\n",
    "    return stats\n",
    "\n",
    "stats = pd.DataFrame([InstanceStats(i, o) for i, o in zip(instances, output_corpuses)]).fillna('-')\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (_, row), instance in zip(stats.iterrows(), instances):\n",
    "    plt.plot([t.epoch_num for t in instance.model.TrainingTelemetry()], \n",
    "             [t.loss for t in instance.model.TrainingTelemetry()], \n",
    "             label=f\"{row['Language']}-{row['Model size']}\")\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.title('Training Losses')\n",
    "\n",
    "# X axis.\n",
    "# plt.xlim((0, 50 - 1))\n",
    "# ax.set_xticklabels([i + 1 for i in ax.get_xticks()])\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Y axis.\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "viz.finalise(size=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
