{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Network Tuning\n",
    "\n",
    "Definitions:\n",
    "* Network parameters: $\\Theta$\n",
    "* CLgen model: $G(\\Theta)$\n",
    "* Discriminator model: $D(G(\\Theta))$\n",
    "\n",
    "From candidate params $\\Theta = \\{\\Theta_1, \\Theta_2, \\ldots, \\Theta_n\\}$,\n",
    "find the params $\\Theta_{i}$ which minimize accuracy of the discriminator.\n",
    "\n",
    "Discriminator functions:\n",
    "* Human-or-robot? Distinguish between programs from GitHub and synthesized codes.\n",
    "* Is it *useful*? Determine closest distance to benchmark features (would have to be a different set of benchmarks).\n",
    "\n",
    "\n",
    "1. $\\epsilon = 0.05$\n",
    "1. $\\Theta = newParams()$\n",
    "1. while $abs(D(G(\\Theta)) - 0.5) > \\epsilon$\n",
    "1. `    ` $\\Theta = newParams()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus of 3950 files"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clgen.corpus import Corpus\n",
    "\n",
    "corpus = Corpus.from_json({\n",
    "    \"path\": \"~/data/github\",\n",
    "    \"vocabulary\": \"greedy\"\n",
    "})\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def corpus_iter(kernels_db):\n",
    "    db = sqlite3.connect(kernels_db)\n",
    "    c = db.cursor()\n",
    "    c.execute(\"SELECT contents FROM PreprocessedFiles WHERE status=0\")\n",
    "    srcs = [row[0] for row in c.fetchall()]\n",
    "    c.close()\n",
    "    db.close()\n",
    "    return srcs\n",
    "\n",
    "def encode_srcs(srcs, atomizer):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from labm8 import fs\n",
    "\n",
    "github_srcs = corpus_iter(fs.path(corpus.contentcache.path, \"kernels.db\"))\n",
    "\n",
    "inpath = fs.path(\"data\", \"encoded-\" + corpus.hash + \".pkl\")\n",
    "if fs.exists(inpath):\n",
    "    with open(inpath, \"rb\") as infile:\n",
    "        github_seqs = pickle.load(infile)\n",
    "else:\n",
    "    github_seqs = [corpus.atomizer.atomize(x) for x in github_srcs]\n",
    "    with open(inpath, \"wb\") as outfile:\n",
    "        pickle.dump(github_seqs, outfile)\n",
    "        print(\"cached\", outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "lens = np.array([len(x) for x in github_seqs])\n",
    "_data = [{\"Percentile\": x, \"Sequence Length\": int(round(np.percentile(lens, x)))} for x in range(0, 101, 10)]\n",
    "data = pd.DataFrame(_data, columns=[\"Percentile\", \"Sequence Length\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogy(data[\"Percentile\"], data[\"Sequence Length\"])\n",
    "plt.title(\"GitHub Corpus\")\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Sequence Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "seq_length = 1024\n",
    "p1 = scipy.stats.percentileofscore(lens, seq_length)\n",
    "p2 = 100 - p1\n",
    "print(\"\"\"\\\n",
    "A sequence length of {seq_length} is the {p1:.1f}% percentile of the GitHub corpus.\n",
    "{p2:.1f}% of sequences will be truncated.\"\"\".format(**vars()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "vocab_size = corpus.vocab_size + 1  # pad value\n",
    "\n",
    "# network param\n",
    "seq_length = 1024\n",
    "embedding_vector_length = 64\n",
    "\n",
    "# training param\n",
    "nb_epoch = 50\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Embedding, merge, LSTM, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    \"\"\" instantiate model \"\"\"\n",
    "    data_in = Input(shape=(2,), name=\"data_in\")\n",
    "\n",
    "    code_in = Input(shape=(seq_length,), dtype=\"int32\", name=\"code_in\")\n",
    "    x = Embedding(output_dim=embedding_vector_length, input_dim=vocab_size, input_length=seq_length)(code_in)\n",
    "    x = LSTM(64, consume_less=\"mem\", return_sequences=True)(x)\n",
    "    x = LSTM(64, consume_less=\"mem\")(x)\n",
    "    out = Dense(2, activation=\"sigmoid\", name=\"out\")(x)\n",
    "\n",
    "    model = Model(input=code_in, output=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# TODO:\n",
    "# # train model\n",
    "# model.fit(X_train, y_train,\n",
    "#           nb_epoch=nb_epoch, batch_size=batch_size,\n",
    "#           verbose=1, shuffle=True)\n",
    "\n",
    "# # predict with model\n",
    "# predictions = np.array(model.predict(y_train, batch_size=batch_size, verbose=0))\n",
    "# predictions = [np.argmax(x) for x in predictions[0]]\n",
    "\n",
    "# model.save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLgen",
   "language": "python",
   "name": "clgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
