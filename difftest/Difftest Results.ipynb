{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difftest Results\n",
    "\n",
    "Connect to results database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import db\n",
    "from db import *\n",
    "%run util.py\n",
    "\n",
    "hostname = \"cc1\"\n",
    "db.init(hostname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "session = db.make_session()\n",
    "\n",
    "TABLE_NAMES = [\"CLSmith\", \"CLSmith w. cldrive\", \"GitHub\", \"CLgen\", \"CLgen w. cl_launcher\", \"CLgen w. co\"]\n",
    "TABLES = [CLSmithResult, cldriveCLSmithResult, GitHubResult, CLgenResult, cl_launcherCLgenResult, coCLgenResult]\n",
    "\n",
    "data = [\n",
    "    (\"#. Programs\", [session.query(t.program_id).group_by(t.program_id).count() for t in TABLES]),\n",
    "    (\"#. Testbeds\", [session.query(t.testbed_id).group_by(t.testbed_id).count() for t in TABLES]),\n",
    "    (\"#. Params\", [session.query(t.params_id).group_by(t.params_id).count() for t in TABLES]),\n",
    "    (\"#. Results\", [session.query(t).count() for t in TABLES]),\n",
    "]\n",
    "i, d = zip(*data)\n",
    "overview = pd.DataFrame(list(d), index=i, columns=TABLE_NAMES)\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestBeds\n",
    "\n",
    "A testbed is a combination of host platform and OpenCL device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sql\n",
    "\n",
    "q = session.query(Testbed).order_by(sql.func.field(Testbed.devtype, 'GPU', 'CPU', 'Emulator'))\n",
    "\n",
    "data = []\n",
    "for testbed in q:\n",
    "    data.append(\n",
    "        (testbed.id, [\n",
    "            host_str(testbed.host), device_str(testbed.device),\n",
    "            DRIVERS.get(testbed.driver, testbed.driver), testbed.opencl, testbed.devtype] +\n",
    "         [session.query(t.testbed).filter(t.testbed == testbed).count() for t in TABLES]))\n",
    "i, d = zip(*data)\n",
    "testbeds = pd.DataFrame(list(d), index=i, columns=[\"Operating System\", \"Device\", \"Driver\", \"OpenCL\", \"Device type\"] + [f\"#. {t}\" for t in TABLE_NAMES])\n",
    "\n",
    "if len(CONFIGURATIONS) != session.query(Testbed).count():\n",
    "    import sys\n",
    "    print(\"warning: missing testbed(s)\", file=sys.stderr)\n",
    "testbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push LaTex to Overleaf\n",
    "!cd ~/docs/paper-project_b/ && git pull --rebase\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_total_submitted(testbed: Testbed):\n",
    "    submitable_results = [coCLgenResult, cl_launcherCLgenResult]\n",
    "    \n",
    "    def get_submitted(table):\n",
    "        return session.query(table).filter(table.testbed_id == testbed.id, table.submitted).count()\n",
    "\n",
    "    def get_generated(table):\n",
    "        return session.query(table).filter(table.testbed_id == testbed.id, sql.or_(table.submitted, table.dupe)).count()\n",
    "    \n",
    "    return (\n",
    "        sum(get_generated(table) for table in submitable_results), \n",
    "        sum(get_submitted(table) for table in submitable_results)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_testbed_info(config_id, testbed_id):\n",
    "    testbed = session.query(Testbed).filter(Testbed.id == testbed_id).first()\n",
    "    d = OrderedDict()\n",
    "    d[\"#.\"] = config_id\n",
    "    d[\"Device\"] = testbed.device\n",
    "    d[\"Platform\"] = platform_str(testbed.platform)\n",
    "    d[\"Driver\"] = driver_str(testbed.driver)\n",
    "    d[\"OpenCL\"] = testbed.opencl\n",
    "    d[\"Operating system\"] = host_str(testbed.host)\n",
    "    d[\"Device type\"] = devtype_str(testbed.devtype)\n",
    "    d[\"B.R. Generated\"], d[\"B.R. Submitted\"] = get_total_submitted(testbed)\n",
    "    return d\n",
    "\n",
    "table = pd.DataFrame([get_testbed_info(*x) for x in CONFIGURATIONS])\n",
    "with open(os.path.expanduser(\"~/docs/paper-project_b/build/tab/platforms.tex\"), \"w\") as outfile:\n",
    "    table.to_latex(buf=outfile, index=None)\n",
    "!cd ~/docs/paper-project_b/build && git add . && git commit -m \"auto: build/tab/platforms.tex\" && git push\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from labm8 import fs\n",
    "\n",
    "null_columns = \" & \".join([\"-\"] * 6)\n",
    "\n",
    "# Measured average CLgen sample time\n",
    "clgen_generation_time = .5\n",
    "\n",
    "def get_counts(results_table, params_table, testbed, no_opt: bool, time_limit: int=None):\n",
    "    \"\"\"\n",
    "    Return results for the specified time limit\n",
    "    \"\"\"\n",
    "    # If no time limit is provided, return all results.\n",
    "    if time_limit is None:\n",
    "        return {\n",
    "            \"w\": q.filter(results_table.classification == \"w\").count(),\n",
    "            \"bf\": q.filter(results_table.classification == \"bf\").count(),\n",
    "            \"c\": q.filter(results_table.classification == \"c\").count(),\n",
    "            \"to\": q.filter(results_table.classification == \"to\").count(),\n",
    "            \"pass\": q.filter(results_table.classification == \"pass\").count(),\n",
    "            \"fail\": q.filter(results_table.classification == \"fail\").count(),\n",
    "            \"total\": q.count()\n",
    "        }\n",
    "    \n",
    "    # Otherwise, simulate results for a fixed period of time\n",
    "    t = 0  # elapsed time\n",
    "    counts = {None: 0, \"w\": 0, \"bf\": 0, \"c\": 0, \"to\": 0, \"pass\": 0, \"fail\": 0}\n",
    "\n",
    "    param_ids = session.query(params_table.id).filter(params_table.optimizations == no_opt)\n",
    "    q = session.query(results_table).filter(\n",
    "        results_table.testbed_id == testbed.id, \n",
    "        results_table.params_id.in_(param_ids))\n",
    "    \n",
    "    for result in q.order_by(results_table.id):\n",
    "        if hasattr(result.program, 'runtime'):\n",
    "            generation_time = result.program.runtime\n",
    "        else:\n",
    "            generation_time = clgen_generation_time\n",
    "        exec_time = generation_time + result.runtime\n",
    "        if t + exec_time > time_limit:\n",
    "            hours = t / 60 / 60\n",
    "            break\n",
    "        t += exec_time\n",
    "        counts[result.classification] = counts.get(result.classification, 0) + 1\n",
    "    else:\n",
    "        counts['total'] = '*'  # asterisk denotes insufficient results to reach time limit\n",
    "\n",
    "    # remove unclassified results from table\n",
    "    counts.pop(None)\n",
    "        \n",
    "    # sum up all results\n",
    "    total = counts.pop(\"total\", \"\")\n",
    "    counts['total'] = str(sum(counts.values())) + total\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "def get_columns(results_table, params_table, testbed: Testbed, no_opt: bool, time_limit: int=172800):\n",
    "    print(f\"{results_table.__tablename__} {testbed.device} {no_opt} \", end=\"\")\n",
    "    c = get_counts(results_table, params_table, testbed, no_opt, time_limit)\n",
    "    print(c)\n",
    "    sys.stdout.flush()\n",
    "    if c['total'].endswith('*'):\n",
    "        print(f\"insufficient {results_table.__tablename__} for {testbed.device} {no_opt}\", file=sys.stderr)\n",
    "        sys.stderr.flush()\n",
    "    return f\"{c['w']} & {c['bf']} & {c['c']} & {c['to']} & {c['pass']} & {c['total']}\"\n",
    "\n",
    "\n",
    "def get_row(config_id, testbed_id):\n",
    "    \"\"\" get mega-table row \"\"\"\n",
    "    testbed = session.query(Testbed).filter(Testbed.id == testbed_id).first()\n",
    "    platform_name = platform_str(testbed.platform)\n",
    "    device_name = device_str(testbed.device)\n",
    "    driver_name = driver_str(testbed.driver)\n",
    "    \n",
    "    time_limit = 172800\n",
    "    \n",
    "    noopt = False\n",
    "    clsmith_columns = get_columns(CLSmithResult, cl_launcherParams, testbed, False)\n",
    "    # clsmith_columns = null_columns\n",
    "    clgen_columns = get_columns(CLgenResult, cldriveParams, testbed, noopt, time_limit)\n",
    "    \n",
    "    noopt = True\n",
    "    clsmith_noopt_columns = get_columns(CLSmithResult, cl_launcherParams, testbed, True)\n",
    "    # clsmith_noopt_columns = null_columns\n",
    "    clgen_noopt_columns = get_columns(CLgenResult, cldriveParams, testbed, noopt, time_limit)\n",
    "    \n",
    "    return f\"\"\"\\\\multirow{{ 2}}{{*}}{{{config_id}}} & \\\\multirow{{ 2}}{{*}}{{{platform_name}}} & \\\n",
    "\\\\multirow{{ 2}}{{*}}{{{device_name}}} & \\\n",
    "$-$ & {clsmith_columns}       & {clgen_columns} \\\\\\\\\n",
    "& & & \\\n",
    "$+$ & {clsmith_noopt_columns} & {clgen_noopt_columns} \\\\\\\\\"\"\"\n",
    "\n",
    "rows = \"\\n\\\\hline\\n\".join(get_row(*x) for x in CONFIGURATIONS)\n",
    "\n",
    "latex = f\"\"\"\\\n",
    "\\\\begin{{tabular}}{{llll | rrrrrr | rrrrrr }}\n",
    "  \\\\toprule\n",
    "  & & & & \\\\multicolumn{{6}}{{c|}}{{\\\\textbf{{CLSmith}}}} & \\\\multicolumn{{6}}{{c}}{{\\\\textbf{{CLgen}}}} \\\\\\\\\n",
    "  \\\\textbf{{\\\\#.}} & \\\\textbf{{Platform}} & \\\\textbf{{Device}} & $\\\\pm$ & \n",
    "  \\\\textbf{{w}} & \\\\textbf{{bf}} & \\\\textbf{{c}} & \\\\textbf{{to}} & \\\\cmark & \\\\textbf{{total}} & \n",
    "  \\\\textbf{{w}} & \\\\textbf{{bf}} & \\\\textbf{{c}} & \\\\textbf{{to}} & \\\\cmark & \\\\textbf{{total}} \\\\\\\\\n",
    "  \\\\midrule\n",
    "  {rows}\n",
    "  \\\\bottomrule\n",
    "\\\\end{{tabular}}\n",
    "\"\"\"\n",
    "\n",
    "# push LaTex to Overleaf\n",
    "!cd ~/docs/paper-project_b/ && git pull --rebase\n",
    "\n",
    "with open(os.path.expanduser(\"~/docs/paper-project_b/build/tab/megatable.tex\"), \"w\") as outfile:\n",
    "    print(latex, file=outfile)\n",
    "\n",
    "!cd ~/docs/paper-project_b/build && git add . && git commit -m \"auto: build/tab/megatable.tex\" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Parameters\n",
    "\n",
    "### cl_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CL_LAUNCHER_TABLE_NAMES = [\"CLSmith\", \"CLgen w. cl_launcher\"]\n",
    "CL_LAUNCHER_TABLES = [CLSmithResult, cl_launcherCLgenResult]\n",
    "\n",
    "q = session.query(cl_launcherParams).order_by(\n",
    "        cl_launcherParams.gsize_x, cl_launcherParams.gsize_y, cl_launcherParams.gsize_z,\n",
    "        cl_launcherParams.lsize_x, cl_launcherParams.lsize_y, cl_launcherParams.lsize_z,\n",
    "        cl_launcherParams.optimizations)\n",
    "\n",
    "data = []\n",
    "for param in q:\n",
    "    nresult_param = session.query(CLSmithResult).filter(CLSmithResult.params == param).count()\n",
    "    data.append((\n",
    "        param.id, [param.gsize, param.lsize, param.optimizations_on_off ] + [\n",
    "            session.query(t).filter(t.params == param).count()\n",
    "            for t in CL_LAUNCHER_TABLES\n",
    "        ]))\n",
    "i, d = zip(*data)\n",
    "\n",
    "cl_launcher_params = pd.DataFrame(list(d), index=i, columns=[\n",
    "    \"Global size\", \"Local size\", \"Optimizations\"] + [\n",
    "        f\"#. {t}\" for t in CL_LAUNCHER_TABLE_NAMES])\n",
    "cl_launcher_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cldrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLDRIVE_TABLE_NAMES = [\"CLSmith w. cldrive\", \"GitHub\", \"CLgen\"]\n",
    "CLDRIVE_TABLES = [cldriveCLSmithResult, GitHubResult, CLgenResult]\n",
    "\n",
    "q = session.query(cldriveParams).order_by(\n",
    "        cldriveParams.size,\n",
    "        cldriveParams.gsize_x, cldriveParams.gsize_y, cldriveParams.gsize_z,\n",
    "        cldriveParams.lsize_x, cldriveParams.lsize_y, cldriveParams.lsize_z,\n",
    "        cldriveParams.generator, cldriveParams.scalar_val, cldriveParams.optimizations)\n",
    "\n",
    "# push LaTex to Overleaf\n",
    "!cd ~/docs/paper-project_b/ && git pull --rebase\n",
    "data = []\n",
    "for param in q:\n",
    "    data.append([param.size, param.gsize, param.lsize, param.optimizations_on_off])\n",
    "table = pd.DataFrame(data, index=range(1, len(data)+1), columns=[\n",
    "    \"Dataset Size\", \"Global size\", \"Workgroup size\", \"Optimizations\"])\n",
    "with open(os.path.expanduser(\"~/docs/paper-project_b/build/tab/cldrive-params.tex\"), \"w\") as outfile:\n",
    "    table.to_latex(buf=outfile)\n",
    "!cd ~/docs/paper-project_b/build && git add . && git commit -m \"auto: build/tab/cldrive-params.tex\" && git push\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtimes\n",
    "\n",
    "Excluding runs which terminated in non-zero status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "runtimes = [np.array(session.query(table.runtime).filter(table.status == 0).all()) for table in TABLES]\n",
    "data = [\n",
    "    (\"Min\", [r.min() for r in runtimes]),\n",
    "    (\"Median\", [np.median(r) for r in runtimes]),\n",
    "    (\"Mean\", [r.mean() for r in runtimes]),\n",
    "    (\"Max\", [r.max() for r in runtimes])\n",
    "]\n",
    "i, d = zip(*data)\n",
    "runtimes = pd.DataFrame(list(d), index=i, columns=TABLE_NAMES)\n",
    "runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outcomes & Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas tables of outcomes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes = {}\n",
    "\n",
    "for name, table in zip(CL_LAUNCHER_TABLE_NAMES + CLDRIVE_TABLE_NAMES, CL_LAUNCHER_TABLES + CLDRIVE_TABLES):\n",
    "    r = []\n",
    "    for testbed in session.query(Testbed).all():\n",
    "        nresult = session.query(table).filter(table.testbed == testbed).count()\n",
    "\n",
    "        q = session.query(table.outcome, sql.func.count(table.outcome)).filter(\n",
    "            table.testbed == testbed).group_by(table.outcome).order_by(\n",
    "                sql.desc(sql.func.count(table.outcome)))\n",
    "\n",
    "        for outcome, count in q.all():\n",
    "            ratio = (count / nresult) * 100\n",
    "            r.append((DEVICES.get(testbed.device, testbed.device), outcome, count, ratio))\n",
    "    outcomes[name] = pd.DataFrame(r, columns=[\"Device\", \"Outcome\", \"Count\", \"% of Total Results\"])\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas tables of classifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = {}\n",
    "\n",
    "classificationsSort = [\n",
    "    'Invalid testcase',\n",
    "    'Build failure',\n",
    "    'Runtime crash',\n",
    "    'No majority',\n",
    "    'Wrong code',\n",
    "    'Okay'\n",
    "]\n",
    "\n",
    "def escape(val):\n",
    "    if val is None:\n",
    "        return val\n",
    "    else:\n",
    "        return str(classificationsSort.index(val)) + \". \" + val\n",
    "\n",
    "for name, table in zip(CL_LAUNCHER_TABLE_NAMES + CLDRIVE_TABLE_NAMES, CL_LAUNCHER_TABLES + CLDRIVE_TABLES):\n",
    "    r = []\n",
    "    for testbed in session.query(Testbed).all():\n",
    "        nresult = session.query(table).filter(table.testbed == testbed).count()\n",
    "\n",
    "        q = session.query(table.classification, sql.func.count(table.classification)).filter(\n",
    "            table.testbed == testbed).group_by(table.classification).order_by(\n",
    "                sql.desc(sql.func.count(table.classification)))\n",
    "\n",
    "        for val, count in q.all():\n",
    "            ratio = (count / nresult) * 100\n",
    "            r.append((DEVICES.get(testbed.device, testbed.device), escape(val), count, ratio))\n",
    "    classifications[name] = pd.DataFrame(r, columns=[\"Device\", \"Classification\", \"Count\", \"% of Total Results\"])\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications[\"CLgen w. cl_launcher\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from labm8 import viz\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_outcomes(table, name, dictname=outcomes, key='Outcome'):\n",
    "    ax = dictname[name].pivot('Device', key)['Count'].plot(\n",
    "        kind='bar', stacked=True, colormap=\"Reds_r\", sort_columns=True)\n",
    "\n",
    "    nprog = session.query(table.program_id).group_by(table.program_id).count()\n",
    "    nparam = session.query(table.params_id).group_by(table.params_id).count()\n",
    "    plt.title(f\"{nprog} {name} x {nparam} parameters\")\n",
    "    plt.ylabel(\"Results\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.ylim(0, nprog * nparam)\n",
    "\n",
    "    # reverse legend order (because plot stacks from bottom to top, and legend goes from top to bottom)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    viz.finalise(figsize=(3.5, 8))\n",
    "    \n",
    "    \n",
    "def summarize(table_name):\n",
    "    \"\"\" summarize a table of classifications \"\"\"\n",
    "    table = classifications[table_name]\n",
    "\n",
    "    def get_val(classification):\n",
    "        try:\n",
    "            return table.loc[\n",
    "                (table['Device'] == device) & (table['Classification'] == classification)]['Count'].values[0]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "    \n",
    "    columns = ['Platform', 'Device', 'Driver', 'Invalid Testcases', 'Build Failures', 'Runtime Crashes', 'Incorrect Outputs', 'Okay']\n",
    "    devices = sorted(set(table['Device'].values))\n",
    "\n",
    "    d = []    \n",
    "    for device in devices:\n",
    "        lookup = dict((v, k) for k, v in DEVICES.items())\n",
    "        full_name = lookup.get(device, device)\n",
    "        \n",
    "        # lookup the testbed\n",
    "        q = session.query(Testbed).filter(Testbed.device == full_name).all()\n",
    "        if len(q) != 1:\n",
    "            raise q\n",
    "        testbed = q[0]\n",
    "        \n",
    "        r = [\n",
    "            PLATFORMS.get(testbed.platform, testbed.platform),\n",
    "            device,\n",
    "            DRIVERS.get(testbed.driver, testbed.driver),\n",
    "            get_val('0. Invalid testcase'),\n",
    "            get_val('1. Build failure'),\n",
    "            get_val('2. Runtime crash'),\n",
    "            get_val('3. Wrong code'),\n",
    "            get_val('4. Okay'),\n",
    "        ]\n",
    "        d.append(r)\n",
    "    summary = pd.DataFrame(d, columns=columns, index=range(1, len(devices)+1))\n",
    "\n",
    "    !cd ~/docs/paper-project_b/ && git pull --rebase >/dev/null\n",
    "    name = '-'.join(table_name.split())\n",
    "    with open(os.path.expanduser(f\"~/docs/paper-project_b/build/tab/results-{name}.tex\"), \"w\") as outfile:\n",
    "        summary.to_latex(buf=outfile)\n",
    "    !cd ~/docs/paper-project_b/build && git add . && git commit -m \"auto: summarize table\" >/dev/null && git push >/dev/null\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes[\"CLSmith\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize('CLSmith')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lc(src):\n",
    "    return len(src.strip().split(\"\\n\"))\n",
    "\n",
    "lcs = np.array([lc(row[0]) for row in session.query(CLSmithProgram.src)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.median(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_outcomes(CLSmithResult, \"CLSmith\", dictname=classifications, key='Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLgen w. cl_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes[\"CLgen w. cl_launcher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize('CLgen w. cl_launcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_outcomes(cl_launcherCLgenResult, \"CLgen w. cl_launcher\", dictname=classifications, key='Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLSmith w. cldrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes[\"CLSmith w. cldrive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize('CLSmith w. cldrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_outcomes(cldriveCLSmithResult, \"CLSmith w. cldrive\", dictname=classifications, key='Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes[\"GitHub\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize('GitHub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_outcomes(GitHubResult, \"GitHub\", dictname=classifications, key='Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes[\"CLgen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize('CLgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_outcomes(CLgenResult, \"CLgen\", dictname=classifications, key='Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_b",
   "language": "python",
   "name": "project_b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
