\begin{abstract}
%We introduce DeepSmith, a novel approach to the compiler validation problem. While prior works require expert domain knowledge to craft random program generators from probabilistic grammars or program mutation, we instead frame the generation of random programs a language modeling problem. We apply deep learning techniques over millions of lines of handwritten code in order to automatically construct models capable of generating plausible, human-like code samples.

% Our approach complements the desires of compiler developers, who desire small test cases based on ``plausible'' language constructs.

Compilers should produce correct code for valid inputs, and meaningful errors for invalid inputs. Failure to do so can hinder software development or even cause catastrophic runtime errors. Still, properly testing compilers is hard. Manually assembled test suites cannot cover the enormous range of possible inputs. %
%
% Random program generators come close to it but they require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. A better way for testing compilers is needed.
% 
Random program generation --- fuzzing --- is an effective technique for discovering bugs in compilers but successful program generators require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. A better way for testing compilers is needed.

% A novel approach for the rapid development of broad, undirected fuzzers
We introduce DeepSmith, a novel approach which takes advantage of state-of-the-art deep learning techniques to automate and accelerate compiler validation. Our approach constructs a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, it applies established differential testing methodologies on them to expose bugs in compilers.

We test our approach on OpenCL, automatically exposing bugs in OpenCL compilers with little effort on our side. In over 1,500 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, %
%
% made up of XX interesting test cases, compared to a similar XX for the state of the art technique given the same time.
%
submitting XX bug reports.
%
Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require $3.03\times$ less time to generate and evaluate, and expose bugs which have not been found using existing techniques. Our system took 14 hours to train for OpenCL comprising only 500 lines of code, versus the state of the art taking 9 man months to port from a generator for C and 37,000 lines of code.
%\cc{TODO: complimentary approach}

%Compilers should produce correct code for valid inputs, and produce meaningful errors when inputs are invalid. Fixed test-suites and formal verification techniques cannot ensure these properties over the enormous range of possible inputs and complexity of optimizing compilers, leading to the development of random program generators that attempt to find compiler bugs by comparing the outputs of programs across compilers.

%Grammar-based random program generators require extensive development effort, and by design, yield large, ``implausible''  programs which bare little resemblance to actual real code written by human programmers. This is in contrast to the needs of compiler developers, who desire small test cases which are symptomatic of  ``plausible'' use cases. What is needed is a technique for biasing the production of random program generation such that they more closely resemble real handwritten code, and which expose bugs which are likely to arise from common use cases.

% Furthermore, extensive effort is expended on minimizing the risk of undefined behavior, which comes at the cost of reduced expressiveness.

%We introduce DeepSmith, a novel approach to the compiler validation problem. By framing the generation of random programs as a language modeling problem, we apply deep learning techniques over large corpuses of open source codes to learn models which describe the structure of real world codes. We use these models to generate tens of thousands of new programs, and show how established differential testing methodologies can be used to expose bugs in compilers which are symptomatic of real hand-written code.

%We apply our technique to OpenCL, automatically exposing bugs in OpenCL compilers which are likely to arise from real world use cases, in a ways which are not possible using existing techniques. In over 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting XX bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require $3.03\times$ less time to generate and evaluate, and expose bugs which have not be found using existing techniques. \cc{TODO: complimentary approach}

% Our paper serves two purposes: first, we propose a novel methodology for compiler test case generation, capable of finding bugs at a faster rate and covering more areas of the compiler than existing approaches; second, we report on the state of OpenCL implementations.
\end{abstract}
