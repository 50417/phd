% 6-10 pages, 9pt font
%
% Topics:
%
% * Machine learning based autotuning.
% * Representative benchmarking.
% * Automatic fault tolerance.
% * Run-time adaption.
%

% The following \documentclass options may be useful:
%
% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.
\documentclass[nonatbib,preprint,9pt]{sigplanconf}

\include{preamble}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{HLPGPGPU '16}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish,
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers,
                                  % short abstracts)

% \titlebanner{banner above paper title}        % These are ignored unless
% \preprintfooter{HLPGPGPU workshop '16}   % 'preprint' option specified.

% \title{Robust Autotuning of Stencil codes for GPUs with OmniTune}
% \title{Towards robust cross-architecture GPGPU patterns autotuning}
\title{Towards Collaborative Performance Tuning of Algorithmic Skeletons}

% \subtitle{Subtitle Text, if any}

\authorinfo{Chris Cummins\and Pavlos Petoumenos \and Michel Steuwer \and Hugh Leather}
           {University of Edinburgh}
           {c.cummins@ed.ac.uk, ppetoume@inf.ed.ac.uk, michel.steuwer@ed.ac.uk, hleather@inf.ed.ac.uk}

\maketitle

\begin{abstract}
  The physical limitations of microprocessor design have forced the
  industry towards increasing heterogeneous designs to extract
  performance. This trend has not been matched with adequate software
  tools, leading to a growing disparity between the availability of
  parallelism and the ability for application developers to exploit
  it.

  Algorithmic skeletons simplify parallel programming by providing
  high-level, reusable patterns of computation. Achieving performant
  skeleton implementations is a difficult task; skeleton authors must
  attempt to anticipate and tune for a wide range of architectures and
  use cases. This results in implementations that target the general
  case and cannot provide the performance advantages that are gained
  from tuning low level optimisation parameters. Autotuning combined
  with machine learning offers promising performance benefits in these
  situations, but the high cost of training and lack of available
  tools limits the practicality of autotuning for real world
  programming. We believe that performing autotuning at the level of
  the skeleton library can overcome these issues.

  In this work, we present OmniTune --- an extensible and distributed
  framework for dynamic autotuning of optimisation parameters at
  runtime. OmniTune uses a client-server model with a flexible API to
  support machine learning enabled autotuning. Training data is shared
  across a network of cooperating systems, using a collective approach
  to performance tuning.

  We demonstrate the practicality of OmniTune in a case study using
  the algorithmic skeleton library SkelCL. By automatically tuning the
  workgroup size of OpenCL Stencil skeleton kernels, we show that that
  static tuning across a range of GPUs and programs can achieve only
  $26\%$ of the optimal performance, while OmniTune achieves $92\%$ of
  this maximum, equating to an average $5.65\times$ speedup. This is
  achieved without introducing a significant runtime overhead, and
  enables portable, cross-device and cross-program tuning.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% % general terms are not compulsory anymore,
% % you may leave them out
% % \terms
% % term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}\label{sec:introduction}

General purpose programming with GPUs has been shown to provide huge
parallel throughput, but poses a significant programming challenge,
requiring application developers to master an unfamiliar programming
model (such as provided by CUDA or OpenCL) and architecture (SIMD with
a multi-level memory hierarchy). As a result, GPGPU programming is
often considered beyond the realm of everyday development. If steps
are not taken to increase the accessibility of such parallelism, the
gap between potential and utilised performance will continue to widen
as hardware core counts increases.

Algorithmic skeletons offer a solution to this this
\emph{programmability challenge} by raising the level of
abstraction. This simplifies parallel programming, allowing developers
to focus on solving problems rather than coordinating parallel
resources. Skeletons provide robust parallel implementations of common
patterns of computation which developers parameterise with their
application-specific code~\cite{Gonzalez2010}. This greatly reduces
the challenge of parallel programming, allowing users to structure
their problem-solving logic sequentially, while offloading the
cognitive cost of parallel coordination to the skeleton library
author. The rising number of skeleton frameworks supporting graphics
hardware illustrates the demand for high level abstractions for GPGPU
programming~\cite{Enmyren2010, Marques2013, Nugteren2014a}. The
challenge is in maintaining portable performance across the breadth of
devices in the rapidly developing GPU and heterogeneous architecture
landscape.


\subsection{The Performance Portability Challenge}

The performance of parallel programs is sensitive to low level
\emph{parameters}, and when tuning such parameters for maximising
performance, one size does not fit all. The suitability of parameter
values depends on the program implementation, the target hardware, and
the dataset that is operated upon. % TODO: citation
Iterative compilation and autotuning have been shown to help in these
cases by automating the process of tuning parameter values to match
individual execution environments. % TODO: citation
However, there have been few attempts to develop general mechanisms
for these techniques, and the time taken to develop ad-hoc autotuning
solutions and gather performance data is often prohibitively
expensive.

We believe that by embedding autotuning at the skeletal level, it is
possible to achieve performance with algorithmic skeletons that is
competitive with --- and in some cases, exceeds --- that of hand tuned
parallel implementations which traditionally came at the cost of many
man hours of work from expert programmers to develop.

Incorporating autotuning into algorithmic skeleton libraries has two
key benefits: first, it minimises development effort by requiring only
a modification to the skeleton implementation rather than to every
user program; and second, by targeting a library, it enables a broader
and more substantive range of performance data to be gathered than
with ad-hoc tuning of individual programs.


\subsection{Contributions}

The key contributions of this work are:

\begin{itemize}
\item The design and implementation of a generic toolset for
  autotuning: \emph{OmniTune} is a novel and extensible framework for
  collaborative autotuning of optimisation parameters across the life
  cycle of programs.
\item The integration of OmniTune with an established skeleton library
  for CPU and multi-GPU parallelism, SkelCL~\cite{Steuwer2011}. We
  extend SkelCL to provide autotuning for the selection of OpenCL
  workgroup size for Stencil skeletons.
\item An empirical evaluation of OmniTune across 7 different
  architectures, demonstrating that OmniTune achieves 92\% of the best
  possible performance, providing a median speedup of $5.65\times$
  over the best possible statically chosen workgroup size.
\end{itemize}

\section{Motivation}

\begin{figure}
\centering
\begin{subfigure}[h]{.45\columnwidth}
\centering
\includegraphics[width=1.0\columnwidth]{img/motivation_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-1}
\end{subfigure}
~%
\begin{subfigure}[h]{.45\columnwidth}
\centering
\includegraphics[width=1.0\columnwidth]{img/motivation_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-2}
\end{subfigure}
\caption{%
  The performance of different workgroup sizes for the same stencil
  program on two different devices: (\subref{fig:motivation-1}) Intel
  CPU, (\subref{fig:motivation-2}) NVIDIA GPU. Selecting an
  appropriate workgroup size depends on the execution device.%
}
\label{fig:motivation-arch}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[h]{.45\columnwidth}
\centering
\includegraphics[width=1.0\columnwidth]{img/motivation_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-3}
\end{subfigure}
~%
\begin{subfigure}[h]{.45\columnwidth}
\centering
\includegraphics[width=1.0\columnwidth]{img/motivation_4}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:motivation-4}
\end{subfigure}
\caption{%
  The performance of different workgroup sizes for two different
  stencil programs on the same execution device. Selecting an
  appropriate workgroup size depends on the program.%
}
\label{fig:motivation-prog}
\end{figure}


In this section we will briefly examine the performance impact of
selecting workgroup size for the SkelCL Stencil skeleton. A full
explanation of SkelCL and the workgroup size parameter space is given
Section~\ref{sec:omnitune-skelcl}.

SkelCL uses OpenCL to parallelise skeleton operations across many
threads. In OpenCL, multiple threads are grouped into
\emph{workgroups}. The shape and size of these groups is known to have
a big impact on performance. For the SkelCL stencil skeleton, the
selection of workgroup size presents a two dimensional parameter
space, consisting of a number of rows and columns ($w_r \times w_c$).
Measuring and plotting the runtime of stencil programs using different
workgroup sizes allows us to compare the performance of different
workgroup sizes for different combinations of architecture and
program. Figure~\ref{fig:motivation-arch} shows this performance
comparison for a single stencil program on two different devices,
demonstrating that a good choice of workgroup size is device
dependent. The optimisation space of the same stencil benchmark on
different devices is radically different --- not only does the optimal
workgroup size change between devices, but the performance of
suboptimal workgroup sizes is also dissimilar. The optimisation space
of~\ref{fig:motivation-1} has a grid-like structure, with clear
performance advantages of workgroup sizes at multiples of 8 for
$w_c$. A developer specifically targeting this device would learn to
select workgroup sizes following this pattern. This domain specific
knowledge clearly does not transfer to the device shown
in~\ref{fig:motivation-2}.

In Figure~~\ref{fig:motivation-prog}, we compare the performance of
two different stencil programs on the \emph{same} device, showing that
workgroup size choice is also program dependent. In each of these four
examples, the optimal workgroup size changes, as does the relative
performance of suboptimal parameters. The average speedup of the best
over the worst workgroup size is $37.0\times$, and the \emph{best}
average performance that can be achieved using a single fixed
workgroup size is only 63\% of the maximum.

SkelCL uses a fixed workgroup size. Since both the execution device
and the user-provided stencil code are not known until runtime,
selection of workgroup size should be made dynamically. To the best of
our knowledge, there are no existing systems for runtime autotuning of
arbitrary parameter values, and autotuners are generally developed
ad-hoc and on a per-case basis.


\section{The OmniTune Framework}\label{sec:autotune}

\begin{figure}
\centering
\includegraphics[width=.98\columnwidth]{img/omnitune-system-overview.pdf}
\caption{%
  OmniTune system architecture, showing the separate components and
  the one to many relationship between servers to client applications,
  and remotes to servers.%
  % \vspace{-2em}%
}
\label{fig:omnitune-system-overview}
\end{figure}

OmniTune is a novel framework for extensible, distributed autotuning
of parameter values at runtime using machine learning. It serves as a
generic platform for developing autotuning solutions, aiming to reduce
both the engineering time required to target new optimisation
parameters, and the time to deploy on new systems.

It emphasises collaborative, online learning of optimisation spaces. A
client-server architecture with clearly delineated separation of
concerns minimises the code footprint in client applications, enabling
quick re-purposing for autotuning targets. OmniTune provides a
lightweight interface for communication between each of the
components, and aims to strike a balance between offering a fully
featured environment for quickly implementing autotuning, while
providing enough flexibility to cater to a wide range of use
cases. First, we describe the overall structure of OmniTune and the
rationale for the design, followed by the interfaces and steps
necessary to apply OmniTune.


\subsection{System Architecture}

Common implementations of autotuning in the literature either embed
the autotuning logic within the each target application, % TODO:
                                % citation
or take a standalone approach in which the autotuner is a program
which must be invoked by the user to tune a target
application. % TODO: citation
Embedding the autotuner within each target application has the
advantage of providing ``always-on'' behaviour, but is infeasible for
complex systems in which the cost of building machine learning models
must be added to each program run. The standalone approach separates
the autotuning logic, at the expense of adding one additional step to
the build process. The approach taken in OmniTune aims to combine the
advantages of both techniques by implementing autotuning \emph{as a
  service}, in which a standalone autotuning server performs the heavy
lifting of managing training data and machine learning models, with a
minimal set of lightweight communication logic to be embedded in
target applications.

OmniTune is built around a three tier client-server model, shown in
Figure~\ref{fig:omnitune-system-overview}. The applications which are
to be autotuned are the \emph{clients}. These clients communicate with
a system-wide \emph{server}, which handles autotuning requests. The
server communicates and caches data sourced from a \emph{remote}
server, which maintains a global store of all autotuning data. There
is a many to one relationship between clients, servers, and remotes,
such that a single remote may handle connections to multiple servers,
which in turn may accept connections from multiple clients. This
design has two primary advantages: the first is that it decouples the
autotuning logic from that of the client program, allowing developers
to easily repurpose the autotuning framework to target additional
optimisation parameters without a significant development overhead for
the target applications; the second advantage is that this enables
collective tuning, in which training data gathered from a range of
devices can be accessed and added to by any OmniTune server.

The OmniTune framework is implemented as a set of Python classes,
which are inherited from or implemented to target specific autotuning
cases. The generic implementation of OmniTune's server and remote
components consists of 8987 lines of Python and MySQL code. No client
logic is provided, since that is use case dependent (See
Section~\ref{sec:omnitune-skelcl} for an example implementation for
SkelCL). Inter-process communication between client programs and the
server uses the D-Bus protocol. D-Bus is cross-platform, and bindings
are available for most major programming languages, allowing
flexibility for use with a range of clients. Communication between
servers and remotes uses TCP/IP (we used an Amazon Web Services
database instance for development).


\subsection{Autotuning Behaviour}

The goal of machine learning enabled autotuning is to build models
from empirical performance data of past programs to select parameter
values for new \emph{unseen} programs. Instead of an iterative process
of trial and improvement, parameter values are \emph{predicted}, by
building correlations between performance, and \emph{features}
(explanatory variables). The data used to build such models is called
training data. OmniTune supports autotuning using a separate offline
training phase, online training, or a mixture of both. For each
autotuning-capable machine, an OmniTune server acts as an intermediary
between training data and the client application, and hosts the
autotuning logic. On launch, a server requests the latest training
data from the remote, which it uses to build the relevant models for
performing prediction of optimisation parameter values. If additional
training data is gathered by the server, this can be uploaded to the
remote.

The autotuning technique is application-specific, and can depend on
the type of parameter being tuned (e.g. a binary flag or one or more
numeric values). The general pattern is that a client application will
request parameter values from an OmniTune server by sending it a set
of explanatory variables. The server will then use machine learning
models to form a prediction for the optimal parameter values and
return these. Crucially, there is a mechanism provided for the client
to \emph{refuse} parameter values. This functionality is provided for
cases where the predicted parameter values are in some way invalid and
do not lead to a functioning program. % TODO: citation

The server contains a library of machine learning tools to perform
parameter prediction, interfacing with the popular datamining software
suite Weka\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}} using
a Java Native Interface. The provided tools include classifiers,
regressors, and a selection of meta-learning algorithms.

OmniTune servers may perform additional feature extraction of
explanatory variables supplied by incoming client requests. The reason
for performing feature extraction on the server as opposed to on the
client side is that this allows the results of expensive operations
(for example, analysing source code of target applications) to be
cached for use across the lifespan of client applications. The
contents of these local caches are periodically and asynchronously
synced with the remote to maintain a global store of lookup tables for
expensive operations.


\subsection{Interfaces}

Key design elements of OmniTune are the interfaces exposed by the
server and remote components. Figure~\ref{fig:omnitune-comms} shows an
example communication pattern between the three components of an
OmniTune system using these interfaces. In the example, a server first
requests training data from the remote. A client application then
performs a training phase in which it requests a set of parameters for
training, evaluates the performance of the parameters, and then
submits a measured value, which the server uses to update the
remote. After training, another client program requests a set of
parameters for performance, refuses them, and makes a new request.

\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{img/omnitune-comms}
\caption{%
  An example communication pattern between OmniTune components,
  showing an offline training phase.%
}
\label{fig:omnitune-comms}
\end{figure}

\paragraph{Client-Server} An OmniTune server exposes a public
interface over D-Bus with four operations. Client applications invoke
these methods to request parameter values, submit new training
observations, and refuse suggested parameters:
%
\begin{itemize}
\item \textsc{Request}$(x) \to p$\\*Given explanatory variables $x$,
  request the parameter values $p$ which are expected to provide
  maximum performance.
\item \textsc{RequestTraining}$(x) \to p$\\*Given explanatory
  variables $x$, allow the server to select parameter values $p$ for
  evaluating their fitness.
\item \textsc{Submit}$(x, p, y)$\\*Submit an observed measurement of
  fitness $y$ for parameter values $p$, given explanatory variables
  $x$.
\item \textsc{Refuse}$(x, p)$\\*Refuse parameter values $p$, given a
  set of explanatory variables $x$. Once refused, those parameters are
  blacklisted and will not be returned by any subsequent calls to
  \textsc{Request()} or \textsc{RequestTraining()} for the same
  explanatory variables $x$.
\end{itemize}
%
% This set of operations enables the core functionality of an autotuner,
% while providing flexibility for the client to control how and when
% training data is collected.

\paragraph{Server-Remote} The role of the remote is to provide
bookkeeping of training data for machine learning. Remotes allow
shared access to data from multiple servers using a transactional
communication pattern, supported by two methods:
%
\begin{itemize}
\item \textsc{Push}$(\bf{x}, \bf{p}, \bf{y})$\\*Asynchronously submit
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\item \textsc{Pull}$() \to (\bf{x}, \bf{p}, \bf{y})$\\*Request
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\end{itemize}


\subsection{Extensibility}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/omnitune-system-flow.pdf}
\caption[Optimisation parameter selection with OmniTune]{%
  Predicting parameter values and collecting training data with
  OmniTune.%
}
\label{fig:omnitune-system-flow}
\end{figure}

To extend OmniTune to target an optimisation parameter, a developer
extends the server class to implement response handlers for the four
public interface operations, and then inserts client code into the
target application to call these operations. The implementation of
these response handlers and invoking client code dictates the type of
autotuning methods supported. Figure~\ref{fig:omnitune-system-flow}
shows the flow diagram for an example OmniTune implementation, in
which a training observation is recorded for every training parameter
requested. In the next Section, we will detail the steps required to
apply OmniTune to SkelCL.


\section{Integration of OmniTune with SkelCL}\label{sec:omnitune-skelcl}

In this section we demonstrate the practicality of OmniTune by
integrating the framework into an established algorithmic skeleton
library. Introduced in~\cite{Steuwer2011}, SkelCL allows users to
easily harness the power of GPUs and CPUs for data parallel computing,
offering a set of OpenCL implementations of data parallel skeletons in
an object oriented C++ library.

The goal of SkelCL is to enable the transition towards higher-level
programming of GPUs, without requiring users to be intimately
knowledgeable of the concepts unique to OpenCL programming, such as
the memory or execution model. SkelCL has been shown to reduce
programming effort for developing real applications through the use of
robust pattern implementations and automated memory
management. Skeletons are parameterised with user functions which are
compiled into OpenCL kernels for execution on device hardware. SkelCL
supports operations on one or two dimensional arrays of data, with the
Vector and Matrix container types transparently handling lazy
transfers between host and device memory, and supporting partitioning
for multi-GPU execution. SkelCL is freely available and distributed
under dual GPL and academic
licenses\footnote{\url{http://skelcl.uni-muenster.de}}.

\subsection{The Stencil Skeleton}

Stencils are patterns of computation which operate on uniform grids of
data, where the value of each cell is updated based on its current
value and the value of one or more neighbouring elements, called the
\emph{border region}. Figure~\ref{fig:stencil-img} shows the use of a
stencil to apply a Gaussian blur to an image. SkelCL provides a 2D
stencil skeleton which allows users to provide a function which
updates a cell's value, while SkelCL orchestrates the parallel
execution of this function across all cells~\cite{Steuwer2014a}.

The border region is described by a \emph{stencil shape}, which
defines an $i \times j$ rectangular region around each cell which is
used to update the cell value. Stencil shapes may be asymmetrical, and
are defined in terms of the number of cells in the border region to
the north, east, south, and west of each cell. Given a function $f$, a
stencil shape $S$, and an $n \times m$ matrix:
%
\begin{equation}
\scriptsize
% \begin{split}
\stencil \left( f, S,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix} \right)
\to
\begin{bmatrix}
  z_{11} & \cdots & z_{1m} \\
  \vdots & \ddots & \vdots \\
  z_{n1} & \cdots & z_{nm}
\end{bmatrix}
% \end{split}
\end{equation}
%
where:
%
\begin{equation}
\scriptsize
z_{ij} = f \left(
\begin{bmatrix}
  x_{i-S_n,j-S_w} & \cdots & x_{i-S_n,j+S_e} \\
  \vdots & \ddots & \vdots \\
  x_{i+S_s,j-S_w} & \cdots & x_{i+S_s,j+S_e}
\end{bmatrix} \right)
\end{equation}
%
For border region elements outside the bounds of the matrix, values
are substituted from either a predefined padding value, or the value
of the nearest element within the matrix, depending on user
preference.

A popular usage of Stencil codes is for iterative problem solving,
whereby a stencil operation is repeated over a range of discrete time
steps $0 \le t \le t_{max}$, and $t \in \mathbb{Z}$. An iterative
stencil operation $g$ accepts a customising function $f$, a Stencil
shape $S$, and a matrix $M$ with initial values $M_{init}$. The value
of an iterative stencil can be defined recursively as:
%
\begin{equation}
\scriptsize
g(f, S, M, t) =
\begin{cases}
  \stencil \left( f, S, g(f, S, M, t-1) \right),& \text{if } t \geq 1\\
  M_{init}, & \text{otherwise}
\end{cases}
\end{equation}
%
Examples of iterative stencils include cellular automata and partial
differential equation solvers. % Another extension of the stencil
% operation accepts an ordered list of customising functions which are
% applied sequentially for each iteration. This has applications for
% multi-stage stencil operations such as Canny Edge Detection, in
% which four distinct stencil operations are performed as a sequence.

In the implementation of the SkelCL stencil skeleton, each element in
the matrix is mapped to a unique thread (known as a \emph{work item}
in OpenCL) which applies the user-specified function. The work items
are then divided into \emph{workgroups} for execution on the target
hardware. Each work-item reads the value of its corresponding matrix
element and the surrounding elements defined by the border
region. Since the border regions of neighbouring elements overlap, the
value of all elements within a workgroup are copied into a
\emph{tile}, allocated as a contiguous region of the fast, but small
local memory. As local memory access times are much faster than that
of global device memory, this greatly reduces the latency of the
border region reads performed by each work item. Changing the size of
workgroups thus affects the amount of local memory required for each
workgroup, and in turn affects the number of workgroups which may be
simultaneously active on the device. While the user defines the data
size and type, the shape of the border region, and the function being
applied to each element, it is the responsibility of the SkelCL
stencil implementation to select an appropriate workgroup size to use.

\subsection{Optimisation Parameters}\label{subsec:op-params}

SkelCL stencil kernels are parameterised by a workgroup size $w$,
which consists of two integer values to denote the number of rows and
columns in a workgroup. The space of optimisation parameter values is
subject to hard constraints, and these constraints cannot conveniently
be statically determined. Contributing factors are architectural
limitations, kernel constraints, and refused parameters.  Each OpenCL
device imposes a maximum workgroup size which can be statically
checked. These are defined by archiectural limitations of how code is
mapped to the underlying execution hardware. Typical values are powers
of two, e.g.\ 1024, 4096, 8192. At runtime, once an OpenCL program has
been compiled to a kernel, users can query the maximum workgroup size
supported by that particular kernel dynamically. This value cannot
easily be obtained statically as there is no mechanism to determine
the maximum workgroup size for a given source code and device without
first compiling it, which in OpenCL does not occur until runtime.

Factors which affect a kernel's maximum workgroup size include the
number registers required for a kernel, and the available number of
SIMD execution units for each type of instructions in a kernel. In
addition to satisfying the constraints of the device and kernel, not
all points in the workgroup size optimisation space are guaranteed to
provide working programs. A \emph{refused parameter} is a workgroup
size which satisfies the kernel and architectural constraints, yet
causes a \texttt{CL\_OUT\_OF\_RESOURCES} error to be thrown when the
kernel is enqueued. Note that in many OpenCL implementations, this
error type acts as a generic placeholder and may not necessarily
indicate that the underlying cause of the error was due to finite
resources constraints. We define a \emph{legal} workgroup size as one
which, for a given \emph{scenario} (a combination of program, device,
and dataset), satisfies the architectural and kernel constraints, and
is not refused. The subset of all possible workgroup sizes
$W_{legal}(s) \subset W$ that are legal for a given scenario $s$ is
then:
%
\begin{equation}
  W_{legal}(s) = \left\{w | w \in W, w < W_{\max}(s) \right\} - W_{refused}(s)
\end{equation}
%
Where $W_{\max}(s)$ can be determined at runtime prior to the kernels
execution, but the set $W_{refused}(s)$ can only be determined
experimentally.


\begin{figure}
\centering
\includegraphics[width=.98\columnwidth]{img/lena-stencil.pdf}
\caption{%
  Application of a Gaussian blur stencil operation to an image, with a
  border region of radius 1. In a Gaussian blur, pixel values are
  interpolated with neighbouring pixels, producing a smoothed effect.%
}
\label{fig:stencil-img}
\end{figure}


The \emph{oracle} workgroup size $\Omega(s) \in W_{legal}(s)$ of a
scenario $s$ is the $w$ value which provides the lowest mean
runtime. The relative performance $p(s,w)$ of a particular workgroup
against the maximum available performance for that scenario, within
the range $0 \le p(s,w) \le 1$, is the ratio of the runtime of a
program with workgroup size $w$ over the oracle workgroup size
$\Omega(s)$. For a given workgroup size, the average performance
$\bar{p}(w)$ across a set of scenarios $S$ can be found using the
geometric mean of performance relative to the oracle:
%
\begin{equation}
  \bar{p}(w) =
  \left(
    \prod_{s \in S} p(s, w)
  \right)^{1/|S|}
\end{equation}
%
The \emph{baseline} workgroup size $\bar{w}$ is the value which
provides the best average case performance across a set of
scenarios. Such a baseline value represents the \emph{best} possible
performance which can be achieved using a single, statically chosen
workgroup size. By defining $W_{safe} \in W$ as the intersection of
legal workgroup sizes, the baseline can be found using:
%
\begin{align}
W_{safe} &= \cap \left\{ W_{legal}(s) | s \in S \right\}\\
\bar{w} &= \argmax_{w \in W_{safe}} \bar{p}(w)
\end{align}


\subsection{Machine Learning}

The optimisation space presented by the workgroup size of OpenCL
kernels is large, complex, and non-linear. The challenge is to design
a system which, given a set of prior observations of the empirical
performance of stencil codes with different workgroup sizes, predict
workgroup sizes for \emph{unseen} stencils which will maximise the
performance. Successfully applying machine learning requires plentiful
training data, the careful selection of features, and appropriate
machine learning methods. For the purpose of this work we use a
\emph{classification} approach, in which a classifier automatically
correlates patterns between explanatory variables (features) and the
workgroup sizes which provide optimal performance. The classifier used
is the popular J48 Decision Tree~\cite{Han2011}, chosen due to its low
classification cost and ability to efficiently handle large
dimensionality training data.

For each scenario, a total of 102 explanatory variables are extracted
to capture information about the device, program, and dataset. Device
features encode the device type (e.g. CPU or GPU, integrated or
external, connection bus), properties about the host (e.g.\ system
memory, maximum clock frequency), and numerous properties about the
execution device (e.g.\ number of compute units, local memory size,
global caches). Program features include per-instruction type
densities, the total number of basic blocks, and the total instruction
count. They are extracted using static instruction count passes over
an LLVM IR compiled version of the user stencil
implementation. Compilation to bitcode is a relatively expensive task,
so lookup tables are used to cache repeated uses of the same stencil
codes, identified by the source code checksum. Dataset features
include the data type and dimensions of the SkelCL container type.

To collect training data, we run multiple iterations of a stencil
program to enumerate the workgroup size optimisation space, and use
the OpenCL's Profiling API to record stencil kernel execution times in
the client application, which are then submitted to the OmniTune
server. The \textsc{RequestTraining}$(x)$ server interface returns a
workgroup size with a randomly selected even number of rows and
columns up to the maximum allowed:
$\left\{ 2x \in \mathbb{Z} | 1 \le x < \frac{W_{max}(s)}{2} \right\}$.

A parameterised template substitution engine is used to generate
synthetic stencil applications for gathering performance
data. Stencils templates are parameterised with a border region size
and \emph{complexity}, a simple metric which broadly dictates the
number of operations in a given stencil code.

Once the performance of a particular workgroup size for a scenario is
assessed, the set of features describing the scenario is labelled with
the oracle workgroup size, which is used as the target class to train
a classifier. A classifier uses these performance results to make
predictions of the oracle workgroup size for new sets of features, by
predicting a workgroup size from the set of oracle workgroup sizes of
the training data.
% TODO: reword? ^^^

This approach presents the problem that after training, there is no
guarantee that the set of workgroup sizes which may be predicted is
within the set of legal workgroup sizes for future scenarios. This may
result in a classifier predicting a workgroup size which is not legal
for a scenario, $w \not\in W_{legal}(s)$, either because it exceeds
$W_{\max}(s)$, or because the parameter is refused. If this occurs, a
\emph{nearest neighbour} approach is used to select the workgroup size
$w$ which is expected to be legal and has the lowest Euclidian
distance to the predicted value $c$:
%
\begin{equation}
  w = \underset{w \in W_{legal(s)}}{\argmin} \sqrt{\left(c_r - w_r\right)^2 + \left(c_c - w_c\right)^2}
\end{equation}
%
This process of selecting alternative parameters will iterate until a
legal parameter is found.

\subsection{Implementation}

The OmniTune framework consists of 8987 lines of Python and MySQL
code. A further 976 lines are required for the SkelCL frontend to
implement the server response handlers and database backend. By
design, the client-server model minimises the impact of number of
modifications that are required to enable autotuning in client
applications. The only modification required to SkelCL is to replace
the hardcoded values for workgroup size with a subroutine to request a
workgroup size from the OmniTune server over a D-Bus connection. To
use the system, a user must download a copy of SkelCL modified with
the OmniTune functionality, and start a local OmniTune server
instance. A configuration file is used to determine the domain address
and authentication details of the remote server. On first launch, the
OmniTune server will fetch the latest training data from the remote.


\section{Experimental Setup}

This section describes an exhaustive enumeration of the workgroup size
optimisation space for 429 combinations of architecture, program, and
dataset. It contains the methodology used to collect empirical
performance data on which to base performance comparisons of different
workgroup sizes, and the steps necessary to obtain repeatable results.

A full enumeration of the workgroup size optimisation spaces was
performed across synthetically generated benchmarks and four reference
stencil benchmarks: Canny Edge Detection, Conway's Game of Life, Heat
Equation, and Gaussian Blur. Performance data was collected from 7
experimental platforms, comprising 4 GPU devices: AMD Tahiti 7970,
Nvidia GTX 590, Nvidia GTX 690, Nvidia GTX TITAN; and 3 CPU devices:
Intel i5-2430M, Intel i5-4570, i7-3820. Each platform was unloaded,
frequency governors disabled, and benchmark processes set to the
highest priority available to the task scheduler. Datasets and
programs were stored in an in-memory file system. For each program,
dataset sizes of size $512\times512$, $1024\times1024$,
$2048\times2048$, and $4096\times4096$ were used. A minimum of 30
samples were recorded for each scenario and workgroup size.

Program behavior was validated by comparing program output against a
gold standard output collected by executing each of the real-world
benchmarks programs using the baseline workgroup size. The output of
real-world benchmarks with other workgroup sizes is compared to this
gold standard output to test for correct program execution.


\section{Evaluation}\label{sec:evaluation}

This section evaluates the performance of OmniTune when tasked with
selecting workgroup sizes for SkelCL stencil codes. First we discuss
measurement noise present in the experimental results, and the methods
used to accommodate for it. Then we examine the observed effect that
workgroup size has on the performance of SkelCL stencils. The
prediction quality of OmniTune is scrutinised for portability across
programs, devices, and datasets.

The experimental results consist of measured runtimes for a set of
\emph{test cases}, collected using the methodology explained in the
previous section. Each test case $\tau_i$ consists of a scenario,
workgroup size pair $\tau_i = (s_i,w_i)$, and is associated with a
\emph{sample} of observed runtimes from multiple runs of the
program. A total of 269813 evaluated, which represents an exhaustive
enumeration of the workgroup size optimisation space for 429
scenarios. For each scenario, runtimes for an average of 629 (max
7260) unique workgroup sizes were measured. The average sample size of
runtimes for each test case is 83 (min 33, total 16917118).


\subsection{Statistical Soundness}

\begin{figure}
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-1}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-2}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-3}
\end{subfigure}
\\
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_4}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-4}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_5}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-5}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_6}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-6}
\end{subfigure}
\\
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_7}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-7}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_8}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-8}
\end{subfigure}
~%
\begin{subfigure}[h]{.32\columnwidth}
\centering
\includegraphics[width=\textwidth]{img/runtimes_histogram_9}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:runtimes-histogram-9}
\end{subfigure}
\caption[Distribution of stencil code runtimes]{%
  Distribution of runtime samples for test cases from three
  devices. Each plot contains a 35-bin histogram of 1000 samples, and
  a fitted kernel density estimate with bandwidth 0.3. The sample mean
  is shown as a vertical dashed line. The top row are from the Intel
  i5-4570, the second row from the Nvidia GTX 590, and the third row
  from the AMD Tahiti 7970. In some of the plots, the distribution of
  runtimes is bi- or multi-modal, and skewed to the lower end of the
  runtimes range.%
}
\label{fig:runtime-histograms}
\end{figure}

The complex interaction between processes competing for the finite
resources of a system introduces many sources for noise in program
runtime measurements. Before making any judgements about the relative
performance of optimisation configurations, we must establish the
level of noise present in these measurements. To do this, we evaluate
the distribution of runtimes for a randomly selected 1000 test cases,
recording 1000 runtime observations for each. We can then produce
fine-grained histograms of runtimes for individual test
cases. Figure~\ref{fig:runtime-histograms} shows an example nine of
these, for test cases from three devices. The plots show that the
distribution of runtimes is not always Gaussian; rather, it is
sometimes multimodal, and generally skewed to the lower end of the
runtime range, with a long ``tail'' to the right. This fits our
intuition that programs have a hard \emph{minimum} runtime enforced by
the time taken to execute the instructions of a program, and that
noise introduced to the system extends this runtime. For example,
preempting an OpenCL process on a CPU so that another process may run
may cause the very long tail visible in
Figure~\ref{fig:runtimes-histogram-1}.

The central limit theorem allows the assumption of an underlying
Gaussian distribution for samples of size $\ge 30$~\cite{Georges2007}.
Given our minimum sample size of 33, we can use 95\% confidence
intervals to provide statistical confidence that the arithmetic mean
of observed runtimes with respect to the true mean. As the number or
samples increases, we should expect the size of the confidence
interval to shrink. This is illustrated in Figure~\ref{fig:ci-trends},
which plots the average size of 95\% confidence intervals across the
1000 test cases, normalised to their respective means, as a function
of sample size. It shows the diminishing returns that increasing
sample size provides. For example, increasing the sample count from 10
to 30 results in an approximate 50\% reduction in confidence interval
size. Increasing the sample size from 30 to 50 results in only a 25\%
reduction.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/ci_trend}
\caption[Confidence interval size vs.\ sample count]{%
  Ratio of confidence interval to mean as a function of sample
  count. Two dashed lines indicate the confidence intervals at the
  minimum (3.7\%) and mean (2.5\%) number of samples used in the
  experimental dataset.%
}
\label{fig:ci-trends}
\end{figure}

By comparing the average confidence interval at different sample
counts against the full experiment results of 269813 test cases, we
can assert with 95\% confidence that the true mean for each test case
is within 2.5\% of the sample mean (given the average number of
samples per test case), or 3.7\% in the worst case (at the minimum
number of samples). Since the differences between baseline and optimal
workgroup sizes is often well in excess of 100\%, there is no overlap
of confidence intervals between competing workgroup sizes.


\subsection{Workgroup Size Optimisation Space}

In this subsection we explore the impact that the workgroup size
optimisation space has on the performance of SkelCL stencil programs.

\subsubsection{Oracle Workgroup Sizes}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/num_params_oracle.pdf}
\caption[Oracle accuracy vs.\ number of workgroup sizes]{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that can be achieved using a
  single statically chosen workgroup size is 15\%. Achieving 50\%
  oracle accuracy requires a minimum of 14 distinct workgroup sizes.%
}
\label{fig:oracle-accuracy}
\end{figure}

For each scenario $s$, the oracle workgroup size $\Omega(s)$ is the
workgroup size which resulted in the lowest mean runtime. If the
performance of stencils were independent of workgroup size, we would
expect that the oracle workgroup size would remain constant across all
scenarios $s \in S$. Instead, we find that there are 135 unique oracle
workgroup sizes, with 31.5\% of scenarios having a unique workgroup
size. This demonstrates the difficult in attempting to tune for
\emph{optimal} parameter values, since 14 distinct workgroup sizes are
needed to achieve just 50\% of the oracle accuracy
(Figure~\ref{fig:oracle-accuracy}), although it is important to make
the distinction that oracle \emph{accuracy} and \emph{performance} are
not equivalent. The workgroup size which is most frequently optimal is
$w_{(64 \times 4)}$, which is optimal for 15\% of scenarios. Note that
this is not adequate to use as a baseline for static tuning, as it
does not respect legality constraints, that is
$w_{(64 \times 4)} \not\in W_{safe}$.


\paragraph{Maximum workgroup sizes}

We define the \emph{coverage} of a workgroup size to be the ratio
$0 \le x \le 1$ between the number of scenarios for which the
workgroup size is less than $W_{\max}(s)$, normalised to the total
number of scenarios. A coverage of 1 implies a workgroup size which is
always legal for all combinations of stencil and architecture. A
workgroup size with a coverage of 0 is never legal. Note that since
$W_{\max}(s)$ defines a hard limit for a given $s$, if statically
selecting a workgroup size, one must limit the optimisation space to
the smallest $W_{\max}(s)$ value, i.e.\ only the workgroup sizes with
a coverage of 1. The observed $W_{\max}(s)$ values range from
256--8192, which results in up to a 97\% reduction in the size of the
optimisation space when $W_{\max}(s) = 8192$, even though only 14\% of
scenarios have the minimum value of $W_{\max}(s) = 256$.

\paragraph{Refused Parameters}

In addition to the hard constraints imposed by the maximum workgroup
size, there are also refused parameters, which are workgroup sizes
which are rejected by the OpenCL runtime and do not provide a
functioning program. Of the 8504 unique workgroup sizes tested, 11.4\%
were refused in one or more test cases. An average of 5.5\% of all
test cases lead to refused parameters. For a workgroup size to be
refused, it must satisfy the architectural and program-specific
constraints which are exposed by OpenCL, but still lead to a
\texttt{CL\_OUT\_OF\_RESOURCES} error when the kernel is
enqueued. While uncommon, a refused parameter is an obvious
inconvenience to the user, as one would expect that any workgroup size
within the specified maximum should behave \emph{correctly}, if not
efficiently. For now, it is imperative that any autotuning system is
capable of adapting to these refused parameters by suggesting
alternatives when they occur.


\subsubsection{Baseline Parameter}

The baseline parameter $\bar{w}$ is the workgroup size which provides
the best overall performance while being legal for all scenarios. It
is the workgroup size $w \in W_{safe}$ which maximises the output of
the performance function $\bar{p}(w)$. As shown in
Table~\ref{tab:highest-legality}, only a \emph{single} workgroup size
$w_{(4 \times 4)}$ from the set of experimental results is found to
have a legality of 100\%, suggesting that an adaptive approach to
setting workgroup size is necessary not just for the sake of
maximising performance, but also for guaranteeing program correctness.

The utility of the baseline parameter is that it represents the best
performance that can be achieved through static tuning of the
workgroup size parameter. We can evaluate the performance of
suboptimal workgroup sizes by calculating the geometric mean of their
\emph{performance} for a particular scenario $p(s, w)$ across all
scenarios, $\bar{p}(w)$. The baseline parameter $\bar{p}(\bar{w})$
achieves only 24\% of the available performance.

Figure~\ref{fig:speedups} shows the speedup of the oracle workgroup
size over the baseline parameter $w_{(4 \times 4)}$ for all
scenarios. If we assume that sufficiently pragmatic developer with
enough time would eventually find this static optimal, then this
provides a reasonable comparison for calculating speedups of an
autotuner for workgroup size. Comparing the runtime of workgroup sizes
relative to the oracle allows us to calculate upper bounds on the
possible performance which can be expected from autotuning.


\subsubsection{Speedup Upper Bounds}

\begin{figure}
  \includegraphics[width=\columnwidth]{img/max_speedups}
  \caption[Workgroup size speedups]{%
    Speedup of oracle workgroup size over: the worst performing
    workgroup size for each scenario (\emph{Max}), the statically
    chosen workgroup size that provides the best overall performance
    ($w_{(4 \times 4)}$), and the human expert selected parameter
    ($w_{(32 \times 4)}$). Note that the human expert parameter is not
    legal for all scenarios.%
  }
\label{fig:speedups}
\end{figure}

For a given scenario $s$, the ratio of the workgroups sizes from
$W_{legal}(s)$ which provide the longest and shortest mean runtimes is
used to calculate an upper bound for the possible performance
influence of workgroup size:
%
\begin{equation}
r_{max}(s) = r(s, \argmax_{w \in W_{legal}(s)} t(s,w), \Omega(s))
\end{equation}
%
When applied to each scenario $s \in S$ of the experimental results,
we find the average of speedup upper bounds to be $15.14\times$ (min
$1.03\times$, max $207.72\times$). This demonstrates the importance of
tuning stencil workgroup sizes --- if chosen incorrectly, the runtime
of stencil programs can be extended by up to $207.72\times$. Note too
that for 5 of the scenarios, the speedup of the best over worst
workgroup sizes is $\le 5\%$. For these scenarios, there is little
benefit to autotuning; however, this represents only 1.1\% of the
tested scenarios. For 50\% of the scenarios, the speedup of the best
over worst workgroup sizes is $\ge 6.19\times$.


\subsubsection{Human Expert}

In the original implementation of the SkelCL stencil
skeleton~\cite{Breuer2013}, \citeauthor{Breuer2013} selected a
workgroup size of $w_{(32 \times 4)}$ in an evaluation of 4 stencil
operations on a Tesla S1070 system. We can use this as an additional
parameter to compare the relative performance of workgroup sizes
against. However, the $w_{(32 \times 4)}$ workgroup size is invalid
for 2.6\% of scenarios, as it is refused in 11 test cases. By device,
those are: 3 on the GTX 690, 6 on the i5-2430M, and 2 on the i5-4570.
For the scenarios where $w_{(32 \times 4)}$ \emph{is} legal, the human
expert chosen workgroup size achieves an impressive geometric mean of
79.2\% of the oracle performance. The average speedup of oracle
workgroup sizes over the workgroup size selected by a human expert is
$1.37\times$ (min $1.0\times$, max $5.17\times$). Since the workgroup
size selected by the human expert is not legal for all scenarios, we
will examine the effectiveness of heuristics for tuning workgroup
size.


\subsubsection{Heuristics}

In this subsection we will consider the effectiveness of instead
selecting workgroup size using two types of heuristics. The first,
using the maximum workgroup size returned by the OpenCL device and
kernel APIs to select the workgroup size adaptively. The second, using
per-device heuristics, in which the workgroup size is selected based
on the specific architecture that a stencil is operating on.


\paragraph{Using maximum legal size}

\begin{figure}
  \begin{subfigure}[h]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_max_wgsize}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-max-wgsize}
  \end{subfigure}
  \\
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_max_c}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-wg-c}
  \end{subfigure}
  ~%
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_max_r}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-wg-r}
  \end{subfigure}

  \caption[Workgroup size performances vs.\ size]{%
    Comparing workgroup performance relative to the oracle as function
    of: (\subref{fig:performance-max-wgsize})~maximum legal size,
    (\subref{fig:performance-wg-c})~number of columns, and
    (\subref{fig:performance-wg-r})~ number of rows. Each workgroup
    size is normalised to the maximum allowed for that scenario, $W_{\max}(s)$.%
  }
  \label{fig:performance-wgsizes}
\end{figure}

\begin{figure}
  \begin{subfigure}[h]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_kernels.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-kernels}
  \end{subfigure}
  \\
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_devices.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-devices}
  \end{subfigure}
  ~%
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_datasets.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-datasets}
  \end{subfigure}
  \caption[Workgroup size performances across device, kernel, and dataset]{%
    Performance relative to the oracle of workgroup sizes across all
    test cases, grouped by: (\subref{fig:performance-kernels})~kernels,
    (\subref{fig:performance-devices})~devices, and
    (\subref{fig:performance-datasets})~datasets.%
  }
  \label{fig:performances}
\end{figure}

A common approach taken by OpenCL developers is to set the workgroup
size for a kernel based on the maximum legal workgroup size queried
from the OpenCL APIs. For example, to set the size of 2D workgroup, a
developer the square root of the (scalar) maximum wgsize to set the
number of columns and rows (since $w_c \cdot w_r$ must be
$< W_{\max}(s)$). To consider the effectiveness of this approach, we
group the workgroup size performances based on the ratio of the
maximum allowed for each scenario. We can also perform this for each
of the two dimensions --- rows and columns --- of the stencil
workgroup size.

Figure~\ref{fig:performance-wgsizes} shows the distribution of
runtimes when grouped this way, demonstrating that the performance of
(legal) workgroup sizes are not correlated with the maximum workgroup
sizes $W_{\max}(s)$. However, when considering individual components,
we observe that the best median workgroup size performances are
achieved with a number of columns that is between 10\% and 20\% of the
maximum, and a number of rows that is between 0\% and 10\% of the
maximum.


\paragraph{Per-device workgroup sizes}

\begin{table}
  \scriptsize
  \centering
  \begin{tabular}{lllp{1cm}p{1cm}}
    \toprule
    Device &         Oracle & Legality & Perf.\ min & Perf.\ avg. \\
    \midrule
    AMD Tahiti 7970 &   $48\times 4$ &      1.0 &       0.54 &        0.91 \\
    Intel i5-2430M &  $64\times 16$ &      0.8 &       0.37 &        0.91 \\
    Intel i5-4570 &   $88\times 8$ &     0.89 &       0.33 &        0.89 \\
    Intel i7-3820 &  $40\times 24$ &     0.95 &       0.76 &        0.97 \\
    NVIDIA GTX 590 &  $12\times 2$ &      0.8 &        0.2 &         0.9 \\
    NVIDIA GTX 690 &   $64\times 4$ &     0.93 &       0.32 &        0.84 \\
    NVIDIA GTX TITAN &   $64\times 4$ &      1.0 &       0.26 &        0.81 \\
    \textbf{CPUs} &   $88\times 8$ &     0.88 &       0.33 &        0.91 \\
    \textbf{GPUs} &   $64\times 4$ &     0.76 &       0.26 &        0.86 \\
    \bottomrule
  \end{tabular}
  \caption[Performance of tuning with a per-device heuristic]{%
    Selecting workgroup size using a per-device heuristic. The mode
    optimal workgroup size for each device type $\bar{w}$ is evaluated
    based on legality, and relative performance to the oracle (minimum
    and average) when legal.%
  }
  \label{tab:heuristic-dev}
\end{table}

One possible technique to selecting workgroup size is to tune
particular values for each targeted execution device. This approach is
sometimes adopted for cases with particularly high requirements for
performance on a single platform, so it produces an interesting
contrast to evaluating a machine learning approach, which attempts to
predict workgroup sizes for unseen platforms without the need for an
expensive exploration phase on the new platform.

Figure~\ref{fig:performances} shows the performance of workgroup sizes
relative to the oracle across scenarios grouped by: kernel, device,
and dataset. When grouped like this, a number of observations can
made. First is that not all of the kernels are sensitive to tuning
workgroup size to the same degree. The \emph{sobel} kernel has the
lowest median performance, indicating that it is the most profitable
to tune, while the \emph{threshold} kernel is the least
profitable. Similarly, the Intel i7-3820 is far less amenable to
tuning than the other devices, while the Intel i5-4570 is the most
sensitive to the workgroup size parameter. Such variances in the
distributions of workgroup sizes suggest that properties underlying
the architecture, kernel, and dataset all contribute towards the
proper selection of workgroup size.

To test the performance of a per-device heuristic for selecting
workgroup size, we group the scenarios by device, and compare the
relative performance of all workgroup sizes for each group of
scenarios. The most frequently optimal workgroup size $\bar{w}$ for
each device is selected, and the legality and performance of each
scenario using that workgroup size is evaluated.
Table~\ref{tab:heuristic-dev} shows the results of this evaluation.
The GTX 690 and GTX TITAN share the same $\bar{w}_{(64 \times 4)}$
value, while every other device has a unique optimum. The general case
performance of these per-device parameters is very good, although
legality is only assured for the GTX TITAN and AMD 7970 (which did not
refuse any parameters). However, the worst case performance of
per-device workgroup sizes is poor for all except the i7-3820 (which
is least sensitive to tuning), suggesting that device alone is not
capable of reliably informing the choice of workgroup size.


\subsubsection{Summary}

In this subsection we have explored the performance impact of the
workgroup size optimisation space. By comparing the relative
performance of an average of 629 workgroup sizes for each of 429
scenarios, the following conclusions can be drawn:

\begin{enumerate}
\item The performance of a workgroup size for a particular scenario
  depends properties of the hardware, software, and dataset.
\item The performance gap between the best and workgroup sizes for a
  particular combination of hardware, software, and dataset is up to
  $207.72\times$.
\item Not all workgroup sizes are legal, and the space of legal
  workgroup sizes cannot statically be determined. Adaptive tuning of
  workgroup size is required to ensure reliable performance.
\item Differing scenarios have wildly different optimal workgroup
  sizes, and the best performance can be achieved using static tuning
  is optimal for only 15\% of scenarios.
\end{enumerate}
%
In the following subsection, we will evaluate the performance of
OmniTune for selecting workgroup sizes.


\subsection{Autotuning Workgroup Sizes}

In this Section we evaluate the performance of OmniTune for predicting
workgroup sizes. To perform an evaluation, we partition the
experimental data into a training set and a test set:
$S_{training} \subset S$ and $S_{testing} = S - S_{training}$. A set
of labelled training data $D_{training}$ is derived from
$S_{training}$, and the prediction quality is testing using the
validation set $D_{testing}$ derived from $S_{training}$. We use
multiple approaches to partitioning test data to evaluate the
prediction quality under different scenarios. The processes for
generating validation sets are:
%
\begin{itemize}
\item 10-fold --- shuffle the set of all data and divide into 10
  validation sets, each containing 10\% of the data. This process is
  repeated for 10 rounds, resulting in 100 validations of 10
  permutations of the data.
\item Synthetic --- divide the training data such that it consists
  solely of data collected from synthetic benchmarks, and use data
  collected from real-world benchmarks to test.
\item Device --- partition the training data into $n$ sets, one for
  each device. Use $n-1$ sets for training, repeating until every
  partition has been used for testing once.
\item Kernel --- partition the training data into $n$ sets, one for
  each kernel. Use $n-1$ sets for training, repeating until every
  partition has been used for testing once.
\item Dataset --- partition the training data into $n$ sets, one for
  each type of dataset. Use $n-1$ sets for training, repeating until
  every partition has been used for testing once.
\end{itemize}
%
Training data consists of pairs of feature vectors $f(s)$ and oracle
workgroup sizes $\Omega(s)$:
%
\begin{equation}
  D_{training} = \left\{ (f(s),\Omega(s)) | s \in S_{training} \right\}
\end{equation}
%
Testing data are not labelled with oracle workgroup sizes:
%
\begin{equation}
  D_{testing} = \left\{ f(s) | s \in S_{testing} \right\}
\end{equation}
%
Workgroup sizes are predicted for each scenario in the testing set,
and the quality of the predicted workgroup size is evaluated using the
following metrics:
%
\begin{itemize}
\item accuracy (binary) --- whether the predicted workgroup size is
  the true oracle, $p(f(s)) = \Omega(s)$.
\item validity (binary) --- whether the classifier predicted a
  workgroup size which satisfies the workgroup size constraints
  constraints, $p(f(s)) < W_{\max}(s)$.
\item refused (binary) --- whether the classifier predicted a
  workgroup size which is refused, $p(f(s)) \in W_{refused}(s)$.
\item performance (real) --- the relative performance of the predicted
  workgroup size relative to the oracle,
  $0 \le r(p(f(s)), \Omega(s)) \le 1$.
\item speedups (real) --- the relative performance of the predicted
  workgroup size relative to the baseline workgroup size
  $w_{(4 \times 4)}$, and human expert workgroup size
  $w_{(32 \times 4)}$ (where applicable).
\item time (real) --- the round trip time required to make the prediction,
  as measured by the OmniTune client. This includes classification
  time and inter-process communication overheads between the client
  and server.
\end{itemize}
%
The \emph{validty} and \emph{refused} metrics measure how often the
nearest neighbour fallback strategy is required to select a legal
workgroup size $w \in W_{legal}(s)$.

The purpose of this evaluation is to test the effectiveness of machine
learning-enabled autotuning for predicting workgroup sizes of SkelCL
stencils codes.

The classifier achieves good speedups over the baseline. Average
classification speedups across all validation sets range between
$4.61\times$ and $5.05\times$. Figures~\ref{fig:class-syn}
and~\ref{fig:class-arch} show a summary of results using 10-fold
cross-validation and cross-device validation, respectively.

By isolating the test cases where an illegal or refused parameter was
predicted, we can analyse the effectiveness of the
\textsc{NearestNeighbour} fallback handler. The handler has an average
speedup across all validation sets of $5.26\times$, indicating that it
successfully exploits the structure of the optimisation spaces.

\begin{table}
\scriptsize
\centering
\begin{tabular}{llll}
\toprule
              Job &    Performance &            Speedup &       Human Expert \\
\midrule
          10-fold &           92\% &       $5.65\times$ &       $1.26\times$ \\
        Synthetic &           92\% &       $4.79\times$ &       $1.13\times$ \\
           Device &           85\% &       $5.23\times$ &       $1.17\times$ \\
           Kernel &           89\% &       $5.43\times$ &       $1.21\times$ \\
          Dataset &           91\% &       $5.63\times$ &       $1.25\times$ \\
 \textbf{Average} &  \textbf{90\%} &  $\bm{5.45\times}$ &  $\bm{1.22\times}$ \\
\bottomrule
\end{tabular}
\caption{Validation results for J48 and \textsc{NearestNeighbour}
  classification.}
\label{tab:class}
\end{table}


\subsection{Classifier selection}

\begin{figure}
\centering
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-1}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-2}
\end{subfigure}
\\
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-3}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_5}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-4}
\end{subfigure}
\caption{%
  Heatmaps of classification errors for a subset of the optimisation
  space using four different classifiers. The shading in each cells
  indicates if it is predicted less frequently (blue), ore more
  frequently (red) than it is optimal. Colour gradients are normalised
  across plots.%
}
\label{fig:class-hmaps}
\end{figure}

The fastest classifier is J48, due to the it's simplicity (it can be
implemented as a sequence of nested \texttt{if}/\texttt{else}
statements).

Figure~\ref{fig:class-hmaps} visualises the classification errors of
each of the autotuning techniques. It shows that while the performance
of all of the classifiers is comparable, the distribution of
predictions is not. Only the NaiveBayes and RandomForest classifiers
predicted the human expert selected workgroup size of
$w_{(32 \times 4)}$ as frequently, or more frequently, than it was
optimal.


\subsubsection{Summary}

From an evaluation of 17 different autotuning techniques using 5
different types of validation sets, the following conclusions about
autotuning performance can be drawn:
%
\begin{itemize}
\item The performance of predicted workgroup sizes for unseen devices
  is within 8\% of the performance for known devices.
\end{itemize}


\section{Related Work}\label{sec:related}

Early work in autotuning applied iterative search techniques to the
space of compiler optimisations~\cite{Bodin1998}. Iterative
compilation has been demonstrated to be a highly effective form of
empirical performance tuning for selecting compiler
optimisations. Given the huge size of the compiler optimisation space
(sometimes exceeding $10^{100}$ possible combinations), it is often
unfeasible to perform an exhaustive search of the entire optimisation
space.

Machine learning techniques have been successfully employed to reduce
the cost of iterative compilation. In~\cite{Agakov}, using statistical
models to focus a search of the optimisation
space. In~\cite{Stephenson2003}, using ``meta optimisation'' to tune
compiler heuristics through an evolutionary algorithm to automate the
search of the optimisation space. In~\cite{Fursin2011},
\citeauthor{Fursin2011} incorporated machine learning-enabled
self-tuning into the GCC compiler.

Optimising GPGPU programs presents different challenges to that of
traditional CPU programming. \citeauthor{Ryoo2008a} demonstrated
speedups of up to $432\times$ for matrix multiplication in CUDA
through the appropriate use of zero-overhead thread scheduling, memory
bandwidth, and thread grouping. The importance of proper exploitation
of local shared memory and synchronisation costs is explored
in~\cite{Lee2010}. In~\cite{Chen2014}, data locality optimisations are
automated using a description of the hardware and a
memory-placement-agnostic compiler. The authors demonstrate impressive
speedups of up to $2.08\times$, although at the cost of requiring
accurate memory hierarchy descriptor files for all targeted
hardware. The descriptor files must be hand generated, requiring
expert knowledge of the underlying hardware in order to properly
exploit memory locality. \citeauthor{Magni2014} demonstrate that
thread coarsening of OpenCL kernels can lead to speedups in program
performance between $1.11\times$ and $1.33\times$
in~\cite{Magni2014}. The authors achieve this using a machine learning
model to predict optimal thread coarsening factors based on the static
features of kernels, and an LLVM function pass to perform the required
code transformations.

% The size of the GPGPU optimisation space is reduced
% in~\cite{Ryoo2008} by using a common subset of optimal
% configurations across a set of training examples. This technique
% reduces the search space by 98\%, although it does not guarantee
% that for a new program, the reduced search space will include the
% optimal configuration.

Auotuning for stencil codes is explored in~\cite{Kamil2010}. A code
generator for Fortran 95 stencils expression and generates tuned
shared-memory parallel implementations in Fortan, C, or CUDA. The
system uses an IR to explore autotuning transformations, enumerating a
subset of the optimisation space and recording only a single execution
time for each configuration, reporting the fastest. They demonstrate
their system on 4 architectures using 3 benchmarks, with speedups of
up to $22\times$ compared to serial implementations. The CUDA code
generator does not optimise for the GPU memory hierarchy, using only
global memory. There is no directed search or cross-program
learning. In~\cite{Lutz2013}, \citeauthor{Lutz2013} demonstrate that
optimal swapping strategy for multi-GPU stencils depends on the size
of the grid, the number of partitions, and the connection mechanism
(e.g.\ PCI express)

OpenTuner is a general purpose toolkit for autotuning using iterative
search~\cite{Ansel2013}. It provides interfaces for application
developers to specify a search space and method for collecting
performance performance results. An \emph{ensemble} of search
techniques is then used to explore the space. They demonstrate their
technique using 14 benchmarks across 6 applications. OpenTuner does
not use machine learning, and performance datasets are not shared
across datasets. For these reasons, the burden of developing ad-hoc
autotuning approaches is lifted only from the application developer,
not from the end user. OpenTuner only stores data locally, so
performance data must be gathered (perhaps redundantly) from each new
device that uses it.  Our approach combines machine learning with
distributed training sets so that new users automatically benefit from
the collective tuning experience of other users. This removes the need
for redundant duplication of experiments, while dramatically reducing
the time to deployment.

In~\cite{Saclay2010,Memon2013}, \citeauthor{Saclay2010} advocate a
collaborative and ``big data'' driven approach to autotuning, arguing
that the challenges facing the widespread adoption of autotuning
methodologies can be attributed to a lack of common benchmarks,
datasets, and experimental methodologies. They propose the use of
``Collective optimization'' to leverage training experience across
devices using shared datasets. This idea is refined
in~\cite{Fursin2014}, in which the authors present a system for
sharing autotuning performance data, as well as additional metadata
about experimental setups. In addition to the mechanism for sharing
training datasets, our system provides the capabilities of performing
autotuning at runtime using a lightweight inter-process communication
interface. Collective Mind does not currently support run-time tuning.
Additionally, Collective Mind uses a NoSQL JSON format for storing
datasets. The relational schema used in this work offers greater
scaling performance and lower storage overhead as the amount of
performance data grows.  The authors do not provide an empirical
evaluation of their technique.


\section{Conclusions}\label{sec:conclusions}

As the trend towards higher core counts and increasing parallelism
continues, the need for high level, accessible abstractions to manage
such parallelism will continue to go. Autotuning proves a valuable aid
for achieving these goals, providing the benefits of low level
performance tuning while maintaining ease of use, without burdening
developers with optimisation concerns. As the need for autotuned
parallelism rises, the desire for collaborative techniques for sharing
performance data must be met with systems capable of supporting this
cross-platform learning.

In this work, we have presented an attempt to provide such a system,
by designing a novel framework which has the benefits of fast,
``always-on'' autotuning, while being able to synchronise data with
global repositories of knowledge which others may contribute to. The
framework provides an interface for autotuning which is sufficiently
generic to be easily re-purposed to target a range of optimisation
parameters.

To demonstrate the utility of this framework, we implemented a
frontend for predicting the workgroup size of OpenCL kernels for
SkelCL stencil codes. The publicly available implementation
\footnote{\url{https://github.com/ChrisCummins/omnitune}} predicts
workgroup sizes for OpenCL stencil skeleton kernels in order to
minimise their runtime on CPUs and multi-GPU systems. This
optimisation space is complex, non linear, and critical for the
performance of stencil kernels, with up to a $207.72\times$ slowdown
if an improper value is picked. Selecting the correct workgroup size
is difficult --- requiring a knowledge of the kernel, dataset, and
underlying architecture. The challenge is increased even more so by
inconsistencies in the underlying system which cause some workgroup
sizes to fail completely. Of the 269813 combinations of workgroup
size, device, program, and dataset tested; only a \emph{single}
workgroup size was valid for all test cases, and achieved only 24\% of
the available performance. The value selected by human experts was
invalid for 2.6\% of test cases. Autotuning in this space requires a
system which is resilient these challenges, and several techniques
were implemented to address them.

Runtime performance of autotuned stencil kernels is very promising,
achieving an average 90\% of the available performance with only a 3ms
autotuning overhead. Even ignoring the cases for which the human
expert selected workgroup size is invalid, this provides a
$1.33\times$ speedup, or a $5.57\times$ speedup over the best
performance that can be achieved using static tuning. Classification
performance is comparable when predicting workgroup sizes for both
unseen programs and unseen devices. I believe that the combination of
performance improvements and the collaborative nature of OmniTune
makes for a compelling case for the use of autotuning as a key
component for enabling performant, high level parallel programming.

The cost of offline training with OmniTune could be reduced by
exploring the use of adaptive sampling plans, such as presented
in~\cite{Leather2009}. This could reduce the number of runtime samples
required to distinguish good from bad optimisation parameter values.

Collaborative training --- hive mind for selecting training
parameters, and built-in redundancy checking/validation

TODO: deltas for push and pull, scalability for huge datasets

TODO: involuntary training requests, or non-binary selectors between
training and performance

%
% \appendix
% \section{Appendix Title}
%
% This is the text of the appendix, if you need one.
%

\acks

This work was supported by the UK Engineering and Physical Sciences
Research Council under grants EP/L01503X/1 for the University of
Edinburgh School of Informatics Centre for Doctoral Training in
Pervasive Parallelism
(\url{http://pervasiveparallelism.inf.ed.ac.uk/}),\\* EP/H044752/1
(ALEA), and EP/M015793/1 (DIVIDEND).

% We recommend abbrvnat bibliography style.

\label{bibliography}
\printbibliography


\end{document}
