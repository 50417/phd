\section{Compiler Test-case Generation}\label{sec:motivation}

\begin{figure}
	\centering
	\includegraphics[width=.9\columnwidth]{img/difftest} %
	% \vspace{-2em}%
	\caption{%
		Differential testing compilers on a single program. The compilers may be on different systems.%
	}%
	\label{fig:deeptune}
\end{figure}

\begin{itemize}
	\item \emph{Csmith} defines a probabilistic grammar over a large subset of the C programming language, and recursively instantiates a program, using rigorous and conservative static and dynamic analysis to ensure that programs are \emph{guaranteed} to be free from the 190 undefined behaviors of the C language specification.
	\item Because Csmith programs are free form undefined behavior, there is only a single interpretation. This allows oracle-less differential testing across compilers, using a voting heuristic to identify erroneous compiler outputs.
	\item Csmith does not guarantee program termination. They used a timeout to catch non-termination, finding that about 10\% of Csmith generated programs do not terminate.
	\item They identify two classes of bugs: \emph{wrong-code} bugs, which are programs which are silently miscompiled so as to compute the wrong result; and \emph{compiler-crash} bugs, which cause the compiler to fail, either by segfault, timeout, internal assertion failure, etc.
	\item We do not believe that the guarantee of undefined behavior is an important factor for identifying compiler-crash bugs. In fact, it is not even necessarily important that programs be \emph{well formed}. Quoting: 
	\emph{``As a matter of implementation quality, a compiler vendor will usually fix a segmentation fault or similar problem even if the crash-inducing test case, for example, uses a variable without initialization.''}~\cite{Regehr2012a}.
	\item Csmith, and all other random program generation tools, suffer from the problem that they cannot guarantee the \emph{interestingness} of their test-cases. The best they can do is indirect confirmation, by showing that a number of their reported bugs have subsequently been fixed or marked with a high bug priority. We hypothesize that the bugs identified by modeling real handwritten source codes would be more likely to be of a high priority, since they stem from \emph{more likely} programs. This is in stark contrast to Csmith, which by design attempts to explore the space of \emph{unlikely} programs.
	\item The ``shape'' of programs generated by Csmith is expert driven. The shape of programs generated by CLgen is data driven. The 80 probabilities which control Csmith program generation are extensively hand tuned to produce programs which ``look right''. Our data driven approach does not require this. In fact, our approach is portable across changing usage of a programming language.
	\item The functionality of Csmith is also expert driven. Every language feature supported by Csmith must be laboriously hand crafted, and results in a very complex system of over 40,000 lines of C++ code (which still omits many language features used in real programs, like heap allocation). The language features supported by CLgen is bound only by those which have been used on GitHub. CLgen assigns a non-zero probability of generating programs using every language feature found on GitHub. We reason that if a language feature remains unused in the entirety of code on GitHub, then it is reasonable to assume that it is not a feature worth testing. CLgen is much simpler than Csmith, at only 1,000 lines of high level Python, of which only around 400 lines is required to describe the core program generator. In fact, in extending CLgen for compiler testing, we actually \emph{reduced} the size of the software.
	\item Csmith makes the claim that larger programs are more useful for differential testing. In their experiments, they found the most distinct crash errors in programs containing 8K--16K tokens (81KB file size). We do not believe this to be the case. Subsequent work in test case reduction~\cite{Regehr2012a} reduced Csmith program sizes by 74--594$\times$ while preserving the bug exposing behavior. They median \emph{reduced} Csmith program size they found was only 20 lines (0.5KB). We hypothesize that it is not the size of programs which is responsible for identifying the most bugs, but simply the \emph{total size of all programs tested}. That is, the rate of bugs is proportional to the total lines of code tested, not the lines of code in each test.
	\item CLsmith extends Csmith to the OpenCL programming language. OpenCL has a number of properties which make compiler testing increasingly significant: first, OpenCL compilers are \emph{aggressively} optimizing in oder to exploit heterogeneous performance. This makes the identification of \emph{wrong-code} bugs significant. Secondly, OpenCL compilers typically execute \emph{online}, meaning that in shipped products which use OpenCL, \emph{compiler crash} errors may be delivered to the customer, rather than caught by the developers themselves. In cases where the compiler crash causes the host operating system to also crash (we experienced this problem with XX drivers), this is an obvious, critical problem. Testing the portability of code should not be the burden of the application developer.
	\item The CLsmith experimental setup is broad but shallow. They test a large number of device, compiler configurations (21 in total); but not different compilers on the same device. This makes it hard to classify bugs as belonging to the compiler front/middle-end, or the device-specific code generation.
	\item Both Csmith and CLsmith use unrealistic \emph{safe-math} macros to wrap arithmetic operations. We do not.
	\item CLsmith makes it impossible for thread ID to modify control flow. However, in almost all real world codes, this is not the case. This is a big restriction.
\end{itemize}
