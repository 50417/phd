\section{Comparison to State-of-the-art}\label{sec:vs_clsmith}

In this section we report on an experiment using both CLSmith and CLgen for 48 hours each.

Total runtime for a test cases consists of the generation time, the time to execute the test harness on a configuration, and, if required, the time to reduce the test case.

Both testing frameworks were used for 48 hours each. CLSmith was configured with default settings. CLReduce was configured with the default settings (four parallel reduction threads).

\subsection{Experimental Setup}

Each device was configured to run the compiler testing system for a fixed 48 hours. All devices were otherwise unloaded.

\subsection{Results}

Table~\ref{tab:outcomes} shows the outcomes of test cases. An average of XX CLSmith and XX CLgen test cases were evaluated on each device, with an average time per test case of XX and XX respectively. The XX-fold increase in testing throughput achieved by CLgen is a result of XXX. Figure~\ref{fig:runtimes} shows runtimes. For CLgen, generation time scales linearly with program size. Disabling optimizaitons generally does not affect testing throughput significantly, with the exception of Config XX, in which the slow compilation of kernels containing structs greatly reduces the number of test cases evaluated. This is a known issue --- in XX the authors omnit testing on this configuration for this reason.

Column \textbf{bc} is of immediate value for developers; for all other outcomes, voting heuristics are required to expose anomalous results. CLSmith crashed 7 of the 20 configurations, CLgen crashed all of them. See Section X for examples of bugs found from build crashes.

Once voting heuristics have been applied, the result is table~\ref{tab:classifications}.

\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/runtimes}%
	\caption{%
		Test case runtimes, excluding timeouts. On average, test case generation and exeuction is $2.86\times$ and $4.70\times$ faster with CLgen than CLSmith, respectively. \cc{reduction times}%
	}%
	\label{fig:runtimes} %
\end{figure}


\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/outcomes}
	\caption{Results from 24 hours of testing using CLSmith and CLgen. Configuration \#. as per Table~\ref{tab:platforms}. $\pm$ denotes optimizations off ($-$) vs on ($+$). The remaining columns denote build failure (\textbf{bf}), build crash (\textbf{bc}), build timeout (\textbf{bto}), runtime crash (\textbf{c}), timeout (\textbf{to}), and passed (\textbf{\cmark}) outcomes for CLSmith and CLgen, respectively. \cc{Asterisk in 'total' column means incomplete data. All results are excluding reductions.}}
	\label{tab:outcomes}
\end{table*}

\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/megatable}
	\caption{Using voting heuristics to expose anomalous results in Table~\ref{tab:outcomes}. Columns denote wrong-code (w), build failure (\textbf{bf}), runtime crash (\textbf{c}), and timeout (\textbf{to}) classifications for CLSmith and CLgen, respectively. \cc{Asterisk in 'total' column means incomplete data. All results are excluding reductions.}}
	\label{tab:classifications}
\end{table*}

\cc{TODO: How long do reductions take? How many CLgen test cases can we run in that amount of time? Given the ratio of interesting CLgen test cases, how many more bugs could we find in the amount of time it takes to run a single reduction?}

\cc{TODO: We note that we tested the wrong-code classification of CLSmith, finding that it does not affect the number of CLSmith wrong-code bugs.}

The 2357 \textbf{bf} results for CLSmith on Configs.\ $4\pm$ and $6\pm$ are all a result of compilers rejecting empty declarations, (e.g. \texttt{int;}) which CLSmith occasionally emits. This is a known issue which will likely be addressed.
% https://github.com/ChrisLidbury/CLSmith/issues/7


\subsection{Analysis of bugs found}

For all except configuration $3\pm$, CLgen exposes more compiler crashes than CLSmith. On average, CLgen hangs the compiler more frequently. We also submitted XX of the bugs found using CLSmith.


\subsection{Test Case Size}

Median CLSmith kernel is 1086 lines long (excluding headers).

For ``interesting'' programs: median CLgen kernel size is 12 lines. Median reduced CLSmith kernel size is 43 lines (894 lines before reduction). Figure~\ref{fig:kernel-sizes}.

Potentially interesting questions: What is the total lines of code which get run on each device during the tests? CLgen programs are smaller, but we get through more of them.

\cc{What is the bug rate per line of code?}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/kernel-sizes}%
	\caption{%
		Kernel line counts, grouped by classification. CLgen test cases are on average two orders of magnitude smaller than CLSmith, rendering reduction unnecessary \cc{maybe \ldots}.%
	}%
	\label{fig:kernel-sizes} %
\end{figure}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/total-tests}%
	\caption{%
		Test cases. \cc{TODO: Replot with the fastest and slowest device for each}%
	}%
	\label{fig:total-tests} %
\end{figure}