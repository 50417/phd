\section{Comparison to State-of-the-art}\label{sec:vs_clsmith}

In this section we compare our approach against CLSmith, the state-of-the-art in OpenCL fuzz testing. For each of the 10 OpenCL configurations, we run the  48 consecutive

Total runtime for a test-cases consists of the generation time, the time to execute the test harness on a configuration.

Both testing frameworks were used for 48 hours each. CLSmith was configured with default settings. CLReduce was configured with the default settings (four parallel reduction threads).

%\subsection{Experimental Setup}
%
%Each OpenCL configuration from Table~\ref{tab:platforms} was configured to run DeepSmith and CLSmith test-cases for 48 hours. This includes the time taken to generate and execute the test-case. All experimental platforms were otherwise unloaded.

\subsection{Test Outcomes}

Table~\ref{tab:outcomes} shows the outcomes of 48 hours of consecutive testing for each of the platforms from Table~\ref{tab:platforms}. We repeated the experiment twice, once with compiler optimizations disabled ($-$) for all configurations, and once with optimizations enabled ($+$).

XX\% of CLSmith test-cases fails to produce an output (non-\cmark). Proportionally, DeepSmith test-cases are less likely to produce an output, with XX\%

An average of XX CLSmith and XX DeepSmith test-cases were evaluated on each device, with an average time per test-case of XX and XX respectively. The XX-fold increase in testing throughput achieved by DeepSmith is a result of XXX. Figure~\ref{fig:runtimes} shows runtimes. For DeepSmith, generation time scales linearly with program size (benchmarked at 465 cps). We also include test harness overhead, though this could be implemented away. Disabling optimizaitons generally does not affect testing throughput significantly, with the exception of Config XX, in which the slow compilation of kernels containing structs greatly reduces the number of test-cases evaluated. This is a known issue --- in XX the authors omnit testing on this configuration for this reason.

Column \textbf{bc} is of immediate value for developers; for all other outcomes, voting heuristics are required to expose anomalous results. CLSmith crashed 8 of the 20 configurations, DeepSmith crashed all of them. See Section X for examples of bugs found from build crashes.

The integrated GPU Conifgs.\ we tested ($3\pm$) frequently failed to compile CLSmith kernels, resulting in thousands of compiler crashes \textbf{bc} or timeouts \textbf{bc}.
% SELECT stderr,COUNT(*) FROM CLSmithResults LEFT JOIN CLSmithMetas ON CLSmithResults.id=CLSmithMetas.id WHERE testbed_id=13 AND CLSmithResults.outcome='bc' AND cumtime < 48 * 3600 GROUP BY CLSmithResults.stderr;
Of the build crashes, 68\% failed silently, and the remainder were caused by the same pointer assignment assertion for which DeepSmith generated a 4 line test-case in Figure~\ref{lst:intel-llvm-assertion}. \cc{We also generated silent build crashes, but with an average line count of XX, versus CLSmith's line count of XX.}

For all except configuration $3\pm$, DeepSmith exposes more compiler crashes than CLSmith. On average, DeepSmith hangs the compiler more frequently. We also submitted XX of the bugs found using CLSmith, after reduction.

% 622 total bc
Configs.\ $4$, $5$, $6$, and $7$ have a number CLSmith bc outcomes when optimizations are enabled. Of the non-silent crashes, the cause is the OpenCL vectorizer pass, same as Figure~\ref{lst:intel-vectorizer-segfault}.

\cc{Config.\ $7+$ has a lot of \textbf{bto}s. Why?}



\subsection{Differential Tests}

Table~\ref{tab:classifications} shows the results of applying voting heuristics to the results of Table~\ref{tab:outcomes}.

\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/runtimes}%
	\caption{%
		Test-case runtimes, excluding timeouts. On average, test-case generation and execution is $2.86\times$ and $4.70\times$ faster with DeepSmith than CLSmith, respectively. \cc{reduction times}%
	}%
	\label{fig:runtimes} %
\end{figure}


\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/outcomes}
	\caption{Results from 48 hours of testing using CLSmith and DeepSmith. Configuration \#. as per Table~\ref{tab:platforms}. $\pm$ denotes optimizations off ($-$) vs on ($+$). The remaining columns denote build failure (\textbf{bf}), build crash (\textbf{bc}), build timeout (\textbf{bto}), runtime crash (\textbf{c}), timeout (\textbf{to}), and passed (\textbf{\cmark}) outcomes for CLSmith and DeepSmith, respectively.}
	\label{tab:outcomes}
\end{table*}

\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/megatable}
	\caption{Using voting heuristics to expose anomalous results from 48 hours of testing using CLSmith and DeepSmith. Columns denote wrong-code (w), build failure (\textbf{bf}), runtime crash (\textbf{c}), and timeout (\textbf{to}) classifications for CLSmith and DeepSmith, respectively.}
	\label{tab:classifications}
\end{table*}

\cc{TODO: How long do reductions take? How many DeepSmith test-cases can we run in that amount of time? Given the ratio of interesting DeepSmith test-cases, how many more bugs could we find in the amount of time it takes to run a single reduction?}

\cc{TODO: We note that we tested the wrong-code classification of CLSmith, finding that it does not affect the number of CLSmith wrong-code bugs.}

The 2357 \textbf{bf} results for CLSmith on Configs.\ $4\pm$ and $6\pm$ are all a result of compilers rejecting empty declarations, (e.g. \texttt{int;}) which CLSmith occasionally emits. This is a known CLSmith issue which will likely be addressed.
% https://github.com/ChrisLidbury/CLSmith/issues/7
Similarly, Intel Configs.\ 4--7 (but not 3) reject DeepSmith kernels which omit a type specified (e.g. \texttt{\_\_global* a}), whereas all other Configs.\ (including 3) emit and warning and default to \texttt{int} type.


Oclgrind was resilient to CLSmith testing, yet we managed to trip it up during compilation, and discovered the switch race condition.

% Anomalous build failures
Configs.\ $1\pm$, $2\pm$ reject kernels which \texttt{size\_t} parameters.


\subsection{Comparison of test-cases}

%SELECT MIN(linecount) as minlen,
%->    AVG(linecount) as meanlen,
%->        MAX(linecount) as maxlen
%-> FROM CLSmithResults results
%-> LEFT JOIN CLSmithMetas meta ON results.id = meta.id
%-> LEFT JOIN CLSmithTestCases testcases ON results.testcase_id = testcases.id
%-> LEFT JOIN CLSmithPrograms programs ON testcases.program_id = programs.id
%-> WHERE cumtime < 48 * 3600;
%+--------+-----------+--------+
%| minlen | meanlen   | maxlen |
%+--------+-----------+--------+
%|     56 | 1186.8496 |  11222 |
%+--------+-----------+--------+
The average CLSmith program is 1189 lines long (excluding headers). CLSmith kernels are unreadable, requiring automatic or manual test-case reduction.
%SELECT MIN(linecount) as minlen,
%->    AVG(linecount) as meanlen,
%->        MAX(linecount) as maxlen
%-> FROM CLgenResults results
%-> LEFT JOIN CLgenMetas meta ON results.id = meta.id
%-> LEFT JOIN CLgenTestCases testcases ON results.testcase_id = testcases.id
%-> LEFT JOIN CLgenPrograms programs ON testcases.program_id = programs.id
%-> WHERE cumtime < 48 * 3600;
%+--------+---------+--------+
%| minlen | meanlen | maxlen |
%+--------+---------+--------+
%|      1 | 20.3163 |    636 |
%+--------+---------+--------+
Average CLgen kernel is 20 lines long.
% SELECT SUM(linecount)
% FROM CLSmithResults results
% INNER JOIN CLSmithMetas meta ON results.id = meta.id
% INNER JOIN CLSmithTestCases testcases ON results.testcase_id = testcases.id
% INNER JOIN CLSmithPrograms programs on testcases.program_id = programs.id
% WHERE cumtime < 48 * 3600;
Over the course of testing, a combined $3.39 \times 10^8$ of CLSmith code was evaluated, compared to $3.76 \times 10^6$ lines of DeepSmith code.

% How many GitHub kernels contain 'struct's?
% $ ls | xargs grep -l struct -- | wc -l
% 331
CLSmith is biased towards identifying struct miscompilations, though only 7.1\% of OpenCL kernels on GitHub use them.

% Average GitHub charchounts: 1451
% Average GitHub linecounts: 53

\newsavebox{\IntelPtrAssertion}
\begin{lrbox}{\IntelPtrAssertion}
  \hspace{1.5em}
  \begin{lstlisting}
    __kernel void A(__global int* a, __global int* b) {
      int c = (int)get_global_id(0);
      a[c] += b;
    }
  \end{lstlisting}
\end{lrbox}

\newsavebox{\IntelScalarAssertion}
\begin{lrbox}{\IntelScalarAssertion}
  \hspace{1.5em}
  \begin{lstlisting}
    __kernel void A(__global float* a, __global float* b, __global float* c, __local float* d, unsigned int e, unsigned int f) {
      for (unsigned int g = get_local_id(0) + get_local_size(0); g < get_local_size(0); g += get_local_size(0)) {
        a[2 * get_local_id(0) + 1] = get_local_id(0);
      }
    }
  \end{lstlisting}
\end{lrbox}

\begin{figure}
  \centering %
  \subfloat[Configs.\ $3\pm$ assertion during code generation for pointer assignment.]{%
    \noindent\mbox{\parbox{\columnwidth}{\usebox{\IntelPtrAssertion}}}%
    \label{lst:intel-ptr-assertion}
  }\\%
  \subfloat[Configs.\ $3\pm$ assertion in scalar type code generation.]{%
    \noindent\mbox{\parbox{\columnwidth}{\usebox{\IntelScalarAssertion}}}%
    \label{lst:intel-scalar-assertion}
  }\\%
  \caption{Example kernels which crash compilers.}%
  \label{lst:compiler-crashes}%
\end{figure}

In 48 hours of testing, DeepSmith triggered 9 distinct compiler assertions, CLSmith 2. Both of the assertions triggered by CLSmith were also triggered by CLgen.
% Test-case sizes:
%
% SELECT assertion, AVG(linecount)
% FROM CLSmithResults results
% LEFT JOIN CLSmithTestCases testcases ON results.testcase_id = testcases.id
% LEFT JOIN CLSmithPrograms programs ON testcases.program_id = programs.id
% INNER JOIN CLSmithAssertions assertions ON results.stderr_id = assertions.id
% GROUP BY assertion;
%
% CLSmith:
% 'ASSERTION FAILED: (isa<AllocaInst>(ptr) || ptrCandidate.empty()) && \"storing/loading pointers only support private array\"', '1410.8486'
% 'ASSERTION FAILED: iter != pointerOrigMap.end()', '889.0990'
%
% CLgen:
% ASSERTION FAILED: (isa<AllocaInst>(ptr) || ptrCandidate.empty()) && "storing/loading pointers only support private array"	12.5382
% ASSERTION FAILED: 0	86.3295
% ASSERTION FAILED: isScalarType(type)	5.0000
% ASSERTION FAILED: iter != pointerOrigMap.end()	9.5455
% ASSERTION FAILED: Missing parameters for sync instruction	10.0000
% ASSERTION FAILED: Not implemented	7.1579
% ASSERTION FAILED: Not supported	4.7857
% ASSERTION FAILED: sel.hasDoubleType()	4.6364
% ASSERTION FAILED: srcType != ir::TYPE_U64	3.0000
The CLSmith kernels which triggered the two assertions were on average 1411 and 889 lines respectively (excluding headers). The same assertions were triggered with DeepSmith kernels of average 13 lines and 10 lines, respectively. \cc{TODO: listings}
%
% SELECT num, src, assertion
%FROM CLgenResults results
%LEFT JOIN CLgenTestCases testcases ON results.testcase_id = testcases.id
%LEFT JOIN CLgenPrograms programs ON testcases.program_id = programs.id
%LEFT JOIN Configurations ON results.testbed_id = Configurations.id
%INNER JOIN CLgenAssertions assertions ON results.stderr_id = assertions.id
%WHERE assertion = 'ASSERTION FAILED: srcType != ir::TYPE_U64';
We were able to trigger the assertion \emph{srcType != ir::TYPE\_U64} in Configs.\ $3\pm$ with only a 3 line test-case.
%
DeepSmith also triggered \emph{unreachable!} compiler errors in 180 distinct test cases, CLSmith triggered 0.

\cc{What is the bug rate per line of code?}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/kernel-sizes}%
	\caption{%
		Kernel line counts, grouped by outcome. DeepSmith test-cases are on average two orders of magnitude smaller than CLSmith.%
	}%
	\label{fig:kernel-sizes} %
\end{figure}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/total-tests}%
	\caption{%
		Test-cases. \cc{TODO: Replot with the fastest and slowest device for each}%
	}%
	\label{fig:total-tests} %
\end{figure}
