\section{Comparison to State-of-the-art}\label{sec:vs_clsmith}

In this section we compare our approach against CLSmith, the state-of-the-art in OpenCL fuzz testing. For each of the 10 OpenCL configurations, we run the  48 consecutive

Total runtime for a test cases consists of the generation time, the time to execute the test harness on a configuration.

Both testing frameworks were used for 48 hours each. CLSmith was configured with default settings. CLReduce was configured with the default settings (four parallel reduction threads).

%\subsection{Experimental Setup}
%
%Each OpenCL configuration from Table~\ref{tab:platforms} was configured to run DeepSmith and CLSmith test cases for 48 hours. This includes the time taken to generate and execute the test case. All experimental platforms were otherwise unloaded.

\subsection{Test Outcomes}

Table~\ref{tab:outcomes} shows the outcomes of 48 hours of consecutive testing for each of the platforms from Table~\ref{tab:platforms}. We repeated the experiment twice, once with compiler optimizations disabled ($-$) for all configurations, and once with optimizations enabled ($+$).

XX\% of CLSmith test cases fails to produce an output (non-\cmark). Proportionally, DeepSmith test cases are less likely to produce an output, with XX\%

An average of XX CLSmith and XX DeepSmith test cases were evaluated on each device, with an average time per test case of XX and XX respectively. The XX-fold increase in testing throughput achieved by DeepSmith is a result of XXX. Figure~\ref{fig:runtimes} shows runtimes. For DeepSmith, generation time scales linearly with program size (benchmarked at 465 cps). We also include test harness overhead, though this could be implemented away. Disabling optimizaitons generally does not affect testing throughput significantly, with the exception of Config XX, in which the slow compilation of kernels containing structs greatly reduces the number of test cases evaluated. This is a known issue --- in XX the authors omnit testing on this configuration for this reason.

Column \textbf{bc} is of immediate value for developers; for all other outcomes, voting heuristics are required to expose anomalous results. CLSmith crashed 8 of the 20 configurations, DeepSmith crashed all of them. See Section X for examples of bugs found from build crashes.

The integrated GPU Conifgs.\ we tested ($3\pm$) frequently failed to compile CLSmith kernels, resulting in thousands of compiler crashes \textbf{bc} or timeouts \textbf{bc}. 
% SELECT stderr,COUNT(*) FROM CLSmithResults LEFT JOIN CLSmithMetas ON CLSmithResults.id=CLSmithMetas.id WHERE testbed_id=13 AND CLSmithResults.outcome='bc' AND cumtime < 48 * 3600 GROUP BY CLSmithResults.stderr;
Of the build crashes, 68\% failed silently, and the remainder were caused by the same pointer assignment assertion for which DeepSmith generated a 4 line test case in Figure~\ref{lst:intel-llvm-assertion}. \cc{We also generated silent build crashes, but with an average line count of XX, versus CLSmith's line count of XX.}

For all except configuration $3\pm$, DeepSmith exposes more compiler crashes than CLSmith. On average, DeepSmith hangs the compiler more frequently. We also submitted XX of the bugs found using CLSmith, after reduction.

% 622 total bc
Configs.\ $4$, $5$, $6$, and $7$ have a number CLSmith bc outcomes when optimizations are enabled. Of the non-silent crashes, the cause is the OpenCL vectorizer pass, same as Figure~\ref{lst:intel-vectorizer-segfault}.

\cc{Config.\ $7+$ has a lot of \textbf{bto}s. Why?}



\subsection{Differential Tests}

Table~\ref{tab:classifications} shows the results of applying voting heuristics to the results of Table~\ref{tab:outcomes}.

\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/runtimes}%
	\caption{%
		Test case runtimes, excluding timeouts. On average, test case generation and execution is $2.86\times$ and $4.70\times$ faster with DeepSmith than CLSmith, respectively. \cc{reduction times}%
	}%
	\label{fig:runtimes} %
\end{figure}


\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/outcomes}
	\caption{Results from 48 hours of testing using CLSmith and DeepSmith. Configuration \#. as per Table~\ref{tab:platforms}. $\pm$ denotes optimizations off ($-$) vs on ($+$). The remaining columns denote build failure (\textbf{bf}), build crash (\textbf{bc}), build timeout (\textbf{bto}), runtime crash (\textbf{c}), timeout (\textbf{to}), and passed (\textbf{\cmark}) outcomes for CLSmith and DeepSmith, respectively.}
	\label{tab:outcomes}
\end{table*}

\begin{table*}
	\scriptsize %
	\centering %
	\input{build/tab/megatable}
	\caption{Using voting heuristics to expose anomalous results from 48 hours of testing using CLSmith and DeepSmith. Columns denote wrong-code (w), build failure (\textbf{bf}), runtime crash (\textbf{c}), and timeout (\textbf{to}) classifications for CLSmith and DeepSmith, respectively.}
	\label{tab:classifications}
\end{table*}

\cc{TODO: How long do reductions take? How many DeepSmith test cases can we run in that amount of time? Given the ratio of interesting DeepSmith test cases, how many more bugs could we find in the amount of time it takes to run a single reduction?}

\cc{TODO: We note that we tested the wrong-code classification of CLSmith, finding that it does not affect the number of CLSmith wrong-code bugs.}

The 2357 \textbf{bf} results for CLSmith on Configs.\ $4\pm$ and $6\pm$ are all a result of compilers rejecting empty declarations, (e.g. \texttt{int;}) which CLSmith occasionally emits. This is a known CLSmith issue which will likely be addressed.
% https://github.com/ChrisLidbury/CLSmith/issues/7
Similarly, Intel Configs.\ 4--7 (but not 3) reject DeepSmith kernels which omit a type specified (e.g. \texttt{\_\_global* a}), whereas all other Configs.\ (including 3) emit and warning and default to \texttt{int} type.


Oclgrind was resilient to CLSmith testing, yet we managed to trip it up during compilation, and discovered the switch race condition.

% Anomalous build failures
Configs.\ $1\pm$, $2\pm$ reject kernels which \texttt{size\_t} parameters.


\subsection{Comparison of test cases}

Average CLSmith kernel is 1200 lines long (excluding headers). CLSmith kernels are unreadable, requiring automatic or manual test case reduction. Average CLgen kernel is 19 lines long.


Potentially interesting questions: What is the total lines of code which get run on each device during the tests? DeepSmith programs are smaller, but we get through more of them.

In 48 hours of testing, DeepSmith triggered 9 distinct compiler assertions, CLSmith 2. Both of the assertions triggered by CLSmith were also triggered by CLgen.
% Test case sizes:
% 
% SELECT assertion, AVG(linecount)
% FROM CLSmithResults results
% LEFT JOIN CLSmithTestCases testcases ON results.testcase_id = testcases.id
% LEFT JOIN CLSmithPrograms programs ON testcases.program_id = programs.id
% INNER JOIN CLSmithAssertions assertions ON results.stderr_id = assertions.id
% GROUP BY assertion;
%
% CLSmith:
% 'ASSERTION FAILED: (isa<AllocaInst>(ptr) || ptrCandidate.empty()) && \"storing/loading pointers only support private array\"', '1410.8486'
% 'ASSERTION FAILED: iter != pointerOrigMap.end()', '889.0990'
%
% CLgen:
% ASSERTION FAILED: (isa<AllocaInst>(ptr) || ptrCandidate.empty()) && "storing/loading pointers only support private array"	12.5382
% ASSERTION FAILED: 0	86.3295
% ASSERTION FAILED: isScalarType(type)	5.0000
% ASSERTION FAILED: iter != pointerOrigMap.end()	9.5455
% ASSERTION FAILED: Missing parameters for sync instruction	10.0000
% ASSERTION FAILED: Not implemented	7.1579
% ASSERTION FAILED: Not supported	4.7857
% ASSERTION FAILED: sel.hasDoubleType()	4.6364
% ASSERTION FAILED: srcType != ir::TYPE_U64	3.0000
The CLSmith kernels which triggered the two assertions were on average 1411 and 889 lines respectively (excluding headers). The same assertions were triggered with DeepSmith kernels of average 13 lines and 10 lines, respectively.
%
% SELECT num, src, assertion
%FROM CLgenResults results
%LEFT JOIN CLgenTestCases testcases ON results.testcase_id = testcases.id
%LEFT JOIN CLgenPrograms programs ON testcases.program_id = programs.id
%LEFT JOIN Configurations ON results.testbed_id = Configurations.id
%INNER JOIN CLgenAssertions assertions ON results.stderr_id = assertions.id
%WHERE assertion = 'ASSERTION FAILED: srcType != ir::TYPE_U64';
We were able to trigger the assertion \emph{srcType != ir::TYPE\_U64} in Configs.\ $3\pm$ with only a 3 line test case.
%
DeepSmith also triggered 227 \emph{unreachable!} compiler errors, CLSmith triggered 0.

\cc{What is the bug rate per line of code?}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/kernel-sizes}%
	\caption{%
		Kernel line counts, grouped by classification. DeepSmith test cases are on average two orders of magnitude smaller than CLSmith, rendering reduction unnecessary \cc{TODO: plot outcomes, not classifications. Maybe file size rather than line count? \ldots}.%
	}%
	\label{fig:kernel-sizes} %
\end{figure}


\begin{figure}
	\centering %
	\includegraphics[width=\columnwidth]{build/img/total-tests}%
	\caption{%
		Test cases. \cc{TODO: Replot with the fastest and slowest device for each}%
	}%
	\label{fig:total-tests} %
\end{figure}