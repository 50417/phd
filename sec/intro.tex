\section{Introduction}\label{sec:intro}

\noindent
Compilers should produce correct code for valid inputs, and meaningful errors for invalid inputs. Properly testing compilers is hard --- modern optimizing compilers are large and complex programs, and their input space is huge. Hand designed suites of test programs, while important, are inadequate for covering such a large space and will not touch all parts of the compiler.

% Compilers are a fundamental, trusted technology. Bugs in them are particularly harmful. They can introduce extremely hard to identify errors in the generated binary, and are often very hard for developers to work around, particularly if their code must be portable. 

Random test case generation --- \emph{fuzzing} --- is a well established and effective method for identifying compiler bugs~\cite{Kossatchev2005}. When fuzzing, randomly generated valid or semi-valid inputs are fed to the compiler. Any kind of unexpected behavior, including crashes, freezes, or wrong binaries, indicates a compiler bug. While crashes and freezes in the compiler are easy to detect, determining that binaries are correctly compiled is not generally possible without either developer provided validation for the particular program's behavior or a gold standard compiler from which to create reference outputs. In the absence of those, Differential Testing~\cite{McKeeman1998} can be used. The generated code and a set of inputs form a \emph{test case} which is compiled and executed on multiple \emph{testbeds}. If the test case should have deterministic behavior, but the output differs between testbeds, then a bug has been discovered.

While CSmith~\cite{Yang2011} has been successfully used to identify hundreds of bugs in compilers, it and similar approaches have a significant drawback. They represent a huge undertaking and require a thorough understanding of the target programming language. CSmith was developed over the course of years, and consists of over 41k lines of handwritten C++ code. By tightly coupling the generation logic with the target programming language, each feature of the grammar must be painstakingly and expertly engineered for each new target language. For example, lifting CSmith from C to OpenCL~\cite{Lidbury2015a} --- a superficially simple task --- took 9 months and an additional 8k lines of code. Given the difficulty of defining a new grammar, typically only a subset of the language is implemented.

We propose a fast, effective, and low effort approach to the generation of random programs for compiler fuzzing. Our methodology uses recent advances in deep learning to automatically construct probabilistic models of how humans write code, instead of painstakingly defining a grammar to the same end. By training a deep neural network on a corpus of handwritten code, it is able to infer both the syntax and semantics of the programming language and the common constructs and patterns. Our approach essentially frames the generation of random programs as a language modeling problem. This greatly simplifies and accelerates the process. The expressiveness of the generated programs is limited only by what is contained in the corpus, not the developer's expertise or available time. Such a corpus can readily be assembled from open source repositories. We make the following contributions:
%
\begin{itemize}
\item a novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. We \emph{infer} programming language syntax, structure, and use from real-world examples, not through an expert-defined grammar. Our system needs two orders of magnitude less code than the state-of-the-art, and takes less than a day to train; % CLSmith requires expert-driven development for every language feature supported. 40+k lines of C++ vs a few hundred lines of Python.

\item we discover a similar number of bugs as the state-of-the-art, but also find bugs which prior work cannot, covering more components of the compiler;
% e.g. ill-formed

\item in modeling real handwritten code, our test cases are more interpretable than other approaches. Average test case size is two orders of magnitude smaller than state-of-the-art, without any expensive reduction process.
\end{itemize}
