\section{Overview of Our Approach and Results}\label{sec:overview}

\cc{DELETE ME, merge into other sections}

\paragraph{Test case generation} We mine over a million lines of handwritten OpenCL code from GitHub, which we use to construct a corpus of representative programs. We learn a generative model over this corpus, using syntactic level language modeling. DeepSmith, our deep learning program generator, automatically and rapidly synthesizes an unbounded number of programs for fuzz testing compilers.

\paragraph{OpenCL testbeds} We conducted testing of 10 OpenCL testbeds, summarized in Table~\ref{tab:platforms}. Each testbed consists of a \emph{<device, driver>} pair. We covered the broadest range of hardware available to us: 3 GPUs, 4 CPUs, a co-processor, and an emulator. 7 of the compilers tested are commercial products, 3 of them are open source. Our suite of testbeds includes both combinations of different drivers for the same device, and different devices using the same driver.


\begin{table*}[t!]
	\scriptsize %
	\centering %
	% \rowcolors{2}{white}{gray!25}
	\input{build/tab/platforms}
	\caption{OpenCL testbeds, the time spent in automated testing, and the number of bug reports submitted to date.}
	\label{tab:platforms}
\end{table*}


\paragraph{Bugs found} All compilers yielded bugs. Every compiler crashed, and every compiler yielded anomalous results --- either programs which crash, or programs which silently compute the wrong result. To date, we have submitted XX bug reports to compiler vendors. Of those, 41\% are compiler crashes, XX\% are compiler hangs, and the remainder are cases where the generated program either silently emits wrong-code, or crashes at runtime. \cc{We also test the Clang frontend of every LLVM release in the past 24 months\ldots}