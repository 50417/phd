\section{Related Work}\label{sec:rw}

% \subsection{Test Case Generation}

The \emph{fuzz taming problem} is introduced in~\cite{Chen2013}. Compiler fuzz testers are indiscriminate - they tend to keep finding noncritical bugs which may already be known.

\cc{TODO:} Compiler test-case generation survey~\cite{Boujarwah1997}. Empirical comparion of compiler testing techniques~\cite{Chen2014a}. Testing using programs at different levels of quality (random character sequence through model-conforming program)~\cite{McKeeman1998}. Analysis of bugs in GCC and LLVM finds 80\% of test-cases to be 45 lines~\cite{Sun2016}. \cc{TODO}~\cite{Godefroid2008a,Le2015,Sun2016a}.

\citeauthor{McKeeman1998} use a lexeme generator to test C compilers by enumerating token sequences in~\cite{McKeeman1998}. Basically the same as DeepSmith, except we weight the generation so that sequences are plausible.

% \paragraph{SPE}
Skeletal program enumeration~\cite{Zhang2016a}. By enumerating entire program space, provides bounded guarantees of compiler correctness. Same shortcoming as CSmith (well formed programs only). No OpenCL implementation. Probably would take a lot of development effort. \cc{More investigation required.}

% \paragraph{CSmith}
Grammar-based \emph{fuzz testers} have been developed for C~\cite{Yang2011} and OpenCL~\cite{Lidbury2015a}. Both tools require test-case reduction (\cite{Regehr2012a} and~\cite{Pflanzer2016}, respectively).

Csmith defines a probabilistic grammar over a large subset of the C programming language, and recursively instantiates a program, using rigorous and conservative static and dynamic analysis to ensure that programs are guaranteed to be free from the 190 undefined behaviors of the C language specification.

Because Csmith programs are free form undefined behavior, there is only a single interpretation. This allows oracle-less differential testing across compilers, using a voting heuristic to identify erroneous compiler outputs.

Csmith does not guarantee program termination. They used a timeout to catch non-termination, finding that about 10\% of Csmith generated programs do not terminate.

They identify two classes of bugs: wrong-code bugs, which are programs which are silently miscompiled so as to compute the wrong result; and compiler-crash bugs, which cause the compiler to fail, either by segfault, timeout, internal assertion failure, etc.

We do not believe that the guarantee of undefined behavior is an important factor for identifying compiler-crash bugs. In fact, it is not even necessarily important that programs be well formed. Quoting: ``As a matter of implementation quality, a compiler vendor will usually fix a segmentation fault or similar problem even if the crash-inducing test-case, for example, uses a variable without initialization.''~\cite{Regehr2012a}.

Csmith, and all other random program generation tools, suffer from the problem that they cannot provide any measure of ``interestingness'' of their test-cases. The best they can do is indirect confirmation, by showing that a number of their reported bugs have subsequently been fixed or marked with a high bug priority. We hypothesize that the bugs identified by modeling real handwritten source codes would be more likely to be of a high priority, since they stem from more likely programs. This is in stark contrast to Csmith, which by design attempts to explore the space of \emph{unlikely} programs, by pairing infrequently combined language features.

The ``shape'' of programs generated by Csmith is expert driven. The shape of programs generated by DeepSmith is data driven. The 80 probabilities which control Csmith program generation are extensively hand tuned to produce programs which ``look right''. Our data driven approach does not require this. In fact, our approach is portable across changing usage of a programming language.

The functionality of Csmith is also expert driven. Every language feature supported by Csmith must be laboriously hand crafted, and results in a very complex system of over 40,000 lines of C++ code (which still omits many language features used in real programs, like heap allocation). The language features supported by DeepSmith is bound only by those which have been used on GitHub. DeepSmith assigns a non-zero probability of generating programs using every language feature found on GitHub. We reason that if a language feature remains unused in the entirety of code on GitHub, then it is reasonable to assume that it is not a feature worth testing. DeepSmith is much simpler than Csmith, at only 1,000 lines of high level Python, of which only around 400 lines is required to describe the core program generator. In fact, in extending DeepSmith for compiler testing, we actually reduced the size of the software.

Csmith makes the claim that larger programs are more useful for differential testing. In their experiments, they found the most distinct crash errors in programs containing 8K-16K tokens (81KB file size). We do not believe this to be the case. Subsequent work in test-case reduction~\cite{Regehr2012a} reduced Csmith program sizes by 74--594$\times$ while preserving the bug exposing behavior. They median reduced Csmith program size they found was only 20 lines (0.5KB). We hypothesize that it is not the size of programs which is responsible for identifying the most bugs, but simply the total size of all programs tested. That is, the rate of bugs is proportional to the total lines of code tested, not the lines of code in each test.

CLsmith extends Csmith to the OpenCL programming language. OpenCL has a number of properties which make compiler testing increasingly significant: first, OpenCL compilers are aggressively optimizing in oder to exploit heterogeneous performance. This makes the identification of wrong-code bugs significant. Secondly, OpenCL compilers typically execute online, meaning that in shipped products which use OpenCL, compiler crash errors may be delivered to the customer, rather than caught by the developers themselves. In cases where the compiler crash causes the host operating system to also crash (we experienced this problem with XX drivers), this is an obvious, critical problem. Testing the portability of code should not be the burden of the application developer.

The CLsmith experimental setup is broad but shallow. They test a large number of device, compiler configurations (21 in total); but not different compilers on the same device. This makes it hard to classify bugs as belonging to the compiler front/middle-end, or the device-specific code generation.

Both Csmith and CLsmith use unrealistic safe-math macros to wrap arithmetic operations. We do not.

CLsmith makes it impossible for thread ID to modify control flow. However, in almost all real world codes, this is not the case. This is a big restriction.

% \paragraph{DeepSmith}
DeepSmith extends prior work in program synthesis for performance benchmarking~\cite{Cummins2017a}. LLVM is used as an oracle to determine if program is well formed. Behaviour is validated using runtime checks, with no desire for correctness. We extend this \cc{\ldots}. Syntactic level modeling has been shown useful for identifying optimization characteristics~\cite{Cummins2017b}.

% \paragraph{Misc}
A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}.

Template-based program generation for performance tuning~\cite{Han2017}.

% \subsection{ML over programs} 

Deep Learning is a nascent field that is responsible for a multitude of breakthroughs in modeling rich, hierarchical datasets. The major milestones are reviewed in~\cite{Wang2017}, and methods in~\cite{Schmidhuber2014}.

% \paragraph{Grammar Variational Autoencoder}
Grammar variational autoencoder - Using masking to ensure sampling only from valid production rules. Both the grammars and the generated sequences are small, it's unclear if it would scale. For arithmetic expressions, the grammar includes not just the structure of equations, but it enumerates all the valid inteegr values. \cc{TODO: study, write up}~\cite{Kusner2017}.

% \paragraph{ILP}
Goal-directed program generators have been used for a variety of domains, including generating linear transforms~\cite{Voronenko2009}, MapReduce programs~\cite{Smith}, and data structure implementations~\cite{Loncaric2016}. Program synthesis from input/output examples is used for simple algorithms in~\cite{Zaremba2015a}, string manipulation in~\cite{Gulwani2011}, and geometry constructions in~\cite{Gulwani2012}.

% \paragraph{Software Testing}
Attention network to identify buffer overruns~\cite{Choi2016}. Localizing bugs from bug reports~\cite{Lam2016,Huo2016}.

TODO~\cite{White2016}.

% \paragraph{Software Engineering}
There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyze software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarization~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. However, no work so far has exploited mined source code for test-case generation. This work is the first to do so.
