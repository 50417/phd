\section{Related Work}\label{sec:rw}

\paragraph{Test Case Generation} 
Testing how compilers operate under unexpected or invalid input is an established practice, aiming at uncovering bugs in the compiler~\cite{Boujarwah1997}. The main question is of how to efficiently generate code for triggering such bugs. The idea of automatically generating test cases for compiler fuzzing was introduced by~\cite{McKeeman1998}, where the author designed a lexeme generator to create test cases failing at different stages of compilation.

CSmith~\cite{Yang2011} and CLSmith~\cite{Lidbury2015a} improved on this with generators pairing infrequently combined language features. They produce correct programs with clearly defined behavior but very unlikely functionality, increasing the chances of triggering a bug. To achieve this, CSmith required extensive engineering work, most of it not portable across languages, and ignored language features, like heap allocation, which can easily affect correctness. Since their search space is unbounded, CSmith and CLSmith programs tend to be too long and require test case reduction~\cite{Regehr2012a,Pflanzer2016} to bring their length down to manageable lengths.

EMI testing~\cite{Le2013a,Le2015} follows a different approach. Starting with existing code, it inserts or deletes statements that will not be executed, so functionality should remain the same. If it is affected, it is due to a compiler bug. While a powerful technique able to find hard to detect bugs, it relies on having a very large number of programs to mutate. As such, it is not a replacement for random code generation but an orthogonal approach. Similarly to CSmith, EMI tends to produce too long test programs. LangFuzz also uses mutation but does by inserting code segments which have previously exposed bugs. This increases the chances of discovering vulnerabilities in scripting language engines.

Skeletal program enumeration~\cite{Zhang2016a} again works by transforming existing code. It identifies algorithmic patterns in short pieces of code and enumerates all the possible variations of the same pattern. Compared to EMI and CSmith, it has the benefit of producing much shorter test cases. \pp{Something bad? It should be rather easy to port btw and does not require well formed programs.}




%CSmith~\cite{Yang2011} and CLSmith~\cite{Lidbury2015a}. Same testing methodology, but very different design goals to our work. The explore the space of \emph{unlikely} programs, by pairing infrequently combined language features. Because Csmith programs are free form undefined behavior, there is only a single interpretation. This allows oracle-less differential testing across compilers, using a voting heuristic to identify erroneous compiler outputs. The ``shape'' of programs generated by Csmith is expert driven. The shape of programs generated by DeepSmith is data driven. The 80 probabilities which control Csmith program generation are extensively hand tuned to produce programs which ``look right''. Our data driven approach does not require this. In fact, our approach is portable across changing usage of a programming language.

%The functionality of Csmith is also expert driven. Every language feature supported by Csmith must be laboriously hand crafted, and results in a very complex system of over 40,000 lines of C++ code (which still omits many language features used in real programs, like heap allocation). The language features supported by DeepSmith is bound only by those which have been used on GitHub. DeepSmith has a non-zero probability of generating programs using every language feature found on GitHub. We reason that if a language feature remains unused in the entirety of code on GitHub, then it is reasonable to assume that it is not a feature worth testing. Both Csmith and CLsmith use unrealistic safe-math macros to wrap arithmetic operations. We do not.


% Increasing complexity of generations: RandProg to CSmith. Then EMI testing, and SPE.

The \emph{fuzz taming problem} is addressed in~\cite{Chen2013}, in which a distance metric is used to rank test cases such that diverse, interesting test cases are highly ranked.

%\emph{LangFuzz} attempts a similar approach of learning to generate test cases from real code~\cite{Holler2012}. For LangFuzz, the input programs are codes known to have previously exposed bugs.

%Empirical comparison of compiler testing techniques~\cite{Chen2014a}

%\cc{is it wise to draw attention to this?} If the output of a program depends on undefined or unspecified behavior, then comparing outputs across compilers is meaningless. Prior works have formulated random program generation so as to minimize or eradicate the possibility of undefined and unspecified behavior~\cite{Yang2011c,Le2013a,Le2015}, at the expense of expressiveness. Recently, work in Skeletal Program Enumeration~\cite{Zhang2017a} has relied on handchecking of C programs and tools such as CompCert~\cite{Leroy2013} and UBsan to detect false-positives. Since DeepSmith programs are not guaranteed to be free from undefined behaviors, and we can only determine if a program is well-formed empirically, additional filtering of test cases is required to prevent false-positives.

Analysis of bugs in GCC and LLVM finds 80\% of test cases to be 45 lines~\cite{Sun2016}.

\cc{TODO}~\cite{Godefroid2008a,Le2015,Sun2016a}~\cite{Kossatchev2005}.

%\cc{TODO:} Directed EMI testing~\cite{Le2015}.

\cc{TODO: finding arithmetic bugs which CSmith cannot~\cite{Nagai2013}}

``As a matter of implementation quality, a compiler vendor will usually fix a segmentation fault or similar problem even if the crash-inducing test case, for example, uses a variable without initialization.''~\cite{Regehr2012a}

%Skeletal program enumeration~\cite{Zhang2016a}. By enumerating entire program space, provides bounded guarantees of compiler correctness. Same shortcoming as CSmith (well formed programs only). No OpenCL implementation. Probably would take a lot of development effort. \cc{More investigation required.}

%Both tools require test case reduction (\cite{Regehr2012a} and~\cite{Pflanzer2016}, respectively). Work in test case reduction~\cite{Regehr2012a} reduced Csmith program sizes by 74--594$\times$ while preserving the bug exposing behavior. They median reduced CSmith program size they found was only 20 lines (0.5KB).

\cc{TODO:}

%EMI testing~\cite{Le2013a}, Skeletal Program enumeration~\cite{Zhang2017a}.

%Fuzzing with code fragments~\cite{Holler2012}.

%A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}.

TODO~\cite{White2016},~\cite{Sheridan2007}.

Grammar-base whitebox testing~\cite{Godefroid2008a}.

Our work is the first to address the challenge of generating human-like, plausible random programs.

\paragraph{GPU Testing} GPU Concurrency --- Small \emph{litmus tests}~\cite{Alglave2015}.

GPUVerify~\cite{Bardsley2014}.


\paragraph{Machine Learning} Deep Learning is a nascent field that is responsible for a multitude of breakthroughs in modeling rich, hierarchical datasets. The major milestones are reviewed in~\cite{Wang2017}, and methods in~\cite{Schmidhuber2014}.

MACHINE LEARNING FOR TESTING: Machine learning selectively unsound static analysis~\cite{Heo2017}, Learning a classifier for static analyzers~\cite{Koc2017}.
Transforming program repair ingredients with DL~\cite{White}. Program repair~\cite{Koukoutos2017a}. Learning to prioritize test programs~\cite{Chen2017}. Attention network to identify buffer overruns~\cite{Choi2016}. Localizing bugs from bug reports~\cite{Lam2016,Huo2016}.

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyze software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarization~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. Previous applications of deep learning to compilers have involved program synthesis for performance benchmarking~\cite{Cummins2017a} and building optimization heuristics~\cite{Cummins2017b}. No work so far has exploited mined source code for test case generation. This work is the first to do so.
