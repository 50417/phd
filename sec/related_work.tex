\section{Related Work}\label{sec:rw}

\paragraph{Test Case Generation} 
Testing how compilers operate under unexpected or invalid input is an established practice, aiming at uncovering bugs in the compiler~\cite{Boujarwah1997}. The main question is of how to efficiently generate code for triggering such bugs. The idea of automatically generating test cases for compiler fuzzing and using differential testing on them was introduced by~\cite{McKeeman1998}, where the author designed a lexeme generator to create test cases failing at different stages of compilation. CSmith~\cite{Yang2011} and CLSmith~\cite{Lidbury2015a} improved on this with generators pairing infrequently combined language features. They produce correct programs with clearly defined behavior but very unlikely functionality, increasing the chances of triggering a bug. To achieve this, CSmith required extensive engineering work, most of it not portable across languages, and ignored some language features. Subsequent generators influenced by CSmith, like Orange3~\cite{Nagai2013}, focus on features and bug types beyond the scope of CSmith, arithmetic bugs in the case of Orange3.
%HJL - it did say "and ignored language features, like heap allocation, which can easily affect correctness." But, heap allocation in C is not a language feature. Also, not clear that we could handle that type of thing anyway.

Equivalence Modulo Inputs (EMI) testing~\cite{Le2013a,Le2015,Sun2016a} follows a different approach. Starting with existing code, it inserts or deletes statements that will not be executed, so functionality should remain the same. If it is affected, it is due to a compiler bug. While a powerful technique able to find hard to detect bugs, it relies on having a very large number of programs to mutate. As such, it still requires an external code generator. Similarly to CSmith, EMI tends to produce very long test programs. LangFuzz~\cite{Holler2012} also uses mutation but does this by inserting code segments which have previously exposed bugs. This increases the chances of discovering vulnerabilities in scripting language engines. Skeletal program enumeration~\cite{Zhang2017a} again works by transforming existing code. It identifies algorithmic patterns in short pieces of code and enumerates all the possible variations of the same pattern. 
%HJL - Just removing: Compared to EMI and CSmith, it has the benefit of producing much shorter test cases. \pp{Something bad? It should be rather easy to port btw and does not require well formed programs.}

With both EMI and CSmith, generated programs can be unnecessarily long. While 80\% of the test cases related to GCC and LLVM bugs are 45 lines~\cite{Sun2016} or less, CSmith/EMI output tends to be thousands of lines long. Most this code is not related to the bug and has to be removed for human compiler engineers to understand the source of the problem. To that end test case reduction techniques~\cite{Regehr2012a,Pflanzer2016} have been used to bring the code down to manageable lengths. These techniques are very slow, potentially taking several hours to reduce a single test case. Another common problem is prioritizing the most bugs out of the hundreds or thousands discovered. This \emph{fuzz taming problem} is addressed in~\cite{Chen2013}, in which a distance metric is used to rank test cases such that diverse, interesting test cases are highly ranked.

Compared to all these, our fuzzing approach is low cost, easy to develop, portable, capable of detecting a wide range of errors, and focusing by design on bugs that are more likely to be encountered in a production scenario.


%CSmith~\cite{Yang2011} and CLSmith~\cite{Lidbury2015a}. Same testing methodology, but very different design goals to our work. The explore the space of \emph{unlikely} programs, by pairing infrequently combined language features. Because Csmith programs are free form undefined behavior, there is only a single interpretation. This allows oracle-less differential testing across compilers, using a voting heuristic to identify erroneous compiler outputs. The ``shape'' of programs generated by Csmith is expert driven. The shape of programs generated by DeepSmith is data driven. The 80 probabilities which control Csmith program generation are extensively hand tuned to produce programs which ``look right''. Our data driven approach does not require this. In fact, our approach is portable across changing usage of a programming language.

%The functionality of Csmith is also expert driven. Every language feature supported by Csmith must be laboriously hand crafted, and results in a very complex system of over 40,000 lines of C++ code (which still omits many language features used in real programs, like heap allocation). The language features supported by DeepSmith is bound only by those which have been used on GitHub. DeepSmith has a non-zero probability of generating programs using every language feature found on GitHub. We reason that if a language feature remains unused in the entirety of code on GitHub, then it is reasonable to assume that it is not a feature worth testing. Both Csmith and CLsmith use unrealistic safe-math macros to wrap arithmetic operations. We do not.


% Increasing complexity of generations: RandProg to CSmith. Then EMI testing, and SPE.

%The \emph{fuzz taming problem} is addressed in~\cite{Chen2013}, in which a distance metric is used to rank test cases such that diverse, interesting test cases are highly ranked.

%\emph{LangFuzz} attempts a similar approach of learning to generate test cases from real code~\cite{Holler2012}. For LangFuzz, the input programs are codes known to have previously exposed bugs.

%Empirical comparison of compiler testing techniques~\cite{Chen2014a}

%\cc{is it wise to draw attention to this?} If the output of a program depends on undefined or unspecified behavior, then comparing outputs across compilers is meaningless. Prior works have formulated random program generation so as to minimize or eradicate the possibility of undefined and unspecified behavior~\cite{Yang2011c,Le2013a,Le2015}, at the expense of expressiveness. Recently, work in Skeletal Program Enumeration~\cite{Zhang2017a} has relied on handchecking of C programs and tools such as CompCert~\cite{Leroy2013} and UBsan to detect false-positives. Since DeepSmith programs are not guaranteed to be free from undefined behaviors, and we can only determine if a program is well-formed empirically, additional filtering of test cases is required to prevent false-positives.

%Analysis of bugs in GCC and LLVM finds 80\% of test cases to be 45 lines~\cite{Sun2016}.

%\cc{TODO}~\cite{Godefroid2008a,Le2015,Sun2016a}~\cite{Kossatchev2005}.

%\cc{TODO:} Directed EMI testing~\cite{Le2015}.

%\cc{TODO: finding arithmetic bugs which CSmith cannot~\cite{Nagai2013}}

%``As a matter of implementation quality, a compiler vendor will usually fix a segmentation fault or similar problem even if the crash-inducing test case, for example, uses a variable without initialization.''~\cite{Regehr2012a}

%Skeletal program enumeration~\cite{Zhang2016a}. By enumerating entire program space, provides bounded guarantees of compiler correctness. Same shortcoming as CSmith (well formed programs only). No OpenCL implementation. Probably would take a lot of development effort. \cc{More investigation required.}

%Both tools require test case reduction (\cite{Regehr2012a} and~\cite{Pflanzer2016}, respectively). Work in test case reduction~\cite{Regehr2012a} reduced Csmith program sizes by 74--594$\times$ while preserving the bug exposing behavior. They median reduced CSmith program size they found was only 20 lines (0.5KB).

%\cc{TODO:}

%EMI testing~\cite{Le2013a}, Skeletal Program enumeration~\cite{Zhang2017a}.

%Fuzzing with code fragments~\cite{Holler2012}.

%A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}.

%TODO~\cite{White2016},~\cite{Sheridan2007}.

%Grammar-base whitebox testing~\cite{Godefroid2008a}.

%Our unique contribution is a low cost method for fuzzing, capable of detecting a wide range of errors, and matching that of human expectation (small, plausible test cases).


%HJL - Removing - \paragraph{GPU Testing} \pp{Do we really need this?} GPU Concurrency --- Small \emph{litmus tests}~\cite{Alglave2015}.
%GPUVerify~\cite{Bardsley2014}.


\paragraph{Deep Learning}
Deep Learning is a nascent field that is responsible for a multitude of breakthroughs in modeling rich, hierarchical datasets. The major milestones are reviewed in~\cite{Wang2017}, and methods in~\cite{Schmidhuber2014}. In software testing,
machine learning has been successfully applied before on areas such as improving bug finding static analyzers~\cite{Heo2017,Koc2017}, repairing programs~\cite{Koukoutos2017a,White}, prioritizing test programs~\cite{Chen2017}, identifying buffer overruns~\cite{Choi2016}, and processing bug reports~\cite{Lam2016,Huo2016}.

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyze software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarization~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. Previous applications of deep learning to compilers have involved program synthesis for performance benchmarking~\cite{Cummins2017a} and building optimization heuristics~\cite{Cummins2017b}. No work so far has exploited mined source code for test case generation. Our work is the first to do so.
