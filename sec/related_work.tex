\section{Related Work}\label{sec:rw}

Compiler test-case generation survey~\cite{Boujarwah1997}. Formative work in '98~\cite{McKeeman1998}, which includes testing with random ASCII strings. \citeauthor{McKeeman1998} use a lexeme generator to test C compilers by enumerating token sequences in~\cite{McKeeman1998}. Basically the same as DeepSmith, except we weight the generation so that sequences are plausible.

Increasing complexity of generations: RandProg to CSmith. Then EMI testing, and SPE.

The \emph{fuzz taming problem} is introduced in~\cite{Chen2013}. Compiler fuzz testers are indiscriminate - they tend to keep finding noncritical bugs which may already be known.

Empirical comparion of compiler testing techniques~\cite{Chen2014a}

Analysis of bugs in GCC and LLVM finds 80\% of test-cases to be 45 lines~\cite{Sun2016}.

\cc{TODO}~\cite{Godefroid2008a,Le2015,Sun2016a}.

\cc{TODO:} Directed EMI testing~\cite{Le2015}.

``As a matter of implementation quality, a compiler vendor will usually fix a segmentation fault or similar problem even if the crash-inducing test-case, for example, uses a variable without initialization.''~\cite{Regehr2012a}

Skeletal program enumeration~\cite{Zhang2016a}. By enumerating entire program space, provides bounded guarantees of compiler correctness. Same shortcoming as CSmith (well formed programs only). No OpenCL implementation. Probably would take a lot of development effort. \cc{More investigation required.}

CSmith~\cite{Yang2011} and CLSmith~\cite{Lidbury2015a}. Same testing methodology, but very different design goals to our work. The explore the space of \emph{unlikely} programs, by pairing infrequently combined language features. Because Csmith programs are free form undefined behavior, there is only a single interpretation. This allows oracle-less differential testing across compilers, using a voting heuristic to identify erroneous compiler outputs. The ``shape'' of programs generated by Csmith is expert driven. The shape of programs generated by DeepSmith is data driven. The 80 probabilities which control Csmith program generation are extensively hand tuned to produce programs which ``look right''. Our data driven approach does not require this. In fact, our approach is portable across changing usage of a programming language.

The functionality of Csmith is also expert driven. Every language feature supported by Csmith must be laboriously hand crafted, and results in a very complex system of over 40,000 lines of C++ code (which still omits many language features used in real programs, like heap allocation). The language features supported by DeepSmith is bound only by those which have been used on GitHub. DeepSmith has a non-zero probability of generating programs using every language feature found on GitHub. We reason that if a language feature remains unused in the entirety of code on GitHub, then it is reasonable to assume that it is not a feature worth testing. Both Csmith and CLsmith use unrealistic safe-math macros to wrap arithmetic operations. We do not.

Both tools require test-case reduction (\cite{Regehr2012a} and~\cite{Pflanzer2016}, respectively). Work in test-case reduction~\cite{Regehr2012a} reduced Csmith program sizes by 74--594$\times$ while preserving the bug exposing behavior. They median reduced CSmith program size they found was only 20 lines (0.5KB).

\cc{TODO:}

GPU Concurrency --- Small \emph{litmus tests}~\cite{Alglave2015}.

A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}.

TODO~\cite{White2016}.

MACHINE LEARNING FOR TESTING: Machine learning selectively unsound static analysis~\cite{Heo2017}, Learning a classifier for static analyzers~\cite{Koc2017}.
Transforming program repair ingredients with DL~\cite{White}. Program repair~\cite{Koukoutos2017a}. Learning to prioritize test programs~\cite{Chen2017}. Attention network to identify buffer overruns~\cite{Choi2016}. Localizing bugs from bug reports~\cite{Lam2016,Huo2016}.

Deep Learning is a nascent field that is responsible for a multitude of breakthroughs in modeling rich, hierarchical datasets. The major milestones are reviewed in~\cite{Wang2017}, and methods in~\cite{Schmidhuber2014}.

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyze software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarization~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. Previous applications of deep learning to compilers have involved program synthesis for performance benchmarking~\cite{Cummins2017a} and building optimization heuristics~\cite{Cummins2017b}. No work so far has exploited mined source code for test-case generation. This work is the first to do so.
